; ModuleID = 'runtime_amdgpu.cpp'
source_filename = "runtime_amdgpu.cpp"
target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7"
target triple = "amdgcn-amd-amdhsa"

%0 = type { double, double, i32 }
%1 = type { double, double }
%2 = type { %3 addrspace(1)*, %4 addrspace(1)*, %union.anon, i64, i64, i64 }
%3 = type { i64, i64, i32, i32 }
%4 = type { [64 x [8 x i64]] }
%5 = type { i64, %union.anon, i64, i32, i32, i64, i64, %6, [2 x i32] }
%6 = type { %7 addrspace(1)* }
%7 = type { %8, [4 x i32], i64, i32, i32, i32, i32, i64, i32, [9 x i32], i64, i32, i32, [4 x i32], i64, i64, i32, i32, [2 x i32], %union.anon, [14 x i32] }
%8 = type { i32, i32, i8 addrspace(1)*, %union.anon, i32, i32, i64 }
%struct.__HIP_Coordinates = type { i8 }
%"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X" = type { i8 }
%struct.__HIP_Coordinates.1 = type { i8 }
%"struct.__HIP_Coordinates<__HIP_BlockDim>::__X" = type { i8 }
%struct.__HIP_Coordinates.2 = type { i8 }
%"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X" = type { i8 }
%struct.__HIP_Coordinates.3 = type { i8 }
%"struct.__HIP_Coordinates<__HIP_GridDim>::__X" = type { i8 }
%struct.PhysicalCoordinates = type { [8 x i32] }
%struct.RuntimeContext = type { %struct.LLVMRuntime*, [64 x i64], [32 x [8 x i32]], i32, [64 x i64], [64 x i8], i64* }
%struct.LLVMRuntime = type { i8, i64, i8*, i8*, i8* (i8*, i64, i64)*, void (i8*)*, void (i8*, ...)*, i32 (i8*, i64, i8*, i8*)*, i8*, [512 x i8*], [512 x i64], i8*, void (i8*, i32, i32, i8*, void (i8*, i32, i32)*)*, [1024 x %struct.ListManager*], [1024 x %struct.NodeManager*], [1024 x i8*], i8*, %struct.RandState*, %struct.MemRequestQueue*, i8*, void (i8*, i8*)*, void (i8*)*, [2048 x i8], [32 x i64], i32, i64, i8*, i32, i32, i64, i8* }
%struct.ListManager = type { [131072 x i8*], i64, i64, i32, i32, i32, %struct.LLVMRuntime* }
%struct.NodeManager = type <{ %struct.LLVMRuntime*, i32, i32, i32, i32, %struct.ListManager*, %struct.ListManager*, %struct.ListManager*, i32, [4 x i8] }>
%struct.RandState = type { i32, i32, i32, i32, i32 }
%struct.MemRequestQueue = type { [65536 x %struct.MemRequest], i32, i32 }
%struct.MemRequest = type { i64, i64, i8*, i64 }
%struct.StructMeta = type { i32, i64, i64, i8* (i8*, i8*, i32)*, i8* (i8*)*, i32 (i8*, i8*, i32)*, i32 (i8*, i8*)*, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, %struct.RuntimeContext* }
%struct.Element = type { i8*, [2 x i32], %struct.PhysicalCoordinates }
%class.anon = type { %struct.LLVMRuntime**, i8**, i32*, i64** }
%class.anon.10 = type { i8 }
%class.anon.0 = type { i64*, %struct.LLVMRuntime*, i64*, i8**, i8* }
%class.anon.12 = type { i8 }
%struct.__HIP_ThreadIdx = type { i8 }
%struct.__HIP_BlockDim = type { i8 }
%struct.__HIP_BlockIdx = type { i8 }
%struct.__HIP_GridDim = type { i8 }
%struct.DenseMeta = type <{ %struct.StructMeta, i32, [4 x i8] }>
%struct.DynamicMeta = type <{ %struct.StructMeta, i32, [4 x i8] }>
%struct.PointerMeta = type <{ %struct.StructMeta, i8, [7 x i8] }>
%struct.RootMeta = type <{ %struct.StructMeta, i32, [4 x i8] }>
%struct.BitmaskedMeta = type <{ %struct.StructMeta, i8, [7 x i8] }>
%class.anon.4 = type { %struct.ListManager*, i32* }
%class.anon.20 = type { i8 }
%union.anon = type { i64 }
%union.anon.5 = type { i64 }
%union.anon.6 = type { i64 }
%union.anon.7 = type { i64 }
%union.anon.8 = type { i64 }
%union.anon.9 = type { i64 }
%class.lock_guard = type { i8 }
%class.anon.11 = type { %class.anon.10*, i8**, %class.anon* }
%class.lock_guard.14 = type { i8 }
%class.anon.16 = type { %class.anon.12*, i8**, %class.anon.0* }
%union.anon.17 = type { i64 }
%union.anon.18 = type { i64 }
%union.anon.19 = type { i64 }
%class.lock_guard.22 = type { i8 }
%class.anon.24 = type { %class.anon.20*, i8**, %class.anon.4* }

$_ZN11LLVMRuntime10set_resultIlEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultIcEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultImEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultIiEEvmT_ = comdat any

$_ZN11ListManager21get_num_active_chunksEv = comdat any

$_ZN11LLVMRuntime10set_resultIP11NodeManagerEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultIPhEEvmT_ = comdat any

$_ZN11LLVMRuntime10set_resultIP15MemRequestQueueEEvmT_ = comdat any

$_ZN6taichi8iroundupImmvEET_S1_T0_ = comdat any

$_ZN11LLVMRuntime10set_resultIPS_EEvmT_ = comdat any

$_ZN11LLVMRuntime6createI11ListManagerJRPS_miEEEPT_DpOT0_ = comdat any

$_ZN11LLVMRuntime6createI11NodeManagerJRPS_RmiEEEPT_DpOT0_ = comdat any

$_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_ = comdat any

$_ZSt3minIfEUa9enable_ifILb1EERKT_S2_S2_ = comdat any

$_ZSt3maxIiEUa9enable_ifILb1EERKT_S2_S2_ = comdat any

$_ZSt3maxIfEUa9enable_ifILb1EERKT_S2_S2_ = comdat any

$_ZN11ListManager5clearEv = comdat any

$_ZN11ListManager3getI7ElementEERT_i = comdat any

$_ZN11ListManager4sizeEv = comdat any

$_ZNK17__HIP_CoordinatesI15__HIP_ThreadIdxE3__XcvjEv = comdat any

$_ZNK17__HIP_CoordinatesI14__HIP_BlockDimE3__XcvjEv = comdat any

$_ZNK17__HIP_CoordinatesI14__HIP_BlockIdxE3__XcvjEv = comdat any

$_ZmlN17__HIP_CoordinatesI14__HIP_BlockDimE3__XENS_I13__HIP_GridDimE3__XE = comdat any

$_ZNK17__HIP_CoordinatesI13__HIP_GridDimE3__XcvjEv = comdat any

$_ZN11ListManager19reserve_new_elementEv = comdat any

$_ZN11ListManager15get_element_ptrEi = comdat any

$_ZN11NodeManager9gc_serialEv = comdat any

$_ZN11ListManager3getIiEERT_i = comdat any

$_ZN11ListManager6resizeEi = comdat any

$_ZN11ListManager9push_backIiEEvRKT_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImlET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImcET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImmET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImiET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImP11NodeManagerET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImP11ListManagerET_T0_ = comdat any

$_ZSt3minImEUa9enable_ifILb1EERKT_S2_S2_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImPhET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImP15MemRequestQueueET_T0_ = comdat any

$_Z38taichi_union_cast_with_different_sizesImP11LLVMRuntimeET_T0_ = comdat any

$_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE = comdat any

$_ZSt7forwardImEOT_RNSt16remove_referenceIS0_E4typeE = comdat any

$_ZSt7forwardIiEOT_RNSt16remove_referenceIS0_E4typeE = comdat any

$_ZN11ListManagerC2EP11LLVMRuntimemm = comdat any

$_ZN6taichi7log2intImEEjT_ = comdat any

$_ZSt7forwardIRmEOT_RNSt16remove_referenceIS1_E4typeE = comdat any

$_ZN11NodeManagerC2EP11LLVMRuntimeii = comdat any

$_ZN11LLVMRuntime6createI11ListManagerJRPS_mRiEEEPT_DpOT0_ = comdat any

$_ZN11LLVMRuntime6createI11ListManagerJRPS_RiS4_EEEPT_DpOT0_ = comdat any

$_ZSt7forwardIRiEOT_RNSt16remove_referenceIS1_E4typeE = comdat any

$_ZNK14__HIP_BlockDimclEj = comdat any

$_ZNK14__HIP_BlockIdxclEj = comdat any

$_ZNK15__HIP_ThreadIdxclEj = comdat any

$_ZNK13__HIP_GridDimclEj = comdat any

$_ZN17__HIP_CoordinatesI15__HIP_ThreadIdxE1xE = comdat any

$_ZN17__HIP_CoordinatesI14__HIP_BlockDimE1xE = comdat any

$_ZN17__HIP_CoordinatesI14__HIP_BlockIdxE1xE = comdat any

$_ZN17__HIP_CoordinatesI13__HIP_GridDimE1xE = comdat any

@__const.__assert_fail.fmt = private unnamed_addr addrspace(4) constant [47 x i8] c"%s:%u: %s: Device-side assertion `%s' failed.\0A\00", align 16
@.str = private unnamed_addr addrspace(4) constant [144 x i8] c"Out of CUDA pre-allocated memory.\0AConsider using ti.init(device_memory_fraction=0.9) or ti.init(device_memory_GB=4) to allocate more GPU memory\00", align 1
@.str.1 = private unnamed_addr addrspace(4) constant [11 x i8] c"Taichi JIT\00", align 1
@.str.2 = private unnamed_addr addrspace(4) constant [21 x i8] c"allocate_from_buffer\00", align 1
@.str.3 = private unnamed_addr addrspace(4) constant [28 x i8] c"Out of pre-allocated memory\00", align 1
@.str.4 = private unnamed_addr addrspace(4) constant [37 x i8] c"Too many memory allocation requests.\00", align 1
@_ZL31taichi_listgen_max_element_size = internal addrspace(4) constant i32 1024, align 4
@_ZL9threadIdx = internal addrspace(4) constant %struct.__HIP_Coordinates undef, align 1
@_ZN17__HIP_CoordinatesI15__HIP_ThreadIdxE1xE = linkonce_odr protected addrspace(4) externally_initialized constant %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X" undef, comdat, align 1
@_ZL8blockDim = internal addrspace(4) constant %struct.__HIP_Coordinates.1 undef, align 1
@_ZN17__HIP_CoordinatesI14__HIP_BlockDimE1xE = linkonce_odr protected addrspace(4) externally_initialized constant %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X" undef, comdat, align 1
@_ZL8blockIdx = internal addrspace(4) constant %struct.__HIP_Coordinates.2 undef, align 1
@_ZN17__HIP_CoordinatesI14__HIP_BlockIdxE1xE = linkonce_odr protected addrspace(4) externally_initialized constant %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X" undef, comdat, align 1
@_ZL7gridDim = internal addrspace(4) constant %struct.__HIP_Coordinates.3 undef, align 1
@_ZN17__HIP_CoordinatesI13__HIP_GridDimE1xE = linkonce_odr protected addrspace(4) externally_initialized constant %"struct.__HIP_Coordinates<__HIP_GridDim>::__X" undef, comdat, align 1
@.str.5 = private unnamed_addr addrspace(4) constant [28 x i8] c"List manager out of chunks.\00", align 1
@.str.6 = private unnamed_addr addrspace(4) constant [40 x i8] c"max_num_elements_per_chunk must be POT.\00", align 1
@__oclc_daz_opt = internal local_unnamed_addr addrspace(4) constant i8 0, align 1
@__oclc_unsafe_math_opt = internal local_unnamed_addr addrspace(4) constant i8 0, align 1
@__oclc_finite_only_opt = internal local_unnamed_addr addrspace(4) constant i8 0, align 1
@__oclc_wavefrontsize64 = internal local_unnamed_addr addrspace(4) constant i8 0, align 1
@__oclc_ISA_version = internal local_unnamed_addr addrspace(4) constant i32 10300, align 4

; Function Attrs: convergent mustprogress noinline noreturn nounwind optnone
define weak void @__cxa_pure_virtual() #0 {
  call void @llvm.trap()
  unreachable
}

; Function Attrs: cold noreturn nounwind
declare void @llvm.trap() #1

; Function Attrs: convergent mustprogress noinline noreturn nounwind optnone
define weak void @__cxa_deleted_virtual() #0 {
  call void @llvm.trap()
  unreachable
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define weak hidden void @__assert_fail(i8* %0, i8* %1, i32 %2, i8* %3) #2 {
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8*, align 8, addrspace(5)
  %9 = alloca [47 x i8], align 16, addrspace(5)
  %10 = alloca i64, align 8, addrspace(5)
  %11 = alloca i32, align 4, addrspace(5)
  %12 = alloca i8*, align 8, addrspace(5)
  %13 = alloca i8*, align 8, addrspace(5)
  %14 = alloca i8*, align 8, addrspace(5)
  %15 = alloca i8*, align 8, addrspace(5)
  %16 = addrspacecast i8* addrspace(5)* %5 to i8**
  %17 = addrspacecast i8* addrspace(5)* %6 to i8**
  %18 = addrspacecast i32 addrspace(5)* %7 to i32*
  %19 = addrspacecast i8* addrspace(5)* %8 to i8**
  %20 = addrspacecast [47 x i8] addrspace(5)* %9 to [47 x i8]*
  %21 = addrspacecast i64 addrspace(5)* %10 to i64*
  %22 = addrspacecast i32 addrspace(5)* %11 to i32*
  %23 = addrspacecast i8* addrspace(5)* %12 to i8**
  %24 = addrspacecast i8* addrspace(5)* %13 to i8**
  %25 = addrspacecast i8* addrspace(5)* %14 to i8**
  %26 = addrspacecast i8* addrspace(5)* %15 to i8**
  store i8* %0, i8** %16, align 8
  store i8* %1, i8** %17, align 8
  store i32 %2, i32* %18, align 4
  store i8* %3, i8** %19, align 8
  %27 = bitcast [47 x i8]* %20 to i8*
  call void @llvm.memcpy.p0i8.p4i8.i64(i8* align 16 %27, i8 addrspace(4)* align 16 getelementptr inbounds ([47 x i8], [47 x i8] addrspace(4)* @__const.__assert_fail.fmt, i32 0, i32 0), i64 47, i1 false)
  %28 = call i64 @__ockl_fprintf_stderr_begin() #24
  store i64 %28, i64* %21, align 8
  store i32 0, i32* %22, align 4
  br label %29

29:                                               ; preds = %4
  %30 = getelementptr inbounds [47 x i8], [47 x i8]* %20, i64 0, i64 0
  store i8* %30, i8** %23, align 8
  br label %31

31:                                               ; preds = %36, %29
  %32 = load i8*, i8** %23, align 8
  %33 = getelementptr inbounds i8, i8* %32, i32 1
  store i8* %33, i8** %23, align 8
  %34 = load i8, i8* %32, align 1
  %35 = icmp ne i8 %34, 0
  br i1 %35, label %36, label %37

36:                                               ; preds = %31
  br label %31, !llvm.loop !5

37:                                               ; preds = %31
  %38 = load i8*, i8** %23, align 8
  %39 = getelementptr inbounds [47 x i8], [47 x i8]* %20, i64 0, i64 0
  %40 = ptrtoint i8* %38 to i64
  %41 = ptrtoint i8* %39 to i64
  %42 = sub i64 %40, %41
  %43 = trunc i64 %42 to i32
  store i32 %43, i32* %22, align 4
  br label %44

44:                                               ; preds = %37
  %45 = load i64, i64* %21, align 8
  %46 = getelementptr inbounds [47 x i8], [47 x i8]* %20, i64 0, i64 0
  %47 = load i32, i32* %22, align 4
  %48 = sext i32 %47 to i64
  %49 = call i64 @__ockl_fprintf_append_string_n(i64 %45, i8* %46, i64 %48, i32 0) #24
  store i64 %49, i64* %21, align 8
  br label %50

50:                                               ; preds = %44
  %51 = load i8*, i8** %17, align 8
  store i8* %51, i8** %24, align 8
  br label %52

52:                                               ; preds = %57, %50
  %53 = load i8*, i8** %24, align 8
  %54 = getelementptr inbounds i8, i8* %53, i32 1
  store i8* %54, i8** %24, align 8
  %55 = load i8, i8* %53, align 1
  %56 = icmp ne i8 %55, 0
  br i1 %56, label %57, label %58

57:                                               ; preds = %52
  br label %52, !llvm.loop !7

58:                                               ; preds = %52
  %59 = load i8*, i8** %24, align 8
  %60 = load i8*, i8** %17, align 8
  %61 = ptrtoint i8* %59 to i64
  %62 = ptrtoint i8* %60 to i64
  %63 = sub i64 %61, %62
  %64 = trunc i64 %63 to i32
  store i32 %64, i32* %22, align 4
  br label %65

65:                                               ; preds = %58
  %66 = load i64, i64* %21, align 8
  %67 = load i8*, i8** %17, align 8
  %68 = load i32, i32* %22, align 4
  %69 = sext i32 %68 to i64
  %70 = call i64 @__ockl_fprintf_append_string_n(i64 %66, i8* %67, i64 %69, i32 0) #24
  store i64 %70, i64* %21, align 8
  %71 = load i64, i64* %21, align 8
  %72 = load i32, i32* %18, align 4
  %73 = zext i32 %72 to i64
  %74 = call i64 @__ockl_fprintf_append_args(i64 %71, i32 1, i64 %73, i64 0, i64 0, i64 0, i64 0, i64 0, i64 0, i32 0) #24
  store i64 %74, i64* %21, align 8
  br label %75

75:                                               ; preds = %65
  %76 = load i8*, i8** %19, align 8
  store i8* %76, i8** %25, align 8
  br label %77

77:                                               ; preds = %82, %75
  %78 = load i8*, i8** %25, align 8
  %79 = getelementptr inbounds i8, i8* %78, i32 1
  store i8* %79, i8** %25, align 8
  %80 = load i8, i8* %78, align 1
  %81 = icmp ne i8 %80, 0
  br i1 %81, label %82, label %83

82:                                               ; preds = %77
  br label %77, !llvm.loop !8

83:                                               ; preds = %77
  %84 = load i8*, i8** %25, align 8
  %85 = load i8*, i8** %19, align 8
  %86 = ptrtoint i8* %84 to i64
  %87 = ptrtoint i8* %85 to i64
  %88 = sub i64 %86, %87
  %89 = trunc i64 %88 to i32
  store i32 %89, i32* %22, align 4
  br label %90

90:                                               ; preds = %83
  %91 = load i64, i64* %21, align 8
  %92 = load i8*, i8** %19, align 8
  %93 = load i32, i32* %22, align 4
  %94 = sext i32 %93 to i64
  %95 = call i64 @__ockl_fprintf_append_string_n(i64 %91, i8* %92, i64 %94, i32 0) #24
  store i64 %95, i64* %21, align 8
  br label %96

96:                                               ; preds = %90
  %97 = load i8*, i8** %16, align 8
  store i8* %97, i8** %26, align 8
  br label %98

98:                                               ; preds = %103, %96
  %99 = load i8*, i8** %26, align 8
  %100 = getelementptr inbounds i8, i8* %99, i32 1
  store i8* %100, i8** %26, align 8
  %101 = load i8, i8* %99, align 1
  %102 = icmp ne i8 %101, 0
  br i1 %102, label %103, label %104

103:                                              ; preds = %98
  br label %98, !llvm.loop !9

104:                                              ; preds = %98
  %105 = load i8*, i8** %26, align 8
  %106 = load i8*, i8** %16, align 8
  %107 = ptrtoint i8* %105 to i64
  %108 = ptrtoint i8* %106 to i64
  %109 = sub i64 %107, %108
  %110 = trunc i64 %109 to i32
  store i32 %110, i32* %22, align 4
  br label %111

111:                                              ; preds = %104
  %112 = load i64, i64* %21, align 8
  %113 = load i8*, i8** %16, align 8
  %114 = load i32, i32* %22, align 4
  %115 = sext i32 %114 to i64
  %116 = call i64 @__ockl_fprintf_append_string_n(i64 %112, i8* %113, i64 %115, i32 1) #24
  call void @llvm.trap()
  ret void
}

; Function Attrs: argmemonly nofree nounwind willreturn
declare void @llvm.memcpy.p0i8.p4i8.i64(i8* noalias nocapture writeonly, i8 addrspace(4)* noalias nocapture readonly, i64, i1 immarg) #3

; Function Attrs: convergent mustprogress noinline nounwind optnone
define weak hidden void @__assertfail(i8* %0, i8* %1, i32 %2, i8* %3, i64 %4) #2 {
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i8*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i8*, align 8, addrspace(5)
  %10 = alloca i64, align 8, addrspace(5)
  %11 = addrspacecast i8* addrspace(5)* %6 to i8**
  %12 = addrspacecast i8* addrspace(5)* %7 to i8**
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i8* addrspace(5)* %9 to i8**
  %15 = addrspacecast i64 addrspace(5)* %10 to i64*
  store i8* %0, i8** %11, align 8
  store i8* %1, i8** %12, align 8
  store i32 %2, i32* %13, align 4
  store i8* %3, i8** %14, align 8
  store i64 %4, i64* %15, align 8
  call void @llvm.trap()
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @thread_idx() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_size() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 32
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_idx() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  %3 = call i32 @thread_idx() #24
  %4 = call i32 @warp_size() #24
  %5 = srem i32 %3, %4
  ret i32 %5
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @block_idx() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @block_dim() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 1
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @grid_dim() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 1
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @get_func_type_host_printf(i8* %0, ...) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = addrspacecast i8* addrspace(5)* %2 to i8**
  store i8* %0, i8** %3, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @mark_force_no_inline() #2 {
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @system_memfence() #2 {
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @taichi_strlen(i8* %0) #2 {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %2 to i64*
  %7 = addrspacecast i8* addrspace(5)* %3 to i8**
  %8 = addrspacecast i64 addrspace(5)* %4 to i64*
  %9 = addrspacecast i8* addrspace(5)* %5 to i8**
  store i8* %0, i8** %7, align 8
  store i64 0, i64* %8, align 8
  %10 = load i8*, i8** %7, align 8
  store i8* %10, i8** %9, align 8
  br label %11

11:                                               ; preds = %18, %1
  %12 = load i8*, i8** %9, align 8
  %13 = load i8, i8* %12, align 1
  %14 = icmp ne i8 %13, 0
  br i1 %14, label %15, label %21

15:                                               ; preds = %11
  %16 = load i64, i64* %8, align 8
  %17 = add i64 %16, 1
  store i64 %17, i64* %8, align 8
  br label %18

18:                                               ; preds = %15
  %19 = load i8*, i8** %9, align 8
  %20 = getelementptr inbounds i8, i8* %19, i32 1
  store i8* %20, i8** %9, align 8
  br label %11, !llvm.loop !10

21:                                               ; preds = %11
  %22 = load i64, i64* %8, align 8
  ret i64 %22
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @exp_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_exp_f32(float %16) #25
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @exp_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_exp_f64(double %11) #25
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @log_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_log_f32(float %16) #25
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @log_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_log_f64(double %11) #25
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @tan_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_tan_f32(float %16) #24
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @tan_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_tan_f64(double %11) #24
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @tanh_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_tanh_f32(float %16) #25
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @tanh_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_tanh_f64(double %11) #25
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @abs_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_fabs_f32(float %16) #17
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @abs_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = addrspacecast double addrspace(5)* %6 to double*
  %9 = addrspacecast double addrspace(5)* %7 to double*
  store double %0, double* %9, align 8
  %10 = load double, double* %9, align 8
  %11 = addrspacecast double addrspace(5)* %4 to double*
  %12 = addrspacecast double addrspace(5)* %5 to double*
  store double %10, double* %12, align 8
  %13 = load double, double* %12, align 8
  %14 = addrspacecast double addrspace(5)* %2 to double*
  %15 = addrspacecast double addrspace(5)* %3 to double*
  store double %13, double* %15, align 8
  %16 = load double, double* %15, align 8
  %17 = call contract double @__ocml_fabs_f64(double %16) #17
  ret double %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @acos_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_acos_f32(float %16) #17
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @acos_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_acos_f64(double %11) #17
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @asin_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_asin_f32(float %16) #17
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @asin_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_asin_f64(double %11) #17
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @cos_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_cos_f32(float %16) #24
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @cos_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_cos_f64(double %11) #24
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @sin_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %6 to float*
  %9 = addrspacecast float addrspace(5)* %7 to float*
  store float %0, float* %9, align 4
  %10 = load float, float* %9, align 4
  %11 = addrspacecast float addrspace(5)* %4 to float*
  %12 = addrspacecast float addrspace(5)* %5 to float*
  store float %10, float* %12, align 4
  %13 = load float, float* %12, align 4
  %14 = addrspacecast float addrspace(5)* %2 to float*
  %15 = addrspacecast float addrspace(5)* %3 to float*
  store float %13, float* %15, align 4
  %16 = load float, float* %15, align 4
  %17 = call contract float @__ocml_sin_f32(float %16) #24
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @sin_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %4 to double*
  %7 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  %8 = load double, double* %7, align 8
  %9 = addrspacecast double addrspace(5)* %2 to double*
  %10 = addrspacecast double addrspace(5)* %3 to double*
  store double %8, double* %10, align 8
  %11 = load double, double* %10, align 8
  %12 = call contract double @__ocml_sin_f64(double %11) #24
  ret double %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @abs_i32(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  %6 = load i32, i32* %5, align 4
  %7 = icmp sgt i32 %6, 0
  br i1 %7, label %8, label %10

8:                                                ; preds = %1
  %9 = load i32, i32* %5, align 4
  store i32 %9, i32* %4, align 4
  br label %13

10:                                               ; preds = %1
  %11 = load i32, i32* %5, align 4
  %12 = sub nsw i32 0, %11
  store i32 %12, i32* %4, align 4
  br label %13

13:                                               ; preds = %10, %8
  %14 = load i32, i32* %4, align 4
  ret i32 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i16 @min_u16(i16 zeroext %0, i16 zeroext %1) #2 {
  %3 = alloca i16, align 2, addrspace(5)
  %4 = alloca i16, align 2, addrspace(5)
  %5 = alloca i16, align 2, addrspace(5)
  %6 = addrspacecast i16 addrspace(5)* %3 to i16*
  %7 = addrspacecast i16 addrspace(5)* %4 to i16*
  %8 = addrspacecast i16 addrspace(5)* %5 to i16*
  store i16 %0, i16* %7, align 2
  store i16 %1, i16* %8, align 2
  %9 = load i16, i16* %7, align 2
  %10 = zext i16 %9 to i32
  %11 = load i16, i16* %8, align 2
  %12 = zext i16 %11 to i32
  %13 = icmp slt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i16, i16* %7, align 2
  br label %18

16:                                               ; preds = %2
  %17 = load i16, i16* %8, align 2
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i16 [ %15, %14 ], [ %17, %16 ]
  ret i16 %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden signext i16 @min_i16(i16 signext %0, i16 signext %1) #2 {
  %3 = alloca i16, align 2, addrspace(5)
  %4 = alloca i16, align 2, addrspace(5)
  %5 = alloca i16, align 2, addrspace(5)
  %6 = addrspacecast i16 addrspace(5)* %3 to i16*
  %7 = addrspacecast i16 addrspace(5)* %4 to i16*
  %8 = addrspacecast i16 addrspace(5)* %5 to i16*
  store i16 %0, i16* %7, align 2
  store i16 %1, i16* %8, align 2
  %9 = load i16, i16* %7, align 2
  %10 = sext i16 %9 to i32
  %11 = load i16, i16* %8, align 2
  %12 = sext i16 %11 to i32
  %13 = icmp slt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i16, i16* %7, align 2
  br label %18

16:                                               ; preds = %2
  %17 = load i16, i16* %8, align 2
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i16 [ %15, %14 ], [ %17, %16 ]
  ret i16 %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @min_u32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ult i32 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i32, i32* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load i32, i32* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i32 [ %13, %12 ], [ %15, %14 ]
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @min_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp slt i32 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i32, i32* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load i32, i32* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i32 [ %13, %12 ], [ %15, %14 ]
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @min_u64(i64 %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i64 addrspace(5)* %4 to i64*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i64 %0, i64* %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load i64, i64* %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = icmp ult i64 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i64, i64* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load i64, i64* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i64 [ %13, %12 ], [ %15, %14 ]
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @min_i64(i64 %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i64 addrspace(5)* %4 to i64*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i64 %0, i64* %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load i64, i64* %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = icmp slt i64 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i64, i64* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load i64, i64* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i64 [ %13, %12 ], [ %15, %14 ]
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i16 @max_u16(i16 zeroext %0, i16 zeroext %1) #2 {
  %3 = alloca i16, align 2, addrspace(5)
  %4 = alloca i16, align 2, addrspace(5)
  %5 = alloca i16, align 2, addrspace(5)
  %6 = addrspacecast i16 addrspace(5)* %3 to i16*
  %7 = addrspacecast i16 addrspace(5)* %4 to i16*
  %8 = addrspacecast i16 addrspace(5)* %5 to i16*
  store i16 %0, i16* %7, align 2
  store i16 %1, i16* %8, align 2
  %9 = load i16, i16* %7, align 2
  %10 = zext i16 %9 to i32
  %11 = load i16, i16* %8, align 2
  %12 = zext i16 %11 to i32
  %13 = icmp sgt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i16, i16* %7, align 2
  br label %18

16:                                               ; preds = %2
  %17 = load i16, i16* %8, align 2
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i16 [ %15, %14 ], [ %17, %16 ]
  ret i16 %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden signext i16 @max_i16(i16 signext %0, i16 signext %1) #2 {
  %3 = alloca i16, align 2, addrspace(5)
  %4 = alloca i16, align 2, addrspace(5)
  %5 = alloca i16, align 2, addrspace(5)
  %6 = addrspacecast i16 addrspace(5)* %3 to i16*
  %7 = addrspacecast i16 addrspace(5)* %4 to i16*
  %8 = addrspacecast i16 addrspace(5)* %5 to i16*
  store i16 %0, i16* %7, align 2
  store i16 %1, i16* %8, align 2
  %9 = load i16, i16* %7, align 2
  %10 = sext i16 %9 to i32
  %11 = load i16, i16* %8, align 2
  %12 = sext i16 %11 to i32
  %13 = icmp sgt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i16, i16* %7, align 2
  br label %18

16:                                               ; preds = %2
  %17 = load i16, i16* %8, align 2
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i16 [ %15, %14 ], [ %17, %16 ]
  ret i16 %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @max_u32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ugt i32 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i32, i32* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load i32, i32* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i32 [ %13, %12 ], [ %15, %14 ]
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @max_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp sgt i32 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i32, i32* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load i32, i32* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i32 [ %13, %12 ], [ %15, %14 ]
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @max_u64(i64 %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i64 addrspace(5)* %4 to i64*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i64 %0, i64* %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load i64, i64* %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = icmp ugt i64 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i64, i64* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load i64, i64* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i64 [ %13, %12 ], [ %15, %14 ]
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @max_i64(i64 %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i64 addrspace(5)* %4 to i64*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i64 %0, i64* %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load i64, i64* %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = icmp sgt i64 %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load i64, i64* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load i64, i64* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi i64 [ %13, %12 ], [ %15, %14 ]
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @logic_not_i32(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  %6 = load i32, i32* %5, align 4
  %7 = icmp ne i32 %6, 0
  %8 = xor i1 %7, true
  %9 = zext i1 %8 to i32
  ret i32 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @sgn_f32(float %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = addrspacecast float addrspace(5)* %2 to float*
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  store float %0, float* %6, align 4
  %8 = load float, float* %6, align 4
  %9 = fcmp contract ogt float %8, 0.000000e+00
  br i1 %9, label %10, label %11

10:                                               ; preds = %1
  store float 1.000000e+00, float* %7, align 4
  br label %17

11:                                               ; preds = %1
  %12 = load float, float* %6, align 4
  %13 = fcmp contract olt float %12, 0.000000e+00
  br i1 %13, label %14, label %15

14:                                               ; preds = %11
  store float -1.000000e+00, float* %7, align 4
  br label %16

15:                                               ; preds = %11
  store float 0.000000e+00, float* %7, align 4
  br label %16

16:                                               ; preds = %15, %14
  br label %17

17:                                               ; preds = %16, %10
  %18 = load float, float* %7, align 4
  ret float %18
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @sgn_f64(double %0) #2 {
  %2 = alloca double, align 8, addrspace(5)
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = addrspacecast double addrspace(5)* %2 to double*
  %6 = addrspacecast double addrspace(5)* %3 to double*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  store double %0, double* %6, align 8
  %8 = load double, double* %6, align 8
  %9 = fcmp contract ogt double %8, 0.000000e+00
  br i1 %9, label %10, label %11

10:                                               ; preds = %1
  store float 1.000000e+00, float* %7, align 4
  br label %17

11:                                               ; preds = %1
  %12 = load double, double* %6, align 8
  %13 = fcmp contract olt double %12, 0.000000e+00
  br i1 %13, label %14, label %15

14:                                               ; preds = %11
  store float -1.000000e+00, float* %7, align 4
  br label %16

15:                                               ; preds = %11
  store float 0.000000e+00, float* %7, align 4
  br label %16

16:                                               ; preds = %15, %14
  br label %17

17:                                               ; preds = %16, %10
  %18 = load float, float* %7, align 4
  %19 = fpext float %18 to double
  ret double %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @atan2_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca float, align 4, addrspace(5)
  %9 = alloca float, align 4, addrspace(5)
  %10 = alloca float, align 4, addrspace(5)
  %11 = alloca float, align 4, addrspace(5)
  %12 = addrspacecast float addrspace(5)* %9 to float*
  %13 = addrspacecast float addrspace(5)* %10 to float*
  %14 = addrspacecast float addrspace(5)* %11 to float*
  store float %0, float* %13, align 4
  store float %1, float* %14, align 4
  %15 = load float, float* %13, align 4
  %16 = load float, float* %14, align 4
  %17 = addrspacecast float addrspace(5)* %6 to float*
  %18 = addrspacecast float addrspace(5)* %7 to float*
  %19 = addrspacecast float addrspace(5)* %8 to float*
  store float %15, float* %18, align 4
  store float %16, float* %19, align 4
  %20 = load float, float* %18, align 4
  %21 = load float, float* %19, align 4
  %22 = addrspacecast float addrspace(5)* %3 to float*
  %23 = addrspacecast float addrspace(5)* %4 to float*
  %24 = addrspacecast float addrspace(5)* %5 to float*
  store float %20, float* %23, align 4
  store float %21, float* %24, align 4
  %25 = load float, float* %23, align 4
  %26 = load float, float* %24, align 4
  %27 = call contract float @__ocml_atan2_f32(float %25, float %26) #17
  ret float %27
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @atan2_f64(double %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = alloca double, align 8, addrspace(5)
  %9 = addrspacecast double addrspace(5)* %6 to double*
  %10 = addrspacecast double addrspace(5)* %7 to double*
  %11 = addrspacecast double addrspace(5)* %8 to double*
  store double %0, double* %10, align 8
  store double %1, double* %11, align 8
  %12 = load double, double* %10, align 8
  %13 = load double, double* %11, align 8
  %14 = addrspacecast double addrspace(5)* %3 to double*
  %15 = addrspacecast double addrspace(5)* %4 to double*
  %16 = addrspacecast double addrspace(5)* %5 to double*
  store double %12, double* %15, align 8
  store double %13, double* %16, align 8
  %17 = load double, double* %15, align 8
  %18 = load double, double* %16, align 8
  %19 = call contract double @__ocml_atan2_f64(double %17, double %18) #17
  ret double %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @pow_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca float, align 4, addrspace(5)
  %9 = alloca float, align 4, addrspace(5)
  %10 = alloca float, align 4, addrspace(5)
  %11 = alloca float, align 4, addrspace(5)
  %12 = addrspacecast float addrspace(5)* %9 to float*
  %13 = addrspacecast float addrspace(5)* %10 to float*
  %14 = addrspacecast float addrspace(5)* %11 to float*
  store float %0, float* %13, align 4
  store float %1, float* %14, align 4
  %15 = load float, float* %13, align 4
  %16 = load float, float* %14, align 4
  %17 = addrspacecast float addrspace(5)* %6 to float*
  %18 = addrspacecast float addrspace(5)* %7 to float*
  %19 = addrspacecast float addrspace(5)* %8 to float*
  store float %15, float* %18, align 4
  store float %16, float* %19, align 4
  %20 = load float, float* %18, align 4
  %21 = load float, float* %19, align 4
  %22 = addrspacecast float addrspace(5)* %3 to float*
  %23 = addrspacecast float addrspace(5)* %4 to float*
  %24 = addrspacecast float addrspace(5)* %5 to float*
  store float %20, float* %23, align 4
  store float %21, float* %24, align 4
  %25 = load float, float* %23, align 4
  %26 = load float, float* %24, align 4
  %27 = call contract float @__ocml_pow_f32(float %25, float %26) #25
  ret float %27
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @pow_f64(double %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = alloca double, align 8, addrspace(5)
  %9 = addrspacecast double addrspace(5)* %6 to double*
  %10 = addrspacecast double addrspace(5)* %7 to double*
  %11 = addrspacecast double addrspace(5)* %8 to double*
  store double %0, double* %10, align 8
  store double %1, double* %11, align 8
  %12 = load double, double* %10, align 8
  %13 = load double, double* %11, align 8
  %14 = addrspacecast double addrspace(5)* %3 to double*
  %15 = addrspacecast double addrspace(5)* %4 to double*
  %16 = addrspacecast double addrspace(5)* %5 to double*
  store double %12, double* %15, align 8
  store double %13, double* %16, align 8
  %17 = load double, double* %15, align 8
  %18 = load double, double* %16, align 8
  %19 = call contract double @__ocml_pow_f64(double %17, double %18) #25
  ret double %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @PhysicalCoordinates_get_val(%struct.PhysicalCoordinates* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.PhysicalCoordinates*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.PhysicalCoordinates* addrspace(5)* %4 to %struct.PhysicalCoordinates**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.PhysicalCoordinates* %0, %struct.PhysicalCoordinates** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.PhysicalCoordinates*, %struct.PhysicalCoordinates** %7, align 8
  %10 = getelementptr inbounds %struct.PhysicalCoordinates, %struct.PhysicalCoordinates* %9, i32 0, i32 0
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [8 x i32], [8 x i32]* %10, i64 0, i64 %12
  %14 = load i32, i32* %13, align 4
  ret i32 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @PhysicalCoordinates_set_val(%struct.PhysicalCoordinates* %0, i32 %1, i32 %2) #2 {
  %4 = alloca %struct.PhysicalCoordinates*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.PhysicalCoordinates* addrspace(5)* %4 to %struct.PhysicalCoordinates**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.PhysicalCoordinates* %0, %struct.PhysicalCoordinates** %7, align 8
  store i32 %1, i32* %8, align 4
  store i32 %2, i32* %9, align 4
  %10 = load i32, i32* %9, align 4
  %11 = load %struct.PhysicalCoordinates*, %struct.PhysicalCoordinates** %7, align 8
  %12 = getelementptr inbounds %struct.PhysicalCoordinates, %struct.PhysicalCoordinates* %11, i32 0, i32 0
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [8 x i32], [8 x i32]* %12, i64 0, i64 %14
  store i32 %10, i32* %15, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @RuntimeContext_get_args(%struct.RuntimeContext* %0, i32 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast %struct.RuntimeContext* addrspace(5)* %4 to %struct.RuntimeContext**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.RuntimeContext*, %struct.RuntimeContext** %7, align 8
  %10 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %9, i32 0, i32 1
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [64 x i64], [64 x i64]* %10, i64 0, i64 %12
  %14 = load i64, i64* %13, align 8
  ret i64 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @RuntimeContext_set_args(%struct.RuntimeContext* %0, i32 %1, i64 %2) #2 {
  %4 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast %struct.RuntimeContext* addrspace(5)* %4 to %struct.RuntimeContext**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i64 addrspace(5)* %6 to i64*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %7, align 8
  store i32 %1, i32* %8, align 4
  store i64 %2, i64* %9, align 8
  %10 = load i64, i64* %9, align 8
  %11 = load %struct.RuntimeContext*, %struct.RuntimeContext** %7, align 8
  %12 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %11, i32 0, i32 1
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [64 x i64], [64 x i64]* %12, i64 0, i64 %14
  store i64 %10, i64* %15, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.LLVMRuntime* @RuntimeContext_get_runtime(%struct.RuntimeContext* %0) #2 {
  %2 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %2 to %struct.LLVMRuntime**
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %7 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %6, i32 0, i32 0
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  ret %struct.LLVMRuntime* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.LLVMRuntime** @RuntimeContext_get_ptr_runtime(%struct.RuntimeContext* %0) #2 {
  %2 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %2 to %struct.LLVMRuntime***
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %7 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %6, i32 0, i32 0
  ret %struct.LLVMRuntime** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @RuntimeContext_set_runtime(%struct.RuntimeContext* %0, %struct.LLVMRuntime* %1) #2 {
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %6 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %6, align 8
  %8 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %9 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %8, i32 0, i32 0
  store %struct.LLVMRuntime* %7, %struct.LLVMRuntime** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64* @RuntimeContext_get_result_buffer(%struct.RuntimeContext* %0) #2 {
  %2 = alloca i64*, align 8, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast i64* addrspace(5)* %2 to i64**
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %7 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %6, i32 0, i32 6
  %8 = load i64*, i64** %7, align 8
  ret i64* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64** @RuntimeContext_get_ptr_result_buffer(%struct.RuntimeContext* %0) #2 {
  %2 = alloca i64**, align 8, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast i64** addrspace(5)* %2 to i64***
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %7 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %6, i32 0, i32 6
  ret i64** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @RuntimeContext_set_result_buffer(%struct.RuntimeContext* %0, i64* %1) #2 {
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %6 = addrspacecast i64* addrspace(5)* %4 to i64**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  store i64* %1, i64** %6, align 8
  %7 = load i64*, i64** %6, align 8
  %8 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %9 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %8, i32 0, i32 6
  store i64* %7, i64** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @RuntimeContext_get_extra_args(%struct.RuntimeContext* %0, i32 %1, i32 %2) #2 {
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast %struct.RuntimeContext* addrspace(5)* %5 to %struct.RuntimeContext**
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %9, align 8
  store i32 %1, i32* %10, align 4
  store i32 %2, i32* %11, align 4
  %12 = load %struct.RuntimeContext*, %struct.RuntimeContext** %9, align 8
  %13 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %12, i32 0, i32 2
  %14 = load i32, i32* %10, align 4
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds [32 x [8 x i32]], [32 x [8 x i32]]* %13, i64 0, i64 %15
  %17 = load i32, i32* %11, align 4
  %18 = sext i32 %17 to i64
  %19 = getelementptr inbounds [8 x i32], [8 x i32]* %16, i64 0, i64 %18
  %20 = load i32, i32* %19, align 4
  ret i32 %20
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_exchange_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32* addrspace(5)* %4 to i32**
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32* %0, i32** %8, align 8
  store i32 %1, i32* %9, align 4
  %11 = load i32*, i32** %8, align 8
  %12 = load i32, i32* %9, align 4
  %13 = atomicrmw volatile xchg i32* %11, i32 %12 seq_cst, align 4
  store i32 %13, i32* %10, align 4
  %14 = load i32, i32* %10, align 4
  ret i32 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_exchange_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast i64 addrspace(5)* %3 to i64*
  %8 = addrspacecast i64* addrspace(5)* %4 to i64**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i64 addrspace(5)* %6 to i64*
  store i64* %0, i64** %8, align 8
  store i64 %1, i64* %9, align 8
  %11 = load i64*, i64** %8, align 8
  %12 = load i64, i64* %9, align 8
  %13 = atomicrmw volatile xchg i64* %11, i64 %12 seq_cst, align 8
  store i64 %13, i64* %10, align 8
  %14 = load i64, i64* %10, align 8
  ret i64 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_exchange_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32* addrspace(5)* %4 to i32**
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32* %0, i32** %8, align 8
  store i32 %1, i32* %9, align 4
  %11 = load i32*, i32** %8, align 8
  %12 = load i32, i32* %9, align 4
  %13 = atomicrmw volatile xchg i32* %11, i32 %12 seq_cst, align 4
  store i32 %13, i32* %10, align 4
  %14 = load i32, i32* %10, align 4
  ret i32 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_exchange_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast i64 addrspace(5)* %3 to i64*
  %8 = addrspacecast i64* addrspace(5)* %4 to i64**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i64 addrspace(5)* %6 to i64*
  store i64* %0, i64** %8, align 8
  store i64 %1, i64* %9, align 8
  %11 = load i64*, i64** %8, align 8
  %12 = load i64, i64* %9, align 8
  %13 = atomicrmw volatile xchg i64* %11, i64 %12 seq_cst, align 8
  store i64 %13, i64* %10, align 8
  %14 = load i64, i64* %10, align 8
  ret i64 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_add_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile add i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_add_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile add i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_and_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile and i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_and_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile and i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_and_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile and i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_and_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile and i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_or_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile or i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_or_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile or i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_or_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile or i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_or_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile or i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_xor_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile xor i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_xor_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile xor i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_xor_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = load i32*, i32** %9, align 8
  %14 = load i32, i32* %10, align 4
  store i32 %14, i32* %11, align 4
  %15 = load i32, i32* %11, align 4
  %16 = atomicrmw volatile xor i32* %13, i32 %15 seq_cst, align 4
  store i32 %16, i32* %12, align 4
  %17 = load i32, i32* %12, align 4
  ret i32 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_xor_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64 addrspace(5)* %3 to i64*
  %9 = addrspacecast i64* addrspace(5)* %4 to i64**
  %10 = addrspacecast i64 addrspace(5)* %5 to i64*
  %11 = addrspacecast i64 addrspace(5)* %6 to i64*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %9, align 8
  store i64 %1, i64* %10, align 8
  %13 = load i64*, i64** %9, align 8
  %14 = load i64, i64* %10, align 8
  store i64 %14, i64* %11, align 8
  %15 = load i64, i64* %11, align 8
  %16 = atomicrmw volatile xor i64* %13, i64 %15 seq_cst, align 8
  store i64 %16, i64* %12, align 8
  %17 = load i64, i64* %12, align 8
  ret i64 %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @add_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = load float, float* %7, align 4
  %10 = load float, float* %8, align 4
  %11 = fadd contract float %9, %10
  ret float %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @add_f64(double %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %3 to double*
  %7 = addrspacecast double addrspace(5)* %4 to double*
  %8 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  store double %1, double* %8, align 8
  %9 = load double, double* %7, align 8
  %10 = load double, double* %8, align 8
  %11 = fadd contract double %9, %10
  ret double %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @min_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = load float, float* %8, align 4
  %10 = load float, float* %7, align 4
  %11 = fcmp contract ogt float %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load float, float* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load float, float* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi contract float [ %13, %12 ], [ %15, %14 ]
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @min_f64(double %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %3 to double*
  %7 = addrspacecast double addrspace(5)* %4 to double*
  %8 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  store double %1, double* %8, align 8
  %9 = load double, double* %8, align 8
  %10 = load double, double* %7, align 8
  %11 = fcmp contract ogt double %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load double, double* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load double, double* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi contract double [ %13, %12 ], [ %15, %14 ]
  ret double %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @max_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = load float, float* %8, align 4
  %10 = load float, float* %7, align 4
  %11 = fcmp contract olt float %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load float, float* %7, align 4
  br label %16

14:                                               ; preds = %2
  %15 = load float, float* %8, align 4
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi contract float [ %13, %12 ], [ %15, %14 ]
  ret float %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @max_f64(double %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = addrspacecast double addrspace(5)* %3 to double*
  %7 = addrspacecast double addrspace(5)* %4 to double*
  %8 = addrspacecast double addrspace(5)* %5 to double*
  store double %0, double* %7, align 8
  store double %1, double* %8, align 8
  %9 = load double, double* %8, align 8
  %10 = load double, double* %7, align 8
  %11 = fcmp contract olt double %9, %10
  br i1 %11, label %12, label %14

12:                                               ; preds = %2
  %13 = load double, double* %7, align 8
  br label %16

14:                                               ; preds = %2
  %15 = load double, double* %8, align 8
  br label %16

16:                                               ; preds = %14, %12
  %17 = phi contract double [ %13, %12 ], [ %15, %14 ]
  ret double %17
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @atomic_add_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast float addrspace(5)* %3 to float*
  %10 = addrspacecast float* addrspace(5)* %4 to float**
  %11 = addrspacecast float addrspace(5)* %5 to float*
  %12 = addrspacecast float addrspace(5)* %6 to float*
  %13 = addrspacecast float addrspace(5)* %7 to float*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store float* %0, float** %10, align 8
  store float %1, float* %11, align 4
  br label %15

15:                                               ; preds = %32, %2
  %16 = load float*, float** %10, align 8
  %17 = load volatile float, float* %16, align 4
  store float %17, float* %12, align 4
  %18 = load float, float* %12, align 4
  %19 = load float, float* %11, align 4
  %20 = call contract float @add_f32(float %18, float %19) #24
  store float %20, float* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load float*, float** %10, align 8
  %23 = bitcast float* %22 to i32*
  %24 = bitcast float* %12 to i32*
  %25 = bitcast float* %13 to i32*
  %26 = load i32, i32* %24, align 4
  %27 = load i32, i32* %25, align 4
  %28 = cmpxchg weak volatile i32* %23, i32 %26, i32 %27 seq_cst seq_cst, align 4
  %29 = extractvalue { i32, i1 } %28, 0
  %30 = extractvalue { i32, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i32 %29, i32* %24, align 4
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !11

37:                                               ; preds = %32
  %38 = load float, float* %12, align 4
  ret float %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @atomic_add_f64(double* %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double*, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast double addrspace(5)* %3 to double*
  %10 = addrspacecast double* addrspace(5)* %4 to double**
  %11 = addrspacecast double addrspace(5)* %5 to double*
  %12 = addrspacecast double addrspace(5)* %6 to double*
  %13 = addrspacecast double addrspace(5)* %7 to double*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store double* %0, double** %10, align 8
  store double %1, double* %11, align 8
  br label %15

15:                                               ; preds = %32, %2
  %16 = load double*, double** %10, align 8
  %17 = load volatile double, double* %16, align 8
  store double %17, double* %12, align 8
  %18 = load double, double* %12, align 8
  %19 = load double, double* %11, align 8
  %20 = call contract double @add_f64(double %18, double %19) #24
  store double %20, double* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load double*, double** %10, align 8
  %23 = bitcast double* %22 to i64*
  %24 = bitcast double* %12 to i64*
  %25 = bitcast double* %13 to i64*
  %26 = load i64, i64* %24, align 8
  %27 = load i64, i64* %25, align 8
  %28 = cmpxchg weak volatile i64* %23, i64 %26, i64 %27 seq_cst seq_cst, align 8
  %29 = extractvalue { i64, i1 } %28, 0
  %30 = extractvalue { i64, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i64 %29, i64* %24, align 8
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !12

37:                                               ; preds = %32
  %38 = load double, double* %12, align 8
  ret double %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_min_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i32 addrspace(5)* %3 to i32*
  %10 = addrspacecast i32* addrspace(5)* %4 to i32**
  %11 = addrspacecast i32 addrspace(5)* %5 to i32*
  %12 = addrspacecast i32 addrspace(5)* %6 to i32*
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i32* %0, i32** %10, align 8
  store i32 %1, i32* %11, align 4
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i32*, i32** %10, align 8
  %17 = load volatile i32, i32* %16, align 4
  store i32 %17, i32* %12, align 4
  %18 = load i32, i32* %12, align 4
  %19 = load i32, i32* %11, align 4
  %20 = call i32 @min_i32(i32 %18, i32 %19) #24
  store i32 %20, i32* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load i32*, i32** %10, align 8
  %23 = load i32, i32* %12, align 4
  %24 = load i32, i32* %13, align 4
  %25 = cmpxchg weak volatile i32* %22, i32 %23, i32 %24 seq_cst seq_cst, align 4
  %26 = extractvalue { i32, i1 } %25, 0
  %27 = extractvalue { i32, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i32 %26, i32* %12, align 4
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !13

34:                                               ; preds = %29
  %35 = load i32, i32* %12, align 4
  ret i32 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_min_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i64 addrspace(5)* %3 to i64*
  %10 = addrspacecast i64* addrspace(5)* %4 to i64**
  %11 = addrspacecast i64 addrspace(5)* %5 to i64*
  %12 = addrspacecast i64 addrspace(5)* %6 to i64*
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i64* %0, i64** %10, align 8
  store i64 %1, i64* %11, align 8
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i64*, i64** %10, align 8
  %17 = load volatile i64, i64* %16, align 8
  store i64 %17, i64* %12, align 8
  %18 = load i64, i64* %12, align 8
  %19 = load i64, i64* %11, align 8
  %20 = call i64 @min_i64(i64 %18, i64 %19) #24
  store i64 %20, i64* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load i64*, i64** %10, align 8
  %23 = load i64, i64* %12, align 8
  %24 = load i64, i64* %13, align 8
  %25 = cmpxchg weak volatile i64* %22, i64 %23, i64 %24 seq_cst seq_cst, align 8
  %26 = extractvalue { i64, i1 } %25, 0
  %27 = extractvalue { i64, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i64 %26, i64* %12, align 8
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !14

34:                                               ; preds = %29
  %35 = load i64, i64* %12, align 8
  ret i64 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @atomic_min_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast float addrspace(5)* %3 to float*
  %10 = addrspacecast float* addrspace(5)* %4 to float**
  %11 = addrspacecast float addrspace(5)* %5 to float*
  %12 = addrspacecast float addrspace(5)* %6 to float*
  %13 = addrspacecast float addrspace(5)* %7 to float*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store float* %0, float** %10, align 8
  store float %1, float* %11, align 4
  br label %15

15:                                               ; preds = %32, %2
  %16 = load float*, float** %10, align 8
  %17 = load volatile float, float* %16, align 4
  store float %17, float* %12, align 4
  %18 = load float, float* %12, align 4
  %19 = load float, float* %11, align 4
  %20 = call contract float @min_f32(float %18, float %19) #24
  store float %20, float* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load float*, float** %10, align 8
  %23 = bitcast float* %22 to i32*
  %24 = bitcast float* %12 to i32*
  %25 = bitcast float* %13 to i32*
  %26 = load i32, i32* %24, align 4
  %27 = load i32, i32* %25, align 4
  %28 = cmpxchg weak volatile i32* %23, i32 %26, i32 %27 seq_cst seq_cst, align 4
  %29 = extractvalue { i32, i1 } %28, 0
  %30 = extractvalue { i32, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i32 %29, i32* %24, align 4
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !15

37:                                               ; preds = %32
  %38 = load float, float* %12, align 4
  ret float %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @atomic_min_f64(double* %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double*, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast double addrspace(5)* %3 to double*
  %10 = addrspacecast double* addrspace(5)* %4 to double**
  %11 = addrspacecast double addrspace(5)* %5 to double*
  %12 = addrspacecast double addrspace(5)* %6 to double*
  %13 = addrspacecast double addrspace(5)* %7 to double*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store double* %0, double** %10, align 8
  store double %1, double* %11, align 8
  br label %15

15:                                               ; preds = %32, %2
  %16 = load double*, double** %10, align 8
  %17 = load volatile double, double* %16, align 8
  store double %17, double* %12, align 8
  %18 = load double, double* %12, align 8
  %19 = load double, double* %11, align 8
  %20 = call contract double @min_f64(double %18, double %19) #24
  store double %20, double* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load double*, double** %10, align 8
  %23 = bitcast double* %22 to i64*
  %24 = bitcast double* %12 to i64*
  %25 = bitcast double* %13 to i64*
  %26 = load i64, i64* %24, align 8
  %27 = load i64, i64* %25, align 8
  %28 = cmpxchg weak volatile i64* %23, i64 %26, i64 %27 seq_cst seq_cst, align 8
  %29 = extractvalue { i64, i1 } %28, 0
  %30 = extractvalue { i64, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i64 %29, i64* %24, align 8
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !16

37:                                               ; preds = %32
  %38 = load double, double* %12, align 8
  ret double %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_max_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i32 addrspace(5)* %3 to i32*
  %10 = addrspacecast i32* addrspace(5)* %4 to i32**
  %11 = addrspacecast i32 addrspace(5)* %5 to i32*
  %12 = addrspacecast i32 addrspace(5)* %6 to i32*
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i32* %0, i32** %10, align 8
  store i32 %1, i32* %11, align 4
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i32*, i32** %10, align 8
  %17 = load volatile i32, i32* %16, align 4
  store i32 %17, i32* %12, align 4
  %18 = load i32, i32* %12, align 4
  %19 = load i32, i32* %11, align 4
  %20 = call i32 @max_i32(i32 %18, i32 %19) #24
  store i32 %20, i32* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load i32*, i32** %10, align 8
  %23 = load i32, i32* %12, align 4
  %24 = load i32, i32* %13, align 4
  %25 = cmpxchg weak volatile i32* %22, i32 %23, i32 %24 seq_cst seq_cst, align 4
  %26 = extractvalue { i32, i1 } %25, 0
  %27 = extractvalue { i32, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i32 %26, i32* %12, align 4
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !17

34:                                               ; preds = %29
  %35 = load i32, i32* %12, align 4
  ret i32 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_max_i64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i64 addrspace(5)* %3 to i64*
  %10 = addrspacecast i64* addrspace(5)* %4 to i64**
  %11 = addrspacecast i64 addrspace(5)* %5 to i64*
  %12 = addrspacecast i64 addrspace(5)* %6 to i64*
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i64* %0, i64** %10, align 8
  store i64 %1, i64* %11, align 8
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i64*, i64** %10, align 8
  %17 = load volatile i64, i64* %16, align 8
  store i64 %17, i64* %12, align 8
  %18 = load i64, i64* %12, align 8
  %19 = load i64, i64* %11, align 8
  %20 = call i64 @max_i64(i64 %18, i64 %19) #24
  store i64 %20, i64* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load i64*, i64** %10, align 8
  %23 = load i64, i64* %12, align 8
  %24 = load i64, i64* %13, align 8
  %25 = cmpxchg weak volatile i64* %22, i64 %23, i64 %24 seq_cst seq_cst, align 8
  %26 = extractvalue { i64, i1 } %25, 0
  %27 = extractvalue { i64, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i64 %26, i64* %12, align 8
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !18

34:                                               ; preds = %29
  %35 = load i64, i64* %12, align 8
  ret i64 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @atomic_max_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast float addrspace(5)* %3 to float*
  %10 = addrspacecast float* addrspace(5)* %4 to float**
  %11 = addrspacecast float addrspace(5)* %5 to float*
  %12 = addrspacecast float addrspace(5)* %6 to float*
  %13 = addrspacecast float addrspace(5)* %7 to float*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store float* %0, float** %10, align 8
  store float %1, float* %11, align 4
  br label %15

15:                                               ; preds = %32, %2
  %16 = load float*, float** %10, align 8
  %17 = load volatile float, float* %16, align 4
  store float %17, float* %12, align 4
  %18 = load float, float* %12, align 4
  %19 = load float, float* %11, align 4
  %20 = call contract float @max_f32(float %18, float %19) #24
  store float %20, float* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load float*, float** %10, align 8
  %23 = bitcast float* %22 to i32*
  %24 = bitcast float* %12 to i32*
  %25 = bitcast float* %13 to i32*
  %26 = load i32, i32* %24, align 4
  %27 = load i32, i32* %25, align 4
  %28 = cmpxchg weak volatile i32* %23, i32 %26, i32 %27 seq_cst seq_cst, align 4
  %29 = extractvalue { i32, i1 } %28, 0
  %30 = extractvalue { i32, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i32 %29, i32* %24, align 4
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !19

37:                                               ; preds = %32
  %38 = load float, float* %12, align 4
  ret float %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden double @atomic_max_f64(double* %0, double %1) #2 {
  %3 = alloca double, align 8, addrspace(5)
  %4 = alloca double*, align 8, addrspace(5)
  %5 = alloca double, align 8, addrspace(5)
  %6 = alloca double, align 8, addrspace(5)
  %7 = alloca double, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast double addrspace(5)* %3 to double*
  %10 = addrspacecast double* addrspace(5)* %4 to double**
  %11 = addrspacecast double addrspace(5)* %5 to double*
  %12 = addrspacecast double addrspace(5)* %6 to double*
  %13 = addrspacecast double addrspace(5)* %7 to double*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store double* %0, double** %10, align 8
  store double %1, double* %11, align 8
  br label %15

15:                                               ; preds = %32, %2
  %16 = load double*, double** %10, align 8
  %17 = load volatile double, double* %16, align 8
  store double %17, double* %12, align 8
  %18 = load double, double* %12, align 8
  %19 = load double, double* %11, align 8
  %20 = call contract double @max_f64(double %18, double %19) #24
  store double %20, double* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load double*, double** %10, align 8
  %23 = bitcast double* %22 to i64*
  %24 = bitcast double* %12 to i64*
  %25 = bitcast double* %13 to i64*
  %26 = load i64, i64* %24, align 8
  %27 = load i64, i64* %25, align 8
  %28 = cmpxchg weak volatile i64* %23, i64 %26, i64 %27 seq_cst seq_cst, align 8
  %29 = extractvalue { i64, i1 } %28, 0
  %30 = extractvalue { i64, i1 } %28, 1
  br i1 %30, label %32, label %31

31:                                               ; preds = %21
  store i64 %29, i64* %24, align 8
  br label %32

32:                                               ; preds = %31, %21
  %33 = zext i1 %30 to i8
  store i8 %33, i8* %14, align 1
  %34 = load i8, i8* %14, align 1
  %35 = trunc i8 %34 to i1
  %36 = xor i1 %35, true
  br i1 %36, label %15, label %37, !llvm.loop !20

37:                                               ; preds = %32
  %38 = load double, double* %12, align 8
  ret double %38
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_min_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i32 addrspace(5)* %3 to i32*
  %10 = addrspacecast i32* addrspace(5)* %4 to i32**
  %11 = addrspacecast i32 addrspace(5)* %5 to i32*
  %12 = addrspacecast i32 addrspace(5)* %6 to i32*
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i32* %0, i32** %10, align 8
  store i32 %1, i32* %11, align 4
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i32*, i32** %10, align 8
  %17 = load volatile i32, i32* %16, align 4
  store i32 %17, i32* %12, align 4
  %18 = load i32, i32* %12, align 4
  %19 = load i32, i32* %11, align 4
  %20 = call i32 @min_u32(i32 %18, i32 %19) #24
  store i32 %20, i32* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load i32*, i32** %10, align 8
  %23 = load i32, i32* %12, align 4
  %24 = load i32, i32* %13, align 4
  %25 = cmpxchg weak volatile i32* %22, i32 %23, i32 %24 seq_cst seq_cst, align 4
  %26 = extractvalue { i32, i1 } %25, 0
  %27 = extractvalue { i32, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i32 %26, i32* %12, align 4
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !21

34:                                               ; preds = %29
  %35 = load i32, i32* %12, align 4
  ret i32 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_min_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i64 addrspace(5)* %3 to i64*
  %10 = addrspacecast i64* addrspace(5)* %4 to i64**
  %11 = addrspacecast i64 addrspace(5)* %5 to i64*
  %12 = addrspacecast i64 addrspace(5)* %6 to i64*
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i64* %0, i64** %10, align 8
  store i64 %1, i64* %11, align 8
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i64*, i64** %10, align 8
  %17 = load volatile i64, i64* %16, align 8
  store i64 %17, i64* %12, align 8
  %18 = load i64, i64* %12, align 8
  %19 = load i64, i64* %11, align 8
  %20 = call i64 @min_u64(i64 %18, i64 %19) #24
  store i64 %20, i64* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load i64*, i64** %10, align 8
  %23 = load i64, i64* %12, align 8
  %24 = load i64, i64* %13, align 8
  %25 = cmpxchg weak volatile i64* %22, i64 %23, i64 %24 seq_cst seq_cst, align 8
  %26 = extractvalue { i64, i1 } %25, 0
  %27 = extractvalue { i64, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i64 %26, i64* %12, align 8
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !22

34:                                               ; preds = %29
  %35 = load i64, i64* %12, align 8
  ret i64 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @atomic_max_u32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i32 addrspace(5)* %3 to i32*
  %10 = addrspacecast i32* addrspace(5)* %4 to i32**
  %11 = addrspacecast i32 addrspace(5)* %5 to i32*
  %12 = addrspacecast i32 addrspace(5)* %6 to i32*
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i32* %0, i32** %10, align 8
  store i32 %1, i32* %11, align 4
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i32*, i32** %10, align 8
  %17 = load volatile i32, i32* %16, align 4
  store i32 %17, i32* %12, align 4
  %18 = load i32, i32* %12, align 4
  %19 = load i32, i32* %11, align 4
  %20 = call i32 @max_u32(i32 %18, i32 %19) #24
  store i32 %20, i32* %13, align 4
  br label %21

21:                                               ; preds = %15
  %22 = load i32*, i32** %10, align 8
  %23 = load i32, i32* %12, align 4
  %24 = load i32, i32* %13, align 4
  %25 = cmpxchg weak volatile i32* %22, i32 %23, i32 %24 seq_cst seq_cst, align 4
  %26 = extractvalue { i32, i1 } %25, 0
  %27 = extractvalue { i32, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i32 %26, i32* %12, align 4
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !23

34:                                               ; preds = %29
  %35 = load i32, i32* %12, align 4
  ret i32 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @atomic_max_u64(i64* %0, i64 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i64 addrspace(5)* %3 to i64*
  %10 = addrspacecast i64* addrspace(5)* %4 to i64**
  %11 = addrspacecast i64 addrspace(5)* %5 to i64*
  %12 = addrspacecast i64 addrspace(5)* %6 to i64*
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i64* %0, i64** %10, align 8
  store i64 %1, i64* %11, align 8
  br label %15

15:                                               ; preds = %29, %2
  %16 = load i64*, i64** %10, align 8
  %17 = load volatile i64, i64* %16, align 8
  store i64 %17, i64* %12, align 8
  %18 = load i64, i64* %12, align 8
  %19 = load i64, i64* %11, align 8
  %20 = call i64 @max_u64(i64 %18, i64 %19) #24
  store i64 %20, i64* %13, align 8
  br label %21

21:                                               ; preds = %15
  %22 = load i64*, i64** %10, align 8
  %23 = load i64, i64* %12, align 8
  %24 = load i64, i64* %13, align 8
  %25 = cmpxchg weak volatile i64* %22, i64 %23, i64 %24 seq_cst seq_cst, align 8
  %26 = extractvalue { i64, i1 } %25, 0
  %27 = extractvalue { i64, i1 } %25, 1
  br i1 %27, label %29, label %28

28:                                               ; preds = %21
  store i64 %26, i64* %12, align 8
  br label %29

29:                                               ; preds = %28, %21
  %30 = zext i1 %27 to i8
  store i8 %30, i8* %14, align 1
  %31 = load i8, i8* %14, align 1
  %32 = trunc i8 %31 to i1
  %33 = xor i1 %32, true
  br i1 %33, label %15, label %34, !llvm.loop !24

34:                                               ; preds = %29
  %35 = load i64, i64* %12, align 8
  ret i64 %35
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @StructMeta_get_snode_id(%struct.StructMeta* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 0
  %8 = load i32, i32* %7, align 8
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32* @StructMeta_get_ptr_snode_id(%struct.StructMeta* %0) #2 {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 0
  ret i32* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_snode_id(%struct.StructMeta* %0, i32 %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load i32, i32* %6, align 4
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 0
  store i32 %7, i32* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @StructMeta_get_element_size(%struct.StructMeta* %0) #2 {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i64 addrspace(5)* %2 to i64*
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 1
  %8 = load i64, i64* %7, align 8
  ret i64 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64* @StructMeta_get_ptr_element_size(%struct.StructMeta* %0) #2 {
  %2 = alloca i64*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i64* addrspace(5)* %2 to i64**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 1
  ret i64* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_element_size(%struct.StructMeta* %0, i64 %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i64 addrspace(5)* %4 to i64*
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i64 %1, i64* %6, align 8
  %7 = load i64, i64* %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 1
  store i64 %7, i64* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @StructMeta_get_max_num_elements(%struct.StructMeta* %0) #2 {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i64 addrspace(5)* %2 to i64*
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 2
  %8 = load i64, i64* %7, align 8
  ret i64 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64* @StructMeta_get_ptr_max_num_elements(%struct.StructMeta* %0) #2 {
  %2 = alloca i64*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i64* addrspace(5)* %2 to i64**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 2
  ret i64* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_max_num_elements(%struct.StructMeta* %0, i64 %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i64 addrspace(5)* %4 to i64*
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i64 %1, i64* %6, align 8
  %7 = load i64, i64* %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 2
  store i64 %7, i64* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i8*)* @StructMeta_get_get_num_elements(%struct.StructMeta* %0) #2 {
  %2 = alloca i32 (i8*, i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i8*)* addrspace(5)* %2 to i32 (i8*, i8*)**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 6
  %8 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %7, align 8
  ret i32 (i8*, i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i8*)** @StructMeta_get_ptr_get_num_elements(%struct.StructMeta* %0) #2 {
  %2 = alloca i32 (i8*, i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i8*)** addrspace(5)* %2 to i32 (i8*, i8*)***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 6
  ret i32 (i8*, i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_get_num_elements(%struct.StructMeta* %0, i32 (i8*, i8*)* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i32 (i8*, i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i32 (i8*, i8*)* addrspace(5)* %4 to i32 (i8*, i8*)**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i32 (i8*, i8*)* %1, i32 (i8*, i8*)** %6, align 8
  %7 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 6
  store i32 (i8*, i8*)* %7, i32 (i8*, i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* (i8*, i8*, i32)* @StructMeta_get_lookup_element(%struct.StructMeta* %0) #2 {
  %2 = alloca i8* (i8*, i8*, i32)*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* (i8*, i8*, i32)* addrspace(5)* %2 to i8* (i8*, i8*, i32)**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 3
  %8 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %7, align 8
  ret i8* (i8*, i8*, i32)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* (i8*, i8*, i32)** @StructMeta_get_ptr_lookup_element(%struct.StructMeta* %0) #2 {
  %2 = alloca i8* (i8*, i8*, i32)**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* (i8*, i8*, i32)** addrspace(5)* %2 to i8* (i8*, i8*, i32)***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 3
  ret i8* (i8*, i8*, i32)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_lookup_element(%struct.StructMeta* %0, i8* (i8*, i8*, i32)* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i8* (i8*, i8*, i32)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i8* (i8*, i8*, i32)* addrspace(5)* %4 to i8* (i8*, i8*, i32)**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i8* (i8*, i8*, i32)* %1, i8* (i8*, i8*, i32)** %6, align 8
  %7 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 3
  store i8* (i8*, i8*, i32)* %7, i8* (i8*, i8*, i32)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* (i8*)* @StructMeta_get_from_parent_element(%struct.StructMeta* %0) #2 {
  %2 = alloca i8* (i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* (i8*)* addrspace(5)* %2 to i8* (i8*)**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 4
  %8 = load i8* (i8*)*, i8* (i8*)** %7, align 8
  ret i8* (i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* (i8*)** @StructMeta_get_ptr_from_parent_element(%struct.StructMeta* %0) #2 {
  %2 = alloca i8* (i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* (i8*)** addrspace(5)* %2 to i8* (i8*)***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 4
  ret i8* (i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_from_parent_element(%struct.StructMeta* %0, i8* (i8*)* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i8* (i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i8* (i8*)* addrspace(5)* %4 to i8* (i8*)**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i8* (i8*)* %1, i8* (i8*)** %6, align 8
  %7 = load i8* (i8*)*, i8* (i8*)** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 4
  store i8* (i8*)* %7, i8* (i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* @StructMeta_get_refine_coordinates(%struct.StructMeta* %0) #2 {
  %2 = alloca void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* addrspace(5)* %2 to void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 7
  %8 = load void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %7, align 8
  ret void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** @StructMeta_get_ptr_refine_coordinates(%struct.StructMeta* %0) #2 {
  %2 = alloca void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** addrspace(5)* %2 to void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 7
  ret void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_refine_coordinates(%struct.StructMeta* %0, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* addrspace(5)* %4 to void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* %1, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %6, align 8
  %7 = load void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 7
  store void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* %7, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i8*, i32)* @StructMeta_get_is_active(%struct.StructMeta* %0) #2 {
  %2 = alloca i32 (i8*, i8*, i32)*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i8*, i32)* addrspace(5)* %2 to i32 (i8*, i8*, i32)**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 5
  %8 = load i32 (i8*, i8*, i32)*, i32 (i8*, i8*, i32)** %7, align 8
  ret i32 (i8*, i8*, i32)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i8*, i32)** @StructMeta_get_ptr_is_active(%struct.StructMeta* %0) #2 {
  %2 = alloca i32 (i8*, i8*, i32)**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i8*, i32)** addrspace(5)* %2 to i32 (i8*, i8*, i32)***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 5
  ret i32 (i8*, i8*, i32)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_is_active(%struct.StructMeta* %0, i32 (i8*, i8*, i32)* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca i32 (i8*, i8*, i32)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast i32 (i8*, i8*, i32)* addrspace(5)* %4 to i32 (i8*, i8*, i32)**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store i32 (i8*, i8*, i32)* %1, i32 (i8*, i8*, i32)** %6, align 8
  %7 = load i32 (i8*, i8*, i32)*, i32 (i8*, i8*, i32)** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 5
  store i32 (i8*, i8*, i32)* %7, i32 (i8*, i8*, i32)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.RuntimeContext* @StructMeta_get_context(%struct.StructMeta* %0) #2 {
  %2 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast %struct.RuntimeContext* addrspace(5)* %2 to %struct.RuntimeContext**
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 8
  %8 = load %struct.RuntimeContext*, %struct.RuntimeContext** %7, align 8
  ret %struct.RuntimeContext* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.RuntimeContext** @StructMeta_get_ptr_context(%struct.StructMeta* %0) #2 {
  %2 = alloca %struct.RuntimeContext**, align 8, addrspace(5)
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = addrspacecast %struct.RuntimeContext** addrspace(5)* %2 to %struct.RuntimeContext***
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  %6 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %7 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %6, i32 0, i32 8
  ret %struct.RuntimeContext** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @StructMeta_set_context(%struct.StructMeta* %0, %struct.RuntimeContext* %1) #2 {
  %3 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %4 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %5 = addrspacecast %struct.StructMeta* addrspace(5)* %3 to %struct.StructMeta**
  %6 = addrspacecast %struct.RuntimeContext* addrspace(5)* %4 to %struct.RuntimeContext**
  store %struct.StructMeta* %0, %struct.StructMeta** %5, align 8
  store %struct.RuntimeContext* %1, %struct.RuntimeContext** %6, align 8
  %7 = load %struct.RuntimeContext*, %struct.RuntimeContext** %6, align 8
  %8 = load %struct.StructMeta*, %struct.StructMeta** %5, align 8
  %9 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %8, i32 0, i32 8
  store %struct.RuntimeContext* %7, %struct.RuntimeContext** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @_Z15is_power_of_twoj(i32 %0) #2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  %6 = load i32, i32* %5, align 4
  %7 = icmp ne i32 %6, 0
  br i1 %7, label %8, label %14

8:                                                ; preds = %1
  %9 = load i32, i32* %5, align 4
  %10 = load i32, i32* %5, align 4
  %11 = sub i32 %10, 1
  %12 = and i32 %9, %11
  %13 = icmp eq i32 %12, 0
  br label %14

14:                                               ; preds = %8, %1
  %15 = phi i1 [ false, %1 ], [ %13, %8 ]
  ret i1 %15
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @Element_get_element(%struct.Element* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = addrspacecast i8* addrspace(5)* %2 to i8**
  %5 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  store %struct.Element* %0, %struct.Element** %5, align 8
  %6 = load %struct.Element*, %struct.Element** %5, align 8
  %7 = getelementptr inbounds %struct.Element, %struct.Element* %6, i32 0, i32 0
  %8 = load i8*, i8** %7, align 8
  ret i8* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8** @Element_get_ptr_element(%struct.Element* %0) #2 {
  %2 = alloca i8**, align 8, addrspace(5)
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = addrspacecast i8** addrspace(5)* %2 to i8***
  %5 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  store %struct.Element* %0, %struct.Element** %5, align 8
  %6 = load %struct.Element*, %struct.Element** %5, align 8
  %7 = getelementptr inbounds %struct.Element, %struct.Element* %6, i32 0, i32 0
  ret i8** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @Element_set_element(%struct.Element* %0, i8* %1) #2 {
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  %6 = addrspacecast i8* addrspace(5)* %4 to i8**
  store %struct.Element* %0, %struct.Element** %5, align 8
  store i8* %1, i8** %6, align 8
  %7 = load i8*, i8** %6, align 8
  %8 = load %struct.Element*, %struct.Element** %5, align 8
  %9 = getelementptr inbounds %struct.Element, %struct.Element* %8, i32 0, i32 0
  store i8* %7, i8** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.PhysicalCoordinates @Element_get_pcoord(%struct.Element* %0) #2 {
  %2 = alloca %struct.PhysicalCoordinates, align 4, addrspace(5)
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = addrspacecast %struct.PhysicalCoordinates addrspace(5)* %2 to %struct.PhysicalCoordinates*
  %5 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  store %struct.Element* %0, %struct.Element** %5, align 8
  %6 = load %struct.Element*, %struct.Element** %5, align 8
  %7 = getelementptr inbounds %struct.Element, %struct.Element* %6, i32 0, i32 2
  %8 = bitcast %struct.PhysicalCoordinates* %4 to i8*
  %9 = bitcast %struct.PhysicalCoordinates* %7 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 4 %8, i8* align 8 %9, i64 32, i1 false)
  %10 = load %struct.PhysicalCoordinates, %struct.PhysicalCoordinates* %4, align 4
  ret %struct.PhysicalCoordinates %10
}

; Function Attrs: argmemonly nofree nounwind willreturn
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* noalias nocapture writeonly, i8* noalias nocapture readonly, i64, i1 immarg) #3

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.PhysicalCoordinates* @Element_get_ptr_pcoord(%struct.Element* %0) #2 {
  %2 = alloca %struct.PhysicalCoordinates*, align 8, addrspace(5)
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = addrspacecast %struct.PhysicalCoordinates* addrspace(5)* %2 to %struct.PhysicalCoordinates**
  %5 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  store %struct.Element* %0, %struct.Element** %5, align 8
  %6 = load %struct.Element*, %struct.Element** %5, align 8
  %7 = getelementptr inbounds %struct.Element, %struct.Element* %6, i32 0, i32 2
  ret %struct.PhysicalCoordinates* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @Element_set_pcoord(%struct.Element* %0, [8 x i32] %1) #2 {
  %3 = alloca %struct.PhysicalCoordinates, align 4, addrspace(5)
  %4 = alloca %struct.Element*, align 8, addrspace(5)
  %5 = addrspacecast %struct.PhysicalCoordinates addrspace(5)* %3 to %struct.PhysicalCoordinates*
  %6 = addrspacecast %struct.Element* addrspace(5)* %4 to %struct.Element**
  %7 = getelementptr inbounds %struct.PhysicalCoordinates, %struct.PhysicalCoordinates* %5, i32 0, i32 0
  store [8 x i32] %1, [8 x i32]* %7, align 4
  store %struct.Element* %0, %struct.Element** %6, align 8
  %8 = load %struct.Element*, %struct.Element** %6, align 8
  %9 = getelementptr inbounds %struct.Element, %struct.Element* %8, i32 0, i32 2
  %10 = bitcast %struct.PhysicalCoordinates* %9 to i8*
  %11 = bitcast %struct.PhysicalCoordinates* %5 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %10, i8* align 4 %11, i64 32, i1 false)
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @Element_get_loop_bounds(%struct.Element* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.Element*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.Element* addrspace(5)* %4 to %struct.Element**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.Element* %0, %struct.Element** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.Element*, %struct.Element** %7, align 8
  %10 = getelementptr inbounds %struct.Element, %struct.Element* %9, i32 0, i32 1
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [2 x i32], [2 x i32]* %10, i64 0, i64 %12
  %14 = load i32, i32* %13, align 4
  ret i32 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @Element_set_loop_bounds(%struct.Element* %0, i32 %1, i32 %2) #2 {
  %4 = alloca %struct.Element*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.Element* addrspace(5)* %4 to %struct.Element**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.Element* %0, %struct.Element** %7, align 8
  store i32 %1, i32* %8, align 4
  store i32 %2, i32* %9, align 4
  %10 = load i32, i32* %9, align 4
  %11 = load %struct.Element*, %struct.Element** %7, align 8
  %12 = getelementptr inbounds %struct.Element, %struct.Element* %11, i32 0, i32 1
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [2 x i32], [2 x i32]* %12, i64 0, i64 %14
  store i32 %10, i32* %15, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @initialize_rand_state(%struct.RandState* %0, i32 %1) #2 {
  %3 = alloca %struct.RandState*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.RandState* addrspace(5)* %3 to %struct.RandState**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.RandState* %0, %struct.RandState** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load i32, i32* %6, align 4
  %8 = mul i32 123456789, %7
  %9 = mul i32 %8, 1000000007
  %10 = load %struct.RandState*, %struct.RandState** %5, align 8
  %11 = getelementptr inbounds %struct.RandState, %struct.RandState* %10, i32 0, i32 0
  store i32 %9, i32* %11, align 4
  %12 = load %struct.RandState*, %struct.RandState** %5, align 8
  %13 = getelementptr inbounds %struct.RandState, %struct.RandState* %12, i32 0, i32 1
  store i32 362436069, i32* %13, align 4
  %14 = load %struct.RandState*, %struct.RandState** %5, align 8
  %15 = getelementptr inbounds %struct.RandState, %struct.RandState* %14, i32 0, i32 2
  store i32 521288629, i32* %15, align 4
  %16 = load %struct.RandState*, %struct.RandState** %5, align 8
  %17 = getelementptr inbounds %struct.RandState, %struct.RandState* %16, i32 0, i32 3
  store i32 88675123, i32* %17, align 4
  %18 = load %struct.RandState*, %struct.RandState** %5, align 8
  %19 = getelementptr inbounds %struct.RandState, %struct.RandState* %18, i32 0, i32 4
  store i32 0, i32* %19, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.ListManager* @LLVMRuntime_get_element_lists(%struct.LLVMRuntime* %0, i32 %1) #2 {
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 13
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %10, i64 0, i64 %12
  %14 = load %struct.ListManager*, %struct.ListManager** %13, align 8
  ret %struct.ListManager* %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_element_lists(%struct.LLVMRuntime* %0, i32 %1, %struct.ListManager* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca %struct.ListManager*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast %struct.ListManager* addrspace(5)* %6 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  store %struct.ListManager* %2, %struct.ListManager** %9, align 8
  %10 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 13
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %12, i64 0, i64 %14
  store %struct.ListManager* %10, %struct.ListManager** %15, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden %struct.NodeManager* @LLVMRuntime_get_node_allocators(%struct.LLVMRuntime* %0, i32 %1) #2 {
  %3 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %3 to %struct.NodeManager**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 14
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %10, i64 0, i64 %12
  %14 = load %struct.NodeManager*, %struct.NodeManager** %13, align 8
  ret %struct.NodeManager* %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_node_allocators(%struct.LLVMRuntime* %0, i32 %1, %struct.NodeManager* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast %struct.NodeManager* addrspace(5)* %6 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  store %struct.NodeManager* %2, %struct.NodeManager** %9, align 8
  %10 = load %struct.NodeManager*, %struct.NodeManager** %9, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 14
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %12, i64 0, i64 %14
  store %struct.NodeManager* %10, %struct.NodeManager** %15, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @LLVMRuntime_get_roots(%struct.LLVMRuntime* %0, i32 %1) #2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 9
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [512 x i8*], [512 x i8*]* %10, i64 0, i64 %12
  %14 = load i8*, i8** %13, align 8
  ret i8* %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_roots(%struct.LLVMRuntime* %0, i32 %1, i8* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i8* addrspace(5)* %6 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  store i8* %2, i8** %9, align 8
  %10 = load i8*, i8** %9, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 9
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [512 x i8*], [512 x i8*]* %12, i64 0, i64 %14
  store i8* %10, i8** %15, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i64 @LLVMRuntime_get_root_mem_sizes(%struct.LLVMRuntime* %0, i32 %1) #2 {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 10
  %11 = load i32, i32* %8, align 4
  %12 = sext i32 %11 to i64
  %13 = getelementptr inbounds [512 x i64], [512 x i64]* %10, i64 0, i64 %12
  %14 = load i64, i64* %13, align 8
  ret i64 %14
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_root_mem_sizes(%struct.LLVMRuntime* %0, i32 %1, i64 %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i64 addrspace(5)* %6 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  store i64 %2, i64* %9, align 8
  %10 = load i64, i64* %9, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 10
  %13 = load i32, i32* %8, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [512 x i64], [512 x i64]* %12, i64 0, i64 %14
  store i64 %10, i64* %15, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @LLVMRuntime_get_temporaries(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i8* addrspace(5)* %2 to i8**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 16
  %8 = load i8*, i8** %7, align 8
  ret i8* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8** @LLVMRuntime_get_ptr_temporaries(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i8**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i8** addrspace(5)* %2 to i8***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 16
  ret i8** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_temporaries(%struct.LLVMRuntime* %0, i8* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast i8* addrspace(5)* %4 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store i8* %1, i8** %6, align 8
  %7 = load i8*, i8** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 16
  store i8* %7, i8** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*)* @LLVMRuntime_get_assert_failed(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*)* addrspace(5)* %2 to void (i8*)**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 5
  %8 = load void (i8*)*, void (i8*)** %7, align 8
  ret void (i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*)** @LLVMRuntime_get_ptr_assert_failed(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*)** addrspace(5)* %2 to void (i8*)***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 5
  ret void (i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_assert_failed(%struct.LLVMRuntime* %0, void (i8*)* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca void (i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast void (i8*)* addrspace(5)* %4 to void (i8*)**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store void (i8*)* %1, void (i8*)** %6, align 8
  %7 = load void (i8*)*, void (i8*)** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 5
  store void (i8*)* %7, void (i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*, ...)* @LLVMRuntime_get_host_printf(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*, ...)*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*, ...)* addrspace(5)* %2 to void (i8*, ...)**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 6
  %8 = load void (i8*, ...)*, void (i8*, ...)** %7, align 8
  ret void (i8*, ...)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*, ...)** @LLVMRuntime_get_ptr_host_printf(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*, ...)**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*, ...)** addrspace(5)* %2 to void (i8*, ...)***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 6
  ret void (i8*, ...)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_host_printf(%struct.LLVMRuntime* %0, void (i8*, ...)* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca void (i8*, ...)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast void (i8*, ...)* addrspace(5)* %4 to void (i8*, ...)**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store void (i8*, ...)* %1, void (i8*, ...)** %6, align 8
  %7 = load void (i8*, ...)*, void (i8*, ...)** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 6
  store void (i8*, ...)* %7, void (i8*, ...)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i64, i8*, i8*)* @LLVMRuntime_get_host_vsnprintf(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i32 (i8*, i64, i8*, i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i64, i8*, i8*)* addrspace(5)* %2 to i32 (i8*, i64, i8*, i8*)**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 7
  %8 = load i32 (i8*, i64, i8*, i8*)*, i32 (i8*, i64, i8*, i8*)** %7, align 8
  ret i32 (i8*, i64, i8*, i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 (i8*, i64, i8*, i8*)** @LLVMRuntime_get_ptr_host_vsnprintf(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i32 (i8*, i64, i8*, i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i32 (i8*, i64, i8*, i8*)** addrspace(5)* %2 to i32 (i8*, i64, i8*, i8*)***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 7
  ret i32 (i8*, i64, i8*, i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_host_vsnprintf(%struct.LLVMRuntime* %0, i32 (i8*, i64, i8*, i8*)* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca i32 (i8*, i64, i8*, i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast i32 (i8*, i64, i8*, i8*)* addrspace(5)* %4 to i32 (i8*, i64, i8*, i8*)**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store i32 (i8*, i64, i8*, i8*)* %1, i32 (i8*, i64, i8*, i8*)** %6, align 8
  %7 = load i32 (i8*, i64, i8*, i8*)*, i32 (i8*, i64, i8*, i8*)** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 7
  store i32 (i8*, i64, i8*, i8*)* %7, i32 (i8*, i64, i8*, i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @LLVMRuntime_get_profiler(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i8* addrspace(5)* %2 to i8**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 19
  %8 = load i8*, i8** %7, align 8
  ret i8* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8** @LLVMRuntime_get_ptr_profiler(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca i8**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast i8** addrspace(5)* %2 to i8***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 19
  ret i8** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_profiler(%struct.LLVMRuntime* %0, i8* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast i8* addrspace(5)* %4 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store i8* %1, i8** %6, align 8
  %7 = load i8*, i8** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 19
  store i8* %7, i8** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*, i8*)* @LLVMRuntime_get_profiler_start(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*, i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*, i8*)* addrspace(5)* %2 to void (i8*, i8*)**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 20
  %8 = load void (i8*, i8*)*, void (i8*, i8*)** %7, align 8
  ret void (i8*, i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*, i8*)** @LLVMRuntime_get_ptr_profiler_start(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*, i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*, i8*)** addrspace(5)* %2 to void (i8*, i8*)***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 20
  ret void (i8*, i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_profiler_start(%struct.LLVMRuntime* %0, void (i8*, i8*)* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca void (i8*, i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast void (i8*, i8*)* addrspace(5)* %4 to void (i8*, i8*)**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store void (i8*, i8*)* %1, void (i8*, i8*)** %6, align 8
  %7 = load void (i8*, i8*)*, void (i8*, i8*)** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 20
  store void (i8*, i8*)* %7, void (i8*, i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*)* @LLVMRuntime_get_profiler_stop(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*)*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*)* addrspace(5)* %2 to void (i8*)**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 21
  %8 = load void (i8*)*, void (i8*)** %7, align 8
  ret void (i8*)* %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void (i8*)** @LLVMRuntime_get_ptr_profiler_stop(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca void (i8*)**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast void (i8*)** addrspace(5)* %2 to void (i8*)***
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  %6 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %7 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %6, i32 0, i32 21
  ret void (i8*)** %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_set_profiler_stop(%struct.LLVMRuntime* %0, void (i8*)* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca void (i8*)*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast void (i8*)* addrspace(5)* %4 to void (i8*)**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store void (i8*)* %1, void (i8*)** %6, align 8
  %7 = load void (i8*)*, void (i8*)** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 21
  store void (i8*)* %7, void (i8*)** %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @RuntimeContext_store_result(%struct.RuntimeContext* %0, i64 %1, i32 %2) #2 {
  %4 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.RuntimeContext* addrspace(5)* %4 to %struct.RuntimeContext**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %7, align 8
  store i64 %1, i64* %8, align 8
  store i32 %2, i32* %9, align 4
  %10 = load i64, i64* %8, align 8
  %11 = load %struct.RuntimeContext*, %struct.RuntimeContext** %7, align 8
  %12 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %11, i32 0, i32 6
  %13 = load i64*, i64** %12, align 8
  %14 = load i32, i32* %9, align 4
  %15 = zext i32 %14 to i64
  %16 = add i64 0, %15
  %17 = getelementptr inbounds i64, i64* %13, i64 %16
  store i64 %10, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_profiler_start(%struct.LLVMRuntime* %0, i8* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast i8* addrspace(5)* %4 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store i8* %1, i8** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %7, i32 0, i32 20
  %9 = load void (i8*, i8*)*, void (i8*, i8*)** %8, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %11 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 19
  %12 = load i8*, i8** %11, align 8
  %13 = load i8*, i8** %6, align 8
  call void %9(i8* %12, i8* %13) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_profiler_stop(%struct.LLVMRuntime* %0) #2 {
  %2 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %3 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %2 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %3, align 8
  %4 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %3, align 8
  %5 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %4, i32 0, i32 21
  %6 = load void (i8*)*, void (i8*)** %5, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %3, align 8
  %8 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %7, i32 0, i32 19
  %9 = load i8*, i8** %8, align 8
  call void %6(i8* %9) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @get_temporary_pointer(%struct.LLVMRuntime* %0, i64 %1) #2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 16
  %11 = load i8*, i8** %10, align 8
  %12 = load i64, i64* %8, align 8
  %13 = getelementptr inbounds i8, i8* %11, i64 %12
  ret i8* %13
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_retrieve_and_reset_error_code(%struct.LLVMRuntime addrspace(1)* %0) #4 {
  %2 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %2 to %struct.LLVMRuntime**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %6, %struct.LLVMRuntime** %4, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %4, align 8
  store %struct.LLVMRuntime* %7, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 25
  %11 = load i64, i64* %10, align 8
  call void @_ZN11LLVMRuntime10set_resultIlEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %8, i64 30, i64 %11) #24
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 25
  store i64 0, i64* %13, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIlEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i64 %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i64 addrspace(5)* %6 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store i64 %2, i64* %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i64, i64* %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImlET_T0_(i64 %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_retrieve_error_message(%struct.LLVMRuntime addrspace(1)* %0, i32 %1) #4 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %9, %struct.LLVMRuntime** %6, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %6, align 8
  store %struct.LLVMRuntime* %10, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 22
  %14 = load i32, i32* %8, align 4
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds [2048 x i8], [2048 x i8]* %13, i64 0, i64 %15
  %17 = load i8, i8* %16, align 1
  call void @_ZN11LLVMRuntime10set_resultIcEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %11, i64 30, i8 signext %17) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIcEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i8 signext %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i8, align 1, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i8 addrspace(5)* %6 to i8*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store i8 %2, i8* %9, align 1
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i8, i8* %9, align 1
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImcET_T0_(i8 signext %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_retrieve_error_message_argument(%struct.LLVMRuntime addrspace(1)* %0, i32 %1) #4 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %9, %struct.LLVMRuntime** %6, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %6, align 8
  store %struct.LLVMRuntime* %10, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 23
  %14 = load i32, i32* %8, align 4
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds [32 x i64], [32 x i64]* %13, i64 0, i64 %15
  %17 = load i64, i64* %16, align 8
  call void @_ZN11LLVMRuntime10set_resultImEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %11, i64 30, i64 %17) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultImEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i64 %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i64 addrspace(5)* %6 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store i64 %2, i64* %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i64, i64* %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImmET_T0_(i64 %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_ListManager_get_num_active_chunks(%struct.LLVMRuntime addrspace(1)* %0, %struct.ListManager addrspace(1)* %1) #4 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca %struct.ListManager*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %8 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %10 = addrspacecast %struct.ListManager* addrspace(5)* %6 to %struct.ListManager**
  %11 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %11, %struct.LLVMRuntime** %7, align 8
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %13 = addrspacecast %struct.ListManager addrspace(1)* %1 to %struct.ListManager*
  store %struct.ListManager* %13, %struct.ListManager** %8, align 8
  %14 = load %struct.ListManager*, %struct.ListManager** %8, align 8
  store %struct.LLVMRuntime* %12, %struct.LLVMRuntime** %9, align 8
  store %struct.ListManager* %14, %struct.ListManager** %10, align 8
  %15 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %16 = load %struct.ListManager*, %struct.ListManager** %10, align 8
  %17 = call i32 @_ZN11ListManager21get_num_active_chunksEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %16) #24
  call void @_ZN11LLVMRuntime10set_resultIiEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %15, i64 31, i32 %17) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIiEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i32 %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store i32 %2, i32* %9, align 4
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i32, i32* %9, align 4
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImiET_T0_(i32 %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZN11ListManager21get_num_active_chunksEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %2 to i32*
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  %10 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  store i32 0, i32* %8, align 4
  store i32 0, i32* %9, align 4
  br label %11

11:                                               ; preds = %25, %1
  %12 = load i32, i32* %9, align 4
  %13 = sext i32 %12 to i64
  %14 = icmp ult i64 %13, 131072
  br i1 %14, label %15, label %28

15:                                               ; preds = %11
  %16 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %10, i32 0, i32 0
  %17 = load i32, i32* %9, align 4
  %18 = sext i32 %17 to i64
  %19 = getelementptr inbounds [131072 x i8*], [131072 x i8*]* %16, i64 0, i64 %18
  %20 = load i8*, i8** %19, align 8
  %21 = icmp ne i8* %20, null
  %22 = zext i1 %21 to i32
  %23 = load i32, i32* %8, align 4
  %24 = add nsw i32 %23, %22
  store i32 %24, i32* %8, align 4
  br label %25

25:                                               ; preds = %15
  %26 = load i32, i32* %9, align 4
  %27 = add nsw i32 %26, 1
  store i32 %27, i32* %9, align 4
  br label %11, !llvm.loop !25

28:                                               ; preds = %11
  %29 = load i32, i32* %8, align 4
  ret i32 %29
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_LLVMRuntime_get_node_allocators(%struct.LLVMRuntime* %0, %struct.LLVMRuntime* %1, i32 %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %8, align 8
  store i32 %2, i32* %9, align 4
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %8, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 14
  %13 = load i32, i32* %9, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %12, i64 0, i64 %14
  %16 = load %struct.NodeManager*, %struct.NodeManager** %15, align 8
  call void @_ZN11LLVMRuntime10set_resultIP11NodeManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %10, i64 31, %struct.NodeManager* %16) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIP11NodeManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, %struct.NodeManager* %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast %struct.NodeManager* addrspace(5)* %6 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store %struct.NodeManager* %2, %struct.NodeManager** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.NodeManager*, %struct.NodeManager** %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImP11NodeManagerET_T0_(%struct.NodeManager* %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_LLVMRuntime_get_element_lists(%struct.LLVMRuntime* %0, %struct.LLVMRuntime* %1, i32 %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %8, align 8
  store i32 %2, i32* %9, align 4
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %8, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 13
  %13 = load i32, i32* %9, align 4
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %12, i64 0, i64 %14
  %16 = load %struct.ListManager*, %struct.ListManager** %15, align 8
  call void @_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %10, i64 31, %struct.ListManager* %16) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, %struct.ListManager* %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca %struct.ListManager*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast %struct.ListManager* addrspace(5)* %6 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store %struct.ListManager* %2, %struct.ListManager** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImP11ListManagerET_T0_(%struct.ListManager* %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_LLVMRuntime_get_total_requested_memory(%struct.LLVMRuntime* %0, %struct.LLVMRuntime* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %6, align 8
  %9 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 29
  %10 = load i64, i64* %9, align 8
  call void @_ZN11LLVMRuntime10set_resultIlEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, i64 %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_NodeManager_get_free_list(%struct.LLVMRuntime* %0, %struct.NodeManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %4 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.NodeManager* %1, %struct.NodeManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.NodeManager*, %struct.NodeManager** %6, align 8
  %9 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %8, i32 0, i32 5
  %10 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  call void @_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, %struct.ListManager* %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_NodeManager_get_recycled_list(%struct.LLVMRuntime* %0, %struct.NodeManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %4 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.NodeManager* %1, %struct.NodeManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.NodeManager*, %struct.NodeManager** %6, align 8
  %9 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %8, i32 0, i32 6
  %10 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  call void @_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, %struct.ListManager* %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_NodeManager_get_data_list(%struct.LLVMRuntime* %0, %struct.NodeManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %4 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.NodeManager* %1, %struct.NodeManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.NodeManager*, %struct.NodeManager** %6, align 8
  %9 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %8, i32 0, i32 7
  %10 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  call void @_ZN11LLVMRuntime10set_resultIP11ListManagerEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, %struct.ListManager* %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_NodeManager_get_free_list_used(%struct.LLVMRuntime* %0, %struct.NodeManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %4 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.NodeManager* %1, %struct.NodeManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.NodeManager*, %struct.NodeManager** %6, align 8
  %9 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %8, i32 0, i32 4
  %10 = load i32, i32* %9, align 4
  call void @_ZN11LLVMRuntime10set_resultIiEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, i32 %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_ListManager_get_num_elements(%struct.LLVMRuntime* %0, %struct.ListManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.ListManager* %1, %struct.ListManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %9 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 5
  %10 = load i32, i32* %9, align 8
  call void @_ZN11LLVMRuntime10set_resultIiEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, i32 %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_ListManager_get_max_num_elements_per_chunk(%struct.LLVMRuntime* %0, %struct.ListManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.ListManager* %1, %struct.ListManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %9 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 2
  %10 = load i64, i64* %9, align 8
  call void @_ZN11LLVMRuntime10set_resultImEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, i64 %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @runtime_ListManager_get_element_size(%struct.LLVMRuntime* %0, %struct.ListManager* %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store %struct.ListManager* %1, %struct.ListManager** %6, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %9 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 1
  %10 = load i64, i64* %9, align 8
  call void @_ZN11LLVMRuntime10set_resultImEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %7, i64 31, i64 %10) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @taichi_assert(%struct.RuntimeContext* %0, i32 %1, i8* %2) #2 {
  %4 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = addrspacecast %struct.RuntimeContext* addrspace(5)* %4 to %struct.RuntimeContext**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i8* addrspace(5)* %6 to i8**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %7, align 8
  store i32 %1, i32* %8, align 4
  store i8* %2, i8** %9, align 8
  %10 = load %struct.RuntimeContext*, %struct.RuntimeContext** %7, align 8
  %11 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %10, i32 0, i32 0
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %11, align 8
  %13 = load i32, i32* %8, align 4
  %14 = load i8*, i8** %9, align 8
  call void @taichi_assert_runtime(%struct.LLVMRuntime* %12, i32 %13, i8* %14) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @taichi_assert_runtime(%struct.LLVMRuntime* %0, i32 %1, i8* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  %9 = addrspacecast i8* addrspace(5)* %6 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i32 %1, i32* %8, align 4
  store i8* %2, i8** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i32, i32* %8, align 4
  %12 = load i8*, i8** %9, align 8
  call void @taichi_assert_format(%struct.LLVMRuntime* %10, i32 %11, i8* %12, i32 0, i64* null) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @taichi_assert_format(%struct.LLVMRuntime* %0, i32 %1, i8* %2, i32 %3, i64* %4) #2 {
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i8*, align 8, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i64*, align 8, addrspace(5)
  %11 = alloca %class.anon, align 8, addrspace(5)
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i8* addrspace(5)* %8 to i8**
  %15 = addrspacecast i32 addrspace(5)* %9 to i32*
  %16 = addrspacecast i64* addrspace(5)* %10 to i64**
  %17 = addrspacecast %class.anon addrspace(5)* %11 to %class.anon*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store i32 %1, i32* %13, align 4
  store i8* %2, i8** %14, align 8
  store i32 %3, i32* %15, align 4
  store i64* %4, i64** %16, align 8
  call void @mark_force_no_inline() #24
  %18 = load i32, i32* %13, align 4
  %19 = icmp ne i32 %18, 0
  br i1 %19, label %20, label %21

20:                                               ; preds = %5
  br label %34

21:                                               ; preds = %5
  %22 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %23 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %22, i32 0, i32 25
  %24 = load i64, i64* %23, align 8
  %25 = icmp ne i64 %24, 0
  br i1 %25, label %34, label %26

26:                                               ; preds = %21
  %27 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %28 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %27, i32 0, i32 24
  %29 = bitcast i32* %28 to i8*
  %30 = getelementptr inbounds %class.anon, %class.anon* %17, i32 0, i32 0
  store %struct.LLVMRuntime** %12, %struct.LLVMRuntime*** %30, align 8
  %31 = getelementptr inbounds %class.anon, %class.anon* %17, i32 0, i32 1
  store i8** %14, i8*** %31, align 8
  %32 = getelementptr inbounds %class.anon, %class.anon* %17, i32 0, i32 2
  store i32* %15, i32** %32, align 8
  %33 = getelementptr inbounds %class.anon, %class.anon* %17, i32 0, i32 3
  store i64** %16, i64*** %33, align 8
  call void @_Z11locked_taskIZ20taichi_assert_formatEUlvE_EvPvRKT_(i8* %29, %class.anon* nonnull align 8 dereferenceable(32) %17) #24
  br label %34

34:                                               ; preds = %20, %26, %21
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZ20taichi_assert_formatEUlvE_EvPvRKT_(i8* %0, %class.anon* nonnull align 8 dereferenceable(32) %1) #2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %class.anon*, align 8, addrspace(5)
  %5 = alloca %class.anon.10, align 1, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %class.anon* addrspace(5)* %4 to %class.anon**
  %8 = addrspacecast %class.anon.10 addrspace(5)* %5 to %class.anon.10*
  store i8* %0, i8** %6, align 8
  store %class.anon* %1, %class.anon** %7, align 8
  %9 = load i8*, i8** %6, align 8
  %10 = load %class.anon*, %class.anon** %7, align 8
  call void @_Z11locked_taskIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EvS2_S5_RKT0_(i8* %9, %class.anon* nonnull align 8 dereferenceable(32) %10, %class.anon.10* nonnull align 1 dereferenceable(1) %8) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i64 %2) #2 align 2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %10 = addrspacecast i64 addrspace(5)* %6 to i64*
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %9, align 8
  store i64 %1, i64* %10, align 8
  store i64 %2, i64* %11, align 8
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 0
  %14 = load i8, i8* %13, align 8
  %15 = trunc i8 %14 to i1
  br i1 %15, label %16, label %20

16:                                               ; preds = %3
  %17 = load i64, i64* %10, align 8
  %18 = load i64, i64* %11, align 8
  %19 = call i8* @_ZN11LLVMRuntime20allocate_from_bufferEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %12, i64 %17, i64 %18) #24
  store i8* %19, i8** %8, align 8
  br label %28

20:                                               ; preds = %3
  %21 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 4
  %22 = load i8* (i8*, i64, i64)*, i8* (i8*, i64, i64)** %21, align 8
  %23 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 8
  %24 = load i8*, i8** %23, align 8
  %25 = load i64, i64* %10, align 8
  %26 = load i64, i64* %11, align 8
  %27 = call i8* %22(i8* %24, i64 %25, i64 %26) #24
  store i8* %27, i8** %8, align 8
  br label %28

28:                                               ; preds = %20, %16
  %29 = load i8*, i8** %8, align 8
  ret i8* %29
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @_ZN11LLVMRuntime20allocate_from_bufferEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i64 %2) #2 align 2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8*, align 8, addrspace(5)
  %9 = alloca i8, align 1, addrspace(5)
  %10 = alloca %class.anon.0, align 8, addrspace(5)
  %11 = addrspacecast i8* addrspace(5)* %4 to i8**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %13 = addrspacecast i64 addrspace(5)* %6 to i64*
  %14 = addrspacecast i64 addrspace(5)* %7 to i64*
  %15 = addrspacecast i8* addrspace(5)* %8 to i8**
  %16 = addrspacecast i8 addrspace(5)* %9 to i8*
  %17 = addrspacecast %class.anon.0 addrspace(5)* %10 to %class.anon.0*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store i64 %1, i64* %13, align 8
  store i64 %2, i64* %14, align 8
  %18 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  store i8* null, i8** %15, align 8
  store i8 0, i8* %16, align 1
  %19 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %18, i32 0, i32 27
  %20 = bitcast i32* %19 to i8*
  %21 = getelementptr inbounds %class.anon.0, %class.anon.0* %17, i32 0, i32 0
  store i64* %14, i64** %21, align 8
  %22 = getelementptr inbounds %class.anon.0, %class.anon.0* %17, i32 0, i32 1
  store %struct.LLVMRuntime* %18, %struct.LLVMRuntime** %22, align 8
  %23 = getelementptr inbounds %class.anon.0, %class.anon.0* %17, i32 0, i32 2
  store i64* %13, i64** %23, align 8
  %24 = getelementptr inbounds %class.anon.0, %class.anon.0* %17, i32 0, i32 3
  store i8** %15, i8*** %24, align 8
  %25 = getelementptr inbounds %class.anon.0, %class.anon.0* %17, i32 0, i32 4
  store i8* %16, i8** %25, align 8
  call void @_Z11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_EvPvRKT_(i8* %20, %class.anon.0* nonnull align 8 dereferenceable(40) %17) #24
  %26 = load i8, i8* %16, align 1
  %27 = trunc i8 %26 to i1
  br i1 %27, label %29, label %28

28:                                               ; preds = %3
  call void @__assertfail(i8* getelementptr inbounds ([144 x i8], [144 x i8]* addrspacecast ([144 x i8] addrspace(4)* @.str to [144 x i8]*), i64 0, i64 0), i8* getelementptr inbounds ([11 x i8], [11 x i8]* addrspacecast ([11 x i8] addrspace(4)* @.str.1 to [11 x i8]*), i64 0, i64 0), i32 0, i8* getelementptr inbounds ([21 x i8], [21 x i8]* addrspacecast ([21 x i8] addrspace(4)* @.str.2 to [21 x i8]*), i64 0, i64 0), i64 1) #24
  br label %29

29:                                               ; preds = %28, %3
  %30 = load i8, i8* %16, align 1
  %31 = trunc i8 %30 to i1
  %32 = zext i1 %31 to i32
  call void @taichi_assert_runtime(%struct.LLVMRuntime* %18, i32 %32, i8* getelementptr inbounds ([28 x i8], [28 x i8]* addrspacecast ([28 x i8] addrspace(4)* @.str.3 to [28 x i8]*), i64 0, i64 0)) #24
  %33 = load i8*, i8** %15, align 8
  ret i8* %33
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_EvPvRKT_(i8* %0, %class.anon.0* nonnull align 8 dereferenceable(40) %1) #2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %class.anon.0*, align 8, addrspace(5)
  %5 = alloca %class.anon.12, align 1, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %class.anon.0* addrspace(5)* %4 to %class.anon.0**
  %8 = addrspacecast %class.anon.12 addrspace(5)* %5 to %class.anon.12*
  store i8* %0, i8** %6, align 8
  store %class.anon.0* %1, %class.anon.0** %7, align 8
  %9 = load i8*, i8** %6, align 8
  %10 = load %class.anon.0*, %class.anon.0** %7, align 8
  call void @_Z11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EvS3_S6_RKT0_(i8* %9, %class.anon.0* nonnull align 8 dereferenceable(40) %10, %class.anon.12* nonnull align 1 dereferenceable(1) %8) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @_ZN11LLVMRuntime8allocateEm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1) #2 align 2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = call i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %9, i64 %10, i64 1) #24
  ret i8* %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i64 %2) #2 align 2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca %struct.MemRequest*, align 8, addrspace(5)
  %10 = addrspacecast i8* addrspace(5)* %4 to i8**
  %11 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %12 = addrspacecast i64 addrspace(5)* %6 to i64*
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i32 addrspace(5)* %8 to i32*
  %15 = addrspacecast %struct.MemRequest* addrspace(5)* %9 to %struct.MemRequest**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %11, align 8
  store i64 %1, i64* %12, align 8
  store i64 %2, i64* %13, align 8
  %16 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %11, align 8
  %17 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %16, i32 0, i32 29
  %18 = load i64, i64* %12, align 8
  %19 = call i64 @atomic_add_i64(i64* %17, i64 %18) #24
  %20 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %16, i32 0, i32 0
  %21 = load i8, i8* %20, align 8
  %22 = trunc i8 %21 to i1
  br i1 %22, label %23, label %27

23:                                               ; preds = %3
  %24 = load i64, i64* %12, align 8
  %25 = load i64, i64* %13, align 8
  %26 = call i8* @_ZN11LLVMRuntime20allocate_from_bufferEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %16, i64 %24, i64 %25) #24
  store i8* %26, i8** %10, align 8
  br label %59

27:                                               ; preds = %3
  %28 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %16, i32 0, i32 18
  %29 = load %struct.MemRequestQueue*, %struct.MemRequestQueue** %28, align 8
  %30 = getelementptr inbounds %struct.MemRequestQueue, %struct.MemRequestQueue* %29, i32 0, i32 1
  %31 = call i32 @atomic_add_i32(i32* %30, i32 1) #24
  store i32 %31, i32* %14, align 4
  %32 = load i32, i32* %14, align 4
  %33 = icmp sle i32 %32, 65536
  %34 = zext i1 %33 to i32
  call void @taichi_assert_runtime(%struct.LLVMRuntime* %16, i32 %34, i8* getelementptr inbounds ([37 x i8], [37 x i8]* addrspacecast ([37 x i8] addrspace(4)* @.str.4 to [37 x i8]*), i64 0, i64 0)) #24
  %35 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %16, i32 0, i32 18
  %36 = load %struct.MemRequestQueue*, %struct.MemRequestQueue** %35, align 8
  %37 = getelementptr inbounds %struct.MemRequestQueue, %struct.MemRequestQueue* %36, i32 0, i32 0
  %38 = load i32, i32* %14, align 4
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds [65536 x %struct.MemRequest], [65536 x %struct.MemRequest]* %37, i64 0, i64 %39
  store volatile %struct.MemRequest* %40, %struct.MemRequest** %15, align 8
  %41 = load volatile %struct.MemRequest*, %struct.MemRequest** %15, align 8
  %42 = getelementptr inbounds %struct.MemRequest, %struct.MemRequest* %41, i32 0, i32 0
  %43 = load i64, i64* %12, align 8
  %44 = call i64 @atomic_exchange_u64(i64* %42, i64 %43) #24
  %45 = load volatile %struct.MemRequest*, %struct.MemRequest** %15, align 8
  %46 = getelementptr inbounds %struct.MemRequest, %struct.MemRequest* %45, i32 0, i32 1
  %47 = load i64, i64* %13, align 8
  %48 = call i64 @atomic_exchange_u64(i64* %46, i64 %47) #24
  br label %49

49:                                               ; preds = %54, %27
  %50 = load volatile %struct.MemRequest*, %struct.MemRequest** %15, align 8
  %51 = getelementptr inbounds %struct.MemRequest, %struct.MemRequest* %50, i32 0, i32 2
  %52 = load i8*, i8** %51, align 8
  %53 = icmp eq i8* %52, null
  br i1 %53, label %54, label %55

54:                                               ; preds = %49
  call void @system_memfence() #24
  br label %49, !llvm.loop !26

55:                                               ; preds = %49
  %56 = load volatile %struct.MemRequest*, %struct.MemRequest** %15, align 8
  %57 = getelementptr inbounds %struct.MemRequest, %struct.MemRequest* %56, i32 0, i32 2
  %58 = load i8*, i8** %57, align 8
  store i8* %58, i8** %10, align 8
  br label %59

59:                                               ; preds = %55, %23
  %60 = load i8*, i8** %10, align 8
  ret i8* %60
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_memory_allocate_aligned(%struct.LLVMRuntime addrspace(1)* %0, i64 %1, i64 %2) #4 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %10 = addrspacecast i64 addrspace(5)* %6 to i64*
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  %12 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %12, %struct.LLVMRuntime** %8, align 8
  %13 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %8, align 8
  store %struct.LLVMRuntime* %13, %struct.LLVMRuntime** %9, align 8
  store i64 %1, i64* %10, align 8
  store i64 %2, i64* %11, align 8
  %14 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %15 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %16 = load i64, i64* %10, align 8
  %17 = load i64, i64* %11, align 8
  %18 = call i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %15, i64 %16, i64 %17) #24
  call void @_ZN11LLVMRuntime10set_resultIPhEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %14, i64 31, i8* %18) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIPhEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, i8* %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast i8* addrspace(5)* %6 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store i8* %2, i8** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load i8*, i8** %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImPhET_T0_(i8* %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_get_mem_req_queue(%struct.LLVMRuntime addrspace(1)* %0) #4 {
  %2 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %2 to %struct.LLVMRuntime**
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %6, %struct.LLVMRuntime** %4, align 8
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %4, align 8
  store %struct.LLVMRuntime* %7, %struct.LLVMRuntime** %5, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %9 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %10 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %9, i32 0, i32 18
  %11 = load %struct.MemRequestQueue*, %struct.MemRequestQueue** %10, align 8
  call void @_ZN11LLVMRuntime10set_resultIP15MemRequestQueueEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %8, i64 0, %struct.MemRequestQueue* %11) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIP15MemRequestQueueEEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, %struct.MemRequestQueue* %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca %struct.MemRequestQueue*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast %struct.MemRequestQueue* addrspace(5)* %6 to %struct.MemRequestQueue**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store %struct.MemRequestQueue* %2, %struct.MemRequestQueue** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.MemRequestQueue*, %struct.MemRequestQueue** %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImP15MemRequestQueueET_T0_(%struct.MemRequestQueue* %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_initialize(i8 addrspace(1)* %0, i8 addrspace(1)* %1, i64 %2, i8 addrspace(1)* %3, i32 %4, i32 %5, i8 addrspace(1)* %6, i8 addrspace(1)* %7, i8 addrspace(1)* %8) #4 {
  %10 = alloca i8*, align 8, addrspace(5)
  %11 = alloca i8*, align 8, addrspace(5)
  %12 = alloca i8*, align 8, addrspace(5)
  %13 = alloca i8*, align 8, addrspace(5)
  %14 = alloca i8*, align 8, addrspace(5)
  %15 = alloca i8*, align 8, addrspace(5)
  %16 = alloca i8*, align 8, addrspace(5)
  %17 = alloca i8*, align 8, addrspace(5)
  %18 = alloca i64, align 8, addrspace(5)
  %19 = alloca i8*, align 8, addrspace(5)
  %20 = alloca i32, align 4, addrspace(5)
  %21 = alloca i32, align 4, addrspace(5)
  %22 = alloca i8*, align 8, addrspace(5)
  %23 = alloca i8*, align 8, addrspace(5)
  %24 = alloca i8*, align 8, addrspace(5)
  %25 = alloca i8* (i8*, i64, i64)*, align 8, addrspace(5)
  %26 = alloca void (i8*, ...)*, align 8, addrspace(5)
  %27 = alloca i32 (i8*, i64, i8*, i8*)*, align 8, addrspace(5)
  %28 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %29 = alloca i8*, align 8, addrspace(5)
  %30 = alloca i32, align 4, addrspace(5)
  %31 = addrspacecast i8* addrspace(5)* %10 to i8**
  %32 = addrspacecast i8* addrspace(5)* %11 to i8**
  %33 = addrspacecast i8* addrspace(5)* %12 to i8**
  %34 = addrspacecast i8* addrspace(5)* %13 to i8**
  %35 = addrspacecast i8* addrspace(5)* %14 to i8**
  %36 = addrspacecast i8* addrspace(5)* %15 to i8**
  %37 = addrspacecast i8* addrspace(5)* %16 to i8**
  %38 = addrspacecast i8* addrspace(5)* %17 to i8**
  %39 = addrspacecast i64 addrspace(5)* %18 to i64*
  %40 = addrspacecast i8* addrspace(5)* %19 to i8**
  %41 = addrspacecast i32 addrspace(5)* %20 to i32*
  %42 = addrspacecast i32 addrspace(5)* %21 to i32*
  %43 = addrspacecast i8* addrspace(5)* %22 to i8**
  %44 = addrspacecast i8* addrspace(5)* %23 to i8**
  %45 = addrspacecast i8* addrspace(5)* %24 to i8**
  %46 = addrspacecast i8* (i8*, i64, i64)* addrspace(5)* %25 to i8* (i8*, i64, i64)**
  %47 = addrspacecast void (i8*, ...)* addrspace(5)* %26 to void (i8*, ...)**
  %48 = addrspacecast i32 (i8*, i64, i8*, i8*)* addrspace(5)* %27 to i32 (i8*, i64, i8*, i8*)**
  %49 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %28 to %struct.LLVMRuntime**
  %50 = addrspacecast i8* addrspace(5)* %29 to i8**
  %51 = addrspacecast i32 addrspace(5)* %30 to i32*
  %52 = addrspacecast i8 addrspace(1)* %0 to i8*
  store i8* %52, i8** %31, align 8
  %53 = load i8*, i8** %31, align 8
  %54 = addrspacecast i8 addrspace(1)* %1 to i8*
  store i8* %54, i8** %32, align 8
  %55 = load i8*, i8** %32, align 8
  %56 = addrspacecast i8 addrspace(1)* %3 to i8*
  store i8* %56, i8** %33, align 8
  %57 = load i8*, i8** %33, align 8
  %58 = addrspacecast i8 addrspace(1)* %6 to i8*
  store i8* %58, i8** %34, align 8
  %59 = load i8*, i8** %34, align 8
  %60 = addrspacecast i8 addrspace(1)* %7 to i8*
  store i8* %60, i8** %35, align 8
  %61 = load i8*, i8** %35, align 8
  %62 = addrspacecast i8 addrspace(1)* %8 to i8*
  store i8* %62, i8** %36, align 8
  %63 = load i8*, i8** %36, align 8
  store i8* %53, i8** %37, align 8
  store i8* %55, i8** %38, align 8
  store i64 %2, i64* %39, align 8
  store i8* %57, i8** %40, align 8
  store i32 %4, i32* %41, align 4
  store i32 %5, i32* %42, align 4
  store i8* %59, i8** %43, align 8
  store i8* %61, i8** %44, align 8
  store i8* %63, i8** %45, align 8
  %64 = load i8*, i8** %43, align 8
  %65 = bitcast i8* %64 to i8* (i8*, i64, i64)*
  store i8* (i8*, i64, i64)* %65, i8* (i8*, i64, i64)** %46, align 8
  %66 = load i8*, i8** %44, align 8
  %67 = bitcast i8* %66 to void (i8*, ...)*
  store void (i8*, ...)* %67, void (i8*, ...)** %47, align 8
  %68 = load i8*, i8** %45, align 8
  %69 = bitcast i8* %68 to i32 (i8*, i64, i8*, i8*)*
  store i32 (i8*, i64, i8*, i8*)* %69, i32 (i8*, i64, i8*, i8*)** %48, align 8
  store %struct.LLVMRuntime* null, %struct.LLVMRuntime** %49, align 8
  %70 = load i8*, i8** %40, align 8
  %71 = load i64, i64* %39, align 8
  %72 = getelementptr inbounds i8, i8* %70, i64 %71
  store i8* %72, i8** %50, align 8
  %73 = load i64, i64* %39, align 8
  %74 = icmp ne i64 %73, 0
  br i1 %74, label %75, label %81

75:                                               ; preds = %9
  %76 = load i8*, i8** %40, align 8
  %77 = bitcast i8* %76 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %77, %struct.LLVMRuntime** %49, align 8
  %78 = call i64 @_ZN6taichi8iroundupImmvEET_S1_T0_(i64 35256, i64 4096) #24
  %79 = load i8*, i8** %40, align 8
  %80 = getelementptr inbounds i8, i8* %79, i64 %78
  store i8* %80, i8** %40, align 8
  br label %86

81:                                               ; preds = %9
  %82 = load i8* (i8*, i64, i64)*, i8* (i8*, i64, i64)** %46, align 8
  %83 = load i8*, i8** %38, align 8
  %84 = call i8* %82(i8* %83, i64 35256, i64 128) #24
  %85 = bitcast i8* %84 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %85, %struct.LLVMRuntime** %49, align 8
  br label %86

86:                                               ; preds = %81, %75
  %87 = load i64, i64* %39, align 8
  %88 = icmp ugt i64 %87, 0
  %89 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %90 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %89, i32 0, i32 0
  %91 = zext i1 %88 to i8
  store i8 %91, i8* %90, align 8
  %92 = load i8*, i8** %40, align 8
  %93 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %94 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %93, i32 0, i32 2
  store i8* %92, i8** %94, align 8
  %95 = load i8*, i8** %50, align 8
  %96 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %97 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %96, i32 0, i32 3
  store i8* %95, i8** %97, align 8
  %98 = load i8*, i8** %37, align 8
  %99 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %100 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %99, i32 0, i32 26
  store i8* %98, i8** %100, align 8
  %101 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %102 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  call void @_ZN11LLVMRuntime10set_resultIPS_EEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %101, i64 0, %struct.LLVMRuntime* %102) #24
  %103 = load i8* (i8*, i64, i64)*, i8* (i8*, i64, i64)** %46, align 8
  %104 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %105 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %104, i32 0, i32 4
  store i8* (i8*, i64, i64)* %103, i8* (i8*, i64, i64)** %105, align 8
  %106 = load void (i8*, ...)*, void (i8*, ...)** %47, align 8
  %107 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %108 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %107, i32 0, i32 6
  store void (i8*, ...)* %106, void (i8*, ...)** %108, align 8
  %109 = load i32 (i8*, i64, i8*, i8*)*, i32 (i8*, i64, i8*, i8*)** %48, align 8
  %110 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %111 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %110, i32 0, i32 7
  store i32 (i8*, i64, i8*, i8*)* %109, i32 (i8*, i64, i8*, i8*)** %111, align 8
  %112 = load i8*, i8** %38, align 8
  %113 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %114 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %113, i32 0, i32 8
  store i8* %112, i8** %114, align 8
  %115 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %116 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %115, i32 0, i32 29
  store i64 0, i64* %116, align 8
  %117 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %118 = call i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %117, i64 2097160, i64 4096) #24
  %119 = bitcast i8* %118 to %struct.MemRequestQueue*
  %120 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %121 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %120, i32 0, i32 18
  store %struct.MemRequestQueue* %119, %struct.MemRequestQueue** %121, align 8
  %122 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %123 = call i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %122, i64 1048576, i64 4096) #24
  %124 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %125 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %124, i32 0, i32 16
  store i8* %123, i8** %125, align 8
  %126 = load i32, i32* %42, align 4
  %127 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %128 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %127, i32 0, i32 28
  store i32 %126, i32* %128, align 4
  %129 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %130 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %131 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %130, i32 0, i32 28
  %132 = load i32, i32* %131, align 4
  %133 = sext i32 %132 to i64
  %134 = mul i64 20, %133
  %135 = call i8* @_ZN11LLVMRuntime16allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %129, i64 %134, i64 4096) #24
  %136 = bitcast i8* %135 to %struct.RandState*
  %137 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %138 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %137, i32 0, i32 17
  store %struct.RandState* %136, %struct.RandState** %138, align 8
  store i32 0, i32* %51, align 4
  br label %139

139:                                              ; preds = %155, %86
  %140 = load i32, i32* %51, align 4
  %141 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %142 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %141, i32 0, i32 28
  %143 = load i32, i32* %142, align 4
  %144 = icmp slt i32 %140, %143
  br i1 %144, label %145, label %158

145:                                              ; preds = %139
  %146 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %49, align 8
  %147 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %146, i32 0, i32 17
  %148 = load %struct.RandState*, %struct.RandState** %147, align 8
  %149 = load i32, i32* %51, align 4
  %150 = sext i32 %149 to i64
  %151 = getelementptr inbounds %struct.RandState, %struct.RandState* %148, i64 %150
  %152 = load i32, i32* %41, align 4
  %153 = load i32, i32* %51, align 4
  %154 = add nsw i32 %152, %153
  call void @initialize_rand_state(%struct.RandState* %151, i32 %154) #24
  br label %155

155:                                              ; preds = %145
  %156 = load i32, i32* %51, align 4
  %157 = add nsw i32 %156, 1
  store i32 %157, i32* %51, align 4
  br label %139, !llvm.loop !27

158:                                              ; preds = %139
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_ZN6taichi8iroundupImmvEET_S1_T0_(i64 %0, i64 %1) #2 comdat {
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i64 addrspace(5)* %4 to i64*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i64 %0, i64* %7, align 8
  store i64 %1, i64* %8, align 8
  %9 = load i64, i64* %7, align 8
  %10 = load i64, i64* %8, align 8
  %11 = add i64 %9, %10
  %12 = sub i64 %11, 1
  %13 = load i64, i64* %8, align 8
  %14 = udiv i64 %12, %13
  %15 = load i64, i64* %8, align 8
  %16 = mul i64 %14, %15
  ret i64 %16
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11LLVMRuntime10set_resultIPS_EEvmT_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, i64 %1, %struct.LLVMRuntime* %2) #2 comdat align 2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i64 %1, i64* %8, align 8
  store %struct.LLVMRuntime* %2, %struct.LLVMRuntime** %9, align 8
  %10 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %12 = call i64 @_Z38taichi_union_cast_with_different_sizesImP11LLVMRuntimeET_T0_(%struct.LLVMRuntime* %11) #24
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %10, i32 0, i32 26
  %14 = load i8*, i8** %13, align 8
  %15 = bitcast i8* %14 to i64*
  %16 = load i64, i64* %8, align 8
  %17 = getelementptr inbounds i64, i64* %15, i64 %16
  store i64 %12, i64* %17, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_initialize_snodes(%struct.LLVMRuntime addrspace(1)* %0, i64 %1, i32 %2, i32 %3, i32 %4, i64 %5, i8 addrspace(1)* %6, i1 %7) #4 {
  %9 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %10 = alloca i8*, align 8, addrspace(5)
  %11 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %12 = alloca i64, align 8, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca i32, align 4, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = alloca i64, align 8, addrspace(5)
  %17 = alloca i8*, align 8, addrspace(5)
  %18 = alloca i8, align 1, addrspace(5)
  %19 = alloca i32, align 4, addrspace(5)
  %20 = alloca i64, align 8, addrspace(5)
  %21 = alloca i32, align 4, addrspace(5)
  %22 = alloca %struct.Element, align 8, addrspace(5)
  %23 = alloca i32, align 4, addrspace(5)
  %24 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %9 to %struct.LLVMRuntime**
  %25 = addrspacecast i8* addrspace(5)* %10 to i8**
  %26 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %11 to %struct.LLVMRuntime**
  %27 = addrspacecast i64 addrspace(5)* %12 to i64*
  %28 = addrspacecast i32 addrspace(5)* %13 to i32*
  %29 = addrspacecast i32 addrspace(5)* %14 to i32*
  %30 = addrspacecast i32 addrspace(5)* %15 to i32*
  %31 = addrspacecast i64 addrspace(5)* %16 to i64*
  %32 = addrspacecast i8* addrspace(5)* %17 to i8**
  %33 = addrspacecast i8 addrspace(5)* %18 to i8*
  %34 = addrspacecast i32 addrspace(5)* %19 to i32*
  %35 = addrspacecast i64 addrspace(5)* %20 to i64*
  %36 = addrspacecast i32 addrspace(5)* %21 to i32*
  %37 = addrspacecast %struct.Element addrspace(5)* %22 to %struct.Element*
  %38 = addrspacecast i32 addrspace(5)* %23 to i32*
  %39 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %39, %struct.LLVMRuntime** %24, align 8
  %40 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  %41 = addrspacecast i8 addrspace(1)* %6 to i8*
  store i8* %41, i8** %25, align 8
  %42 = load i8*, i8** %25, align 8
  store %struct.LLVMRuntime* %40, %struct.LLVMRuntime** %26, align 8
  store i64 %1, i64* %27, align 8
  store i32 %2, i32* %28, align 4
  store i32 %3, i32* %29, align 4
  store i32 %4, i32* %30, align 4
  store i64 %5, i64* %31, align 8
  store i8* %42, i8** %32, align 8
  %43 = zext i1 %7 to i8
  store i8 %43, i8* %33, align 1
  %44 = load i64, i64* %31, align 8
  %45 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  %46 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %45, i32 0, i32 10
  %47 = load i32, i32* %30, align 4
  %48 = sext i32 %47 to i64
  %49 = getelementptr inbounds [512 x i64], [512 x i64]* %46, i64 0, i64 %48
  store i64 %44, i64* %49, align 8
  %50 = load i8*, i8** %32, align 8
  %51 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  %52 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %51, i32 0, i32 9
  %53 = load i32, i32* %30, align 4
  %54 = sext i32 %53 to i64
  %55 = getelementptr inbounds [512 x i8*], [512 x i8*]* %52, i64 0, i64 %54
  store i8* %50, i8** %55, align 8
  %56 = load i8, i8* %33, align 1
  %57 = trunc i8 %56 to i1
  br i1 %57, label %58, label %59

58:                                               ; preds = %8
  br label %110

59:                                               ; preds = %8
  %60 = load i32, i32* %28, align 4
  store i32 %60, i32* %34, align 4
  br label %61

61:                                               ; preds = %75, %59
  %62 = load i32, i32* %34, align 4
  %63 = load i32, i32* %28, align 4
  %64 = load i32, i32* %29, align 4
  %65 = add nsw i32 %63, %64
  %66 = icmp slt i32 %62, %65
  br i1 %66, label %67, label %78

67:                                               ; preds = %61
  %68 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  store i64 48, i64* %35, align 8
  store i32 65536, i32* %36, align 4
  %69 = call %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_miEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %68, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %26, i64* nonnull align 8 dereferenceable(8) %35, i32* nonnull align 4 dereferenceable(4) %36) #24
  %70 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  %71 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %70, i32 0, i32 13
  %72 = load i32, i32* %34, align 4
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %71, i64 0, i64 %73
  store %struct.ListManager* %69, %struct.ListManager** %74, align 8
  br label %75

75:                                               ; preds = %67
  %76 = load i32, i32* %34, align 4
  %77 = add nsw i32 %76, 1
  store i32 %77, i32* %34, align 4
  br label %61, !llvm.loop !28

78:                                               ; preds = %61
  %79 = getelementptr inbounds %struct.Element, %struct.Element* %37, i32 0, i32 1
  %80 = getelementptr inbounds [2 x i32], [2 x i32]* %79, i64 0, i64 0
  store i32 0, i32* %80, align 8
  %81 = getelementptr inbounds %struct.Element, %struct.Element* %37, i32 0, i32 1
  %82 = getelementptr inbounds [2 x i32], [2 x i32]* %81, i64 0, i64 1
  store i32 1, i32* %82, align 4
  %83 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  %84 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %83, i32 0, i32 9
  %85 = load i32, i32* %30, align 4
  %86 = sext i32 %85 to i64
  %87 = getelementptr inbounds [512 x i8*], [512 x i8*]* %84, i64 0, i64 %86
  %88 = load i8*, i8** %87, align 8
  %89 = getelementptr inbounds %struct.Element, %struct.Element* %37, i32 0, i32 0
  store i8* %88, i8** %89, align 8
  store i32 0, i32* %38, align 4
  br label %90

90:                                               ; preds = %99, %78
  %91 = load i32, i32* %38, align 4
  %92 = icmp slt i32 %91, 8
  br i1 %92, label %93, label %102

93:                                               ; preds = %90
  %94 = getelementptr inbounds %struct.Element, %struct.Element* %37, i32 0, i32 2
  %95 = getelementptr inbounds %struct.PhysicalCoordinates, %struct.PhysicalCoordinates* %94, i32 0, i32 0
  %96 = load i32, i32* %38, align 4
  %97 = sext i32 %96 to i64
  %98 = getelementptr inbounds [8 x i32], [8 x i32]* %95, i64 0, i64 %97
  store i32 0, i32* %98, align 4
  br label %99

99:                                               ; preds = %93
  %100 = load i32, i32* %38, align 4
  %101 = add nsw i32 %100, 1
  store i32 %101, i32* %38, align 4
  br label %90, !llvm.loop !29

102:                                              ; preds = %90
  %103 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %26, align 8
  %104 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %103, i32 0, i32 13
  %105 = load i32, i32* %28, align 4
  %106 = sext i32 %105 to i64
  %107 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %104, i64 0, i64 %106
  %108 = load %struct.ListManager*, %struct.ListManager** %107, align 8
  %109 = bitcast %struct.Element* %37 to i8*
  call void @_ZN11ListManager6appendEPv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %108, i8* %109) #24
  br label %110

110:                                              ; preds = %102, %58
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_miEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %1, i64* nonnull align 8 dereferenceable(8) %2, i32* nonnull align 4 dereferenceable(4) %3) #2 comdat align 2 {
  %5 = alloca %struct.ListManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %8 = alloca i64*, align 8, addrspace(5)
  %9 = alloca i32*, align 8, addrspace(5)
  %10 = alloca %struct.ListManager*, align 8, addrspace(5)
  %11 = addrspacecast %struct.ListManager* addrspace(5)* %5 to %struct.ListManager**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %7 to %struct.LLVMRuntime***
  %14 = addrspacecast i64* addrspace(5)* %8 to i64**
  %15 = addrspacecast i32* addrspace(5)* %9 to i32**
  %16 = addrspacecast %struct.ListManager* addrspace(5)* %10 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store %struct.LLVMRuntime** %1, %struct.LLVMRuntime*** %13, align 8
  store i64* %2, i64** %14, align 8
  store i32* %3, i32** %15, align 8
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %18 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %17, i64 1048616, i64 4096) #24
  %19 = bitcast i8* %18 to %struct.ListManager*
  store %struct.ListManager* %19, %struct.ListManager** %16, align 8
  %20 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  %21 = bitcast %struct.ListManager* %20 to i8*
  %22 = bitcast i8* %21 to %struct.ListManager*
  %23 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %13, align 8
  %24 = call nonnull align 8 dereferenceable(8) %struct.LLVMRuntime** @_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE(%struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %23) #24
  %25 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  %26 = load i64*, i64** %14, align 8
  %27 = call nonnull align 8 dereferenceable(8) i64* @_ZSt7forwardImEOT_RNSt16remove_referenceIS0_E4typeE(i64* nonnull align 8 dereferenceable(8) %26) #24
  %28 = load i64, i64* %27, align 8
  %29 = load i32*, i32** %15, align 8
  %30 = call nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIiEOT_RNSt16remove_referenceIS0_E4typeE(i32* nonnull align 4 dereferenceable(4) %29) #24
  %31 = load i32, i32* %30, align 4
  %32 = sext i32 %31 to i64
  call void @_ZN11ListManagerC2EP11LLVMRuntimemm(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %22, %struct.LLVMRuntime* %25, i64 %28, i64 %32) #24
  %33 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  ret %struct.ListManager* %33
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @_ZN11ListManager6appendEPv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i8* %1) #2 align 2 {
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast i8* addrspace(5)* %5 to i8**
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  store i8* %1, i8** %8, align 8
  %11 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %12 = call i8* @_ZN11ListManager8allocateEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %11) #24
  store i8* %12, i8** %9, align 8
  store i32 0, i32* %10, align 4
  br label %13

13:                                               ; preds = %29, %2
  %14 = load i32, i32* %10, align 4
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %11, i32 0, i32 1
  %17 = load i64, i64* %16, align 8
  %18 = icmp ult i64 %15, %17
  br i1 %18, label %19, label %32

19:                                               ; preds = %13
  %20 = load i8*, i8** %8, align 8
  %21 = load i32, i32* %10, align 4
  %22 = sext i32 %21 to i64
  %23 = getelementptr inbounds i8, i8* %20, i64 %22
  %24 = load i8, i8* %23, align 1
  %25 = load i8*, i8** %9, align 8
  %26 = load i32, i32* %10, align 4
  %27 = sext i32 %26 to i64
  %28 = getelementptr inbounds i8, i8* %25, i64 %27
  store i8 %24, i8* %28, align 1
  br label %29

29:                                               ; preds = %19
  %30 = load i32, i32* %10, align 4
  %31 = add nsw i32 %30, 1
  store i32 %31, i32* %10, align 4
  br label %13, !llvm.loop !30

32:                                               ; preds = %13
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @LLVMRuntime_initialize_thread_pool(%struct.LLVMRuntime* %0, i8* %1, i8* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %8 = addrspacecast i8* addrspace(5)* %5 to i8**
  %9 = addrspacecast i8* addrspace(5)* %6 to i8**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %7, align 8
  store i8* %1, i8** %8, align 8
  store i8* %2, i8** %9, align 8
  %10 = load i8*, i8** %8, align 8
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %12 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %11, i32 0, i32 11
  store i8* %10, i8** %12, align 8
  %13 = load i8*, i8** %9, align 8
  %14 = bitcast i8* %13 to void (i8*, i32, i32, i8*, void (i8*, i32, i32)*)*
  %15 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %16 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %15, i32 0, i32 12
  store void (i8*, i32, i32, i8*, void (i8*, i32, i32)*)* %14, void (i8*, i32, i32, i8*, void (i8*, i32, i32)*)** %16, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_NodeAllocator_initialize(%struct.LLVMRuntime addrspace(1)* %0, i32 %1, i64 %2) #4 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %10 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %14, %struct.LLVMRuntime** %9, align 8
  %15 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  store %struct.LLVMRuntime* %15, %struct.LLVMRuntime** %10, align 8
  store i32 %1, i32* %11, align 4
  store i64 %2, i64* %12, align 8
  %16 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %10, align 8
  store i32 16384, i32* %13, align 4
  %17 = call %struct.NodeManager* @_ZN11LLVMRuntime6createI11NodeManagerJRPS_RmiEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %16, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %10, i64* nonnull align 8 dereferenceable(8) %12, i32* nonnull align 4 dereferenceable(4) %13) #24
  %18 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %10, align 8
  %19 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %18, i32 0, i32 14
  %20 = load i32, i32* %11, align 4
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %19, i64 0, i64 %21
  store %struct.NodeManager* %17, %struct.NodeManager** %22, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden %struct.NodeManager* @_ZN11LLVMRuntime6createI11NodeManagerJRPS_RmiEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %1, i64* nonnull align 8 dereferenceable(8) %2, i32* nonnull align 4 dereferenceable(4) %3) #2 comdat align 2 {
  %5 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %8 = alloca i64*, align 8, addrspace(5)
  %9 = alloca i32*, align 8, addrspace(5)
  %10 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %11 = addrspacecast %struct.NodeManager* addrspace(5)* %5 to %struct.NodeManager**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %7 to %struct.LLVMRuntime***
  %14 = addrspacecast i64* addrspace(5)* %8 to i64**
  %15 = addrspacecast i32* addrspace(5)* %9 to i32**
  %16 = addrspacecast %struct.NodeManager* addrspace(5)* %10 to %struct.NodeManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store %struct.LLVMRuntime** %1, %struct.LLVMRuntime*** %13, align 8
  store i64* %2, i64** %14, align 8
  store i32* %3, i32** %15, align 8
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %18 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %17, i64 56, i64 4096) #24
  %19 = bitcast i8* %18 to %struct.NodeManager*
  store %struct.NodeManager* %19, %struct.NodeManager** %16, align 8
  %20 = load %struct.NodeManager*, %struct.NodeManager** %16, align 8
  %21 = bitcast %struct.NodeManager* %20 to i8*
  %22 = bitcast i8* %21 to %struct.NodeManager*
  %23 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %13, align 8
  %24 = call nonnull align 8 dereferenceable(8) %struct.LLVMRuntime** @_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE(%struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %23) #24
  %25 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  %26 = load i64*, i64** %14, align 8
  %27 = call nonnull align 8 dereferenceable(8) i64* @_ZSt7forwardIRmEOT_RNSt16remove_referenceIS1_E4typeE(i64* nonnull align 8 dereferenceable(8) %26) #24
  %28 = load i64, i64* %27, align 8
  %29 = trunc i64 %28 to i32
  %30 = load i32*, i32** %15, align 8
  %31 = call nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIiEOT_RNSt16remove_referenceIS0_E4typeE(i32* nonnull align 4 dereferenceable(4) %30) #24
  %32 = load i32, i32* %31, align 4
  call void @_ZN11NodeManagerC2EP11LLVMRuntimeii(%struct.NodeManager* nonnull align 8 dereferenceable(52) %22, %struct.LLVMRuntime* %25, i32 %29, i32 %32) #24
  %33 = load %struct.NodeManager*, %struct.NodeManager** %16, align 8
  ret %struct.NodeManager* %33
}

; Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
define protected amdgpu_kernel void @runtime_allocate_ambient(%struct.LLVMRuntime addrspace(1)* %0, i32 %1, i64 %2) #4 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %9 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  %12 = addrspacecast %struct.LLVMRuntime addrspace(1)* %0 to %struct.LLVMRuntime*
  store %struct.LLVMRuntime* %12, %struct.LLVMRuntime** %8, align 8
  %13 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %8, align 8
  store %struct.LLVMRuntime* %13, %struct.LLVMRuntime** %9, align 8
  store i32 %1, i32* %10, align 4
  store i64 %2, i64* %11, align 8
  %14 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %15 = load i64, i64* %11, align 8
  %16 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %14, i64 %15, i64 128) #24
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %9, align 8
  %18 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %17, i32 0, i32 15
  %19 = load i32, i32* %10, align 4
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds [1024 x i8*], [1024 x i8*]* %18, i64 0, i64 %20
  store i8* %16, i8** %21, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @mutex_lock_i32(i8* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = addrspacecast i8* addrspace(5)* %2 to i8**
  store i8* %0, i8** %3, align 8
  br label %4

4:                                                ; preds = %9, %1
  %5 = load i8*, i8** %3, align 8
  %6 = bitcast i8* %5 to i32*
  %7 = call i32 @atomic_exchange_i32(i32* %6, i32 1) #24
  %8 = icmp eq i32 %7, 1
  br i1 %8, label %9, label %10

9:                                                ; preds = %4
  br label %4, !llvm.loop !31

10:                                               ; preds = %4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @mutex_unlock_i32(i8* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = addrspacecast i8* addrspace(5)* %2 to i8**
  store i8* %0, i8** %3, align 8
  %4 = load i8*, i8** %3, align 8
  %5 = bitcast i8* %4 to i32*
  %6 = call i32 @atomic_exchange_i32(i32* %5, i32 0) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @ctlz_i32(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cttz_i32(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_compute_capability() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_ballot(i1 zeroext %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i8, align 1, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i8 addrspace(5)* %3 to i8*
  %6 = zext i1 %0 to i8
  store i8 %6, i8* %5, align 1
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_shfl_down_sync_i32(i32 %0, i32 %1, i32 %2, i32 %3) #2 {
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store i32 %1, i32* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_shfl_down_i32(i32 %0, i32 %1, i32 %2) #2 {
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32 %0, i32* %9, align 4
  store i32 %1, i32* %10, align 4
  store i32 %2, i32* %11, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @cuda_shfl_down_sync_f32(i32 %0, float %1, i32 %2, i32 %3) #2 {
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store float %1, float* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret float 0.000000e+00
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @cuda_shfl_down_f32(i32 %0, float %1, i32 %2) #2 {
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca float, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %4 to float*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast float addrspace(5)* %6 to float*
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32 %0, i32* %9, align 4
  store float %1, float* %10, align 4
  store i32 %2, i32* %11, align 4
  ret float 0.000000e+00
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_shfl_xor_sync_i32(i32 %0, i32 %1, i32 %2, i32 %3) #2 {
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store i32 %1, i32* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_shfl_up_sync_i32(i32 %0, i32 %1, i32 %2, i32 %3) #2 {
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store i32 %1, i32* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @cuda_shfl_up_sync_f32(i32 %0, float %1, i32 %2, i32 %3) #2 {
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store float %1, float* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret float 0.000000e+00
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_shfl_sync_i32(i32 %0, i32 %1, i32 %2, i32 %3) #2 {
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store i32 %1, i32* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @cuda_shfl_sync_f32(i32 %0, float %1, i32 %2, i32 %3) #2 {
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  %13 = addrspacecast i32 addrspace(5)* %8 to i32*
  %14 = addrspacecast i32 addrspace(5)* %9 to i32*
  store i32 %0, i32* %11, align 4
  store float %1, float* %12, align 4
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  ret float 0.000000e+00
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @cuda_all_sync(i32 %0, i1 zeroext %1) #2 {
  %3 = alloca i1, align 1, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i8, align 1, addrspace(5)
  %6 = addrspacecast i1 addrspace(5)* %3 to i1*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i8 addrspace(5)* %5 to i8*
  store i32 %0, i32* %7, align 4
  %9 = zext i1 %1 to i8
  store i8 %9, i8* %8, align 1
  ret i1 false
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_all_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ne i32 %10, 0
  %12 = call zeroext i1 @cuda_all_sync(i32 %9, i1 zeroext %11) #24
  %13 = zext i1 %12 to i32
  ret i32 %13
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @cuda_any_sync(i32 %0, i1 zeroext %1) #2 {
  %3 = alloca i1, align 1, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i8, align 1, addrspace(5)
  %6 = addrspacecast i1 addrspace(5)* %3 to i1*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i8 addrspace(5)* %5 to i8*
  store i32 %0, i32* %7, align 4
  %9 = zext i1 %1 to i8
  store i8 %9, i8* %8, align 1
  ret i1 false
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_any_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ne i32 %10, 0
  %12 = call zeroext i1 @cuda_any_sync(i32 %9, i1 zeroext %11) #24
  %13 = zext i1 %12 to i32
  ret i32 %13
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @cuda_uni_sync(i32 %0, i1 zeroext %1) #2 {
  %3 = alloca i1, align 1, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i8, align 1, addrspace(5)
  %6 = addrspacecast i1 addrspace(5)* %3 to i1*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i8 addrspace(5)* %5 to i8*
  store i32 %0, i32* %7, align 4
  %9 = zext i1 %1 to i8
  store i8 %9, i8* %8, align 1
  ret i1 false
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_uni_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ne i32 %10, 0
  %12 = call zeroext i1 @cuda_uni_sync(i32 %9, i1 zeroext %11) #24
  %13 = zext i1 %12 to i32
  ret i32 %13
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_ballot_sync(i32 %0, i1 zeroext %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i8, align 1, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i8 addrspace(5)* %5 to i8*
  store i32 %0, i32* %7, align 4
  %9 = zext i1 %1 to i8
  store i8 %9, i8* %8, align 1
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_ballot_i32(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast i32 addrspace(5)* %3 to i32*
  store i32 %0, i32* %5, align 4
  %6 = load i32, i32* %5, align 4
  %7 = icmp ne i32 %6, 0
  %8 = call i32 @cuda_ballot_sync(i32 -1, i1 zeroext %7) #24
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_ballot_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = icmp ne i32 %10, 0
  %12 = call i32 @cuda_ballot_sync(i32 %9, i1 zeroext %11) #24
  ret i32 %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_match_any_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_match_all_sync_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_match_any_sync_i64(i32 %0, i64 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i64 addrspace(5)* %5 to i64*
  store i32 %0, i32* %7, align 4
  store i64 %1, i64* %8, align 8
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @cuda_active_mask() #2 {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = addrspacecast i32 addrspace(5)* %1 to i32*
  ret i32 0
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @block_barrier() #2 {
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @warp_barrier(i32 %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = addrspacecast i32 addrspace(5)* %2 to i32*
  store i32 %0, i32* %3, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @block_memfence() #2 {
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @grid_memfence() #2 {
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_add_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = add nsw i32 %9, %10
  ret i32 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @op_add_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = load float, float* %7, align 4
  %10 = load float, float* %8, align 4
  %11 = fadd contract float %9, %10
  ret float %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_min_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %7, i32* nonnull align 4 dereferenceable(4) %8) #24
  %10 = load i32, i32* %9, align 4
  ret i32 %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %0, i32* nonnull align 4 dereferenceable(4) %1) #2 comdat {
  %3 = alloca i32*, align 8, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32*, align 8, addrspace(5)
  %6 = addrspacecast i32* addrspace(5)* %3 to i32**
  %7 = addrspacecast i32* addrspace(5)* %4 to i32**
  %8 = addrspacecast i32* addrspace(5)* %5 to i32**
  store i32* %0, i32** %7, align 8
  store i32* %1, i32** %8, align 8
  %9 = load i32*, i32** %7, align 8
  %10 = load i32, i32* %9, align 4
  %11 = load i32*, i32** %8, align 8
  %12 = load i32, i32* %11, align 4
  %13 = icmp slt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i32*, i32** %7, align 8
  br label %18

16:                                               ; preds = %2
  %17 = load i32*, i32** %8, align 8
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i32* [ %15, %14 ], [ %17, %16 ]
  ret i32* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @op_min_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = call nonnull align 4 dereferenceable(4) float* @_ZSt3minIfEUa9enable_ifILb1EERKT_S2_S2_(float* nonnull align 4 dereferenceable(4) %7, float* nonnull align 4 dereferenceable(4) %8) #24
  %10 = load float, float* %9, align 4
  ret float %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 4 dereferenceable(4) float* @_ZSt3minIfEUa9enable_ifILb1EERKT_S2_S2_(float* nonnull align 4 dereferenceable(4) %0, float* nonnull align 4 dereferenceable(4) %1) #2 comdat {
  %3 = alloca float*, align 8, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float*, align 8, addrspace(5)
  %6 = addrspacecast float* addrspace(5)* %3 to float**
  %7 = addrspacecast float* addrspace(5)* %4 to float**
  %8 = addrspacecast float* addrspace(5)* %5 to float**
  store float* %0, float** %7, align 8
  store float* %1, float** %8, align 8
  %9 = load float*, float** %7, align 8
  %10 = load float, float* %9, align 4
  %11 = load float*, float** %8, align 8
  %12 = load float, float* %11, align 4
  %13 = fcmp contract olt float %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load float*, float** %7, align 8
  br label %18

16:                                               ; preds = %2
  %17 = load float*, float** %8, align 8
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi float* [ %15, %14 ], [ %17, %16 ]
  ret float* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_max_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3maxIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %7, i32* nonnull align 4 dereferenceable(4) %8) #24
  %10 = load i32, i32* %9, align 4
  ret i32 %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 4 dereferenceable(4) i32* @_ZSt3maxIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %0, i32* nonnull align 4 dereferenceable(4) %1) #2 comdat {
  %3 = alloca i32*, align 8, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32*, align 8, addrspace(5)
  %6 = addrspacecast i32* addrspace(5)* %3 to i32**
  %7 = addrspacecast i32* addrspace(5)* %4 to i32**
  %8 = addrspacecast i32* addrspace(5)* %5 to i32**
  store i32* %0, i32** %7, align 8
  store i32* %1, i32** %8, align 8
  %9 = load i32*, i32** %7, align 8
  %10 = load i32, i32* %9, align 4
  %11 = load i32*, i32** %8, align 8
  %12 = load i32, i32* %11, align 4
  %13 = icmp slt i32 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i32*, i32** %8, align 8
  br label %18

16:                                               ; preds = %2
  %17 = load i32*, i32** %7, align 8
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i32* [ %15, %14 ], [ %17, %16 ]
  ret i32* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @op_max_f32(float %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = addrspacecast float addrspace(5)* %3 to float*
  %7 = addrspacecast float addrspace(5)* %4 to float*
  %8 = addrspacecast float addrspace(5)* %5 to float*
  store float %0, float* %7, align 4
  store float %1, float* %8, align 4
  %9 = call nonnull align 4 dereferenceable(4) float* @_ZSt3maxIfEUa9enable_ifILb1EERKT_S2_S2_(float* nonnull align 4 dereferenceable(4) %7, float* nonnull align 4 dereferenceable(4) %8) #24
  %10 = load float, float* %9, align 4
  ret float %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 4 dereferenceable(4) float* @_ZSt3maxIfEUa9enable_ifILb1EERKT_S2_S2_(float* nonnull align 4 dereferenceable(4) %0, float* nonnull align 4 dereferenceable(4) %1) #2 comdat {
  %3 = alloca float*, align 8, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float*, align 8, addrspace(5)
  %6 = addrspacecast float* addrspace(5)* %3 to float**
  %7 = addrspacecast float* addrspace(5)* %4 to float**
  %8 = addrspacecast float* addrspace(5)* %5 to float**
  store float* %0, float** %7, align 8
  store float* %1, float** %8, align 8
  %9 = load float*, float** %7, align 8
  %10 = load float, float* %9, align 4
  %11 = load float*, float** %8, align 8
  %12 = load float, float* %11, align 4
  %13 = fcmp contract olt float %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load float*, float** %8, align 8
  br label %18

16:                                               ; preds = %2
  %17 = load float*, float** %7, align 8
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi float* [ %15, %14 ], [ %17, %16 ]
  ret float* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_and_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = and i32 %9, %10
  ret i32 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_or_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = or i32 %9, %10
  ret i32 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @op_xor_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store i32 %0, i32* %7, align 4
  store i32 %1, i32* %8, align 4
  %9 = load i32, i32* %7, align 4
  %10 = load i32, i32* %8, align 4
  %11 = xor i32 %9, %10
  ret i32 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_add_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_add_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !32

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_add_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_add_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_add_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_add_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @warp_reduce_add_f32(i32 %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast float addrspace(5)* %3 to float*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast float addrspace(5)* %5 to float*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store float %1, float* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load float, float* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load float, float* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call contract float @cuda_shfl_down_sync_f32(i32 %16, float %17, i32 %18, i32 31) #24
  %20 = call contract float @op_add_f32(float %15, float %19) #24
  store float %20, float* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !33

24:                                               ; preds = %11
  %25 = load float, float* %9, align 4
  ret float %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @reduce_add_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %3 to float*
  %9 = addrspacecast float* addrspace(5)* %4 to float**
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  store float* %0, float** %9, align 8
  store float %1, float* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load float*, float** %9, align 8
  %18 = load float, float* %10, align 4
  %19 = call contract float @atomic_add_f32(float* %17, float %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load float, float* %10, align 4
  %22 = call contract float @warp_reduce_add_f32(i32 -1, float %21) #24
  store float %22, float* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load float*, float** %9, align 8
  %30 = load float, float* %12, align 4
  %31 = call contract float @atomic_add_f32(float* %29, float %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load float, float* %10, align 4
  ret float %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_min_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_min_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !34

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_min_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_min_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_min_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_min_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @warp_reduce_min_f32(i32 %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast float addrspace(5)* %3 to float*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast float addrspace(5)* %5 to float*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store float %1, float* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load float, float* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load float, float* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call contract float @cuda_shfl_down_sync_f32(i32 %16, float %17, i32 %18, i32 31) #24
  %20 = call contract float @op_min_f32(float %15, float %19) #24
  store float %20, float* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !35

24:                                               ; preds = %11
  %25 = load float, float* %9, align 4
  ret float %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @reduce_min_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %3 to float*
  %9 = addrspacecast float* addrspace(5)* %4 to float**
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  store float* %0, float** %9, align 8
  store float %1, float* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load float*, float** %9, align 8
  %18 = load float, float* %10, align 4
  %19 = call contract float @atomic_min_f32(float* %17, float %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load float, float* %10, align 4
  %22 = call contract float @warp_reduce_min_f32(i32 -1, float %21) #24
  store float %22, float* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load float*, float** %9, align 8
  %30 = load float, float* %12, align 4
  %31 = call contract float @atomic_min_f32(float* %29, float %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load float, float* %10, align 4
  ret float %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_max_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_max_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !36

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_max_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_max_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_max_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_max_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @warp_reduce_max_f32(i32 %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast float addrspace(5)* %3 to float*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast float addrspace(5)* %5 to float*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store float %1, float* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load float, float* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load float, float* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call contract float @cuda_shfl_down_sync_f32(i32 %16, float %17, i32 %18, i32 31) #24
  %20 = call contract float @op_max_f32(float %15, float %19) #24
  store float %20, float* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !37

24:                                               ; preds = %11
  %25 = load float, float* %9, align 4
  ret float %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @reduce_max_f32(float* %0, float %1) #2 {
  %3 = alloca float, align 4, addrspace(5)
  %4 = alloca float*, align 8, addrspace(5)
  %5 = alloca float, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca float, align 4, addrspace(5)
  %8 = addrspacecast float addrspace(5)* %3 to float*
  %9 = addrspacecast float* addrspace(5)* %4 to float**
  %10 = addrspacecast float addrspace(5)* %5 to float*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast float addrspace(5)* %7 to float*
  store float* %0, float** %9, align 8
  store float %1, float* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load float*, float** %9, align 8
  %18 = load float, float* %10, align 4
  %19 = call contract float @atomic_max_f32(float* %17, float %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load float, float* %10, align 4
  %22 = call contract float @warp_reduce_max_f32(i32 -1, float %21) #24
  store float %22, float* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load float*, float** %9, align 8
  %30 = load float, float* %12, align 4
  %31 = call contract float @atomic_max_f32(float* %29, float %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load float, float* %10, align 4
  ret float %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_and_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_and_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !38

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_and_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_and_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_and_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_and_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_or_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_or_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !39

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_or_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_or_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_or_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_or_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @warp_reduce_xor_i32(i32 %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i32 addrspace(5)* %3 to i32*
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i32 %0, i32* %8, align 4
  store i32 %1, i32* %9, align 4
  store i32 16, i32* %10, align 4
  br label %11

11:                                               ; preds = %21, %2
  %12 = load i32, i32* %10, align 4
  %13 = icmp sgt i32 %12, 0
  br i1 %13, label %14, label %24

14:                                               ; preds = %11
  %15 = load i32, i32* %9, align 4
  %16 = load i32, i32* %8, align 4
  %17 = load i32, i32* %9, align 4
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @cuda_shfl_down_sync_i32(i32 %16, i32 %17, i32 %18, i32 31) #24
  %20 = call i32 @op_xor_i32(i32 %15, i32 %19) #24
  store i32 %20, i32* %9, align 4
  br label %21

21:                                               ; preds = %14
  %22 = load i32, i32* %10, align 4
  %23 = sdiv i32 %22, 2
  store i32 %23, i32* %10, align 4
  br label %11, !llvm.loop !40

24:                                               ; preds = %11
  %25 = load i32, i32* %9, align 4
  ret i32 %25
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @reduce_xor_i32(i32* %0, i32 %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %3 to i32*
  %9 = addrspacecast i32* addrspace(5)* %4 to i32**
  %10 = addrspacecast i32 addrspace(5)* %5 to i32*
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %9, align 8
  store i32 %1, i32* %10, align 4
  %13 = call i32 @cuda_active_mask() #24
  store i32 %13, i32* %11, align 4
  %14 = load i32, i32* %11, align 4
  %15 = icmp ne i32 %14, -1
  br i1 %15, label %16, label %20

16:                                               ; preds = %2
  %17 = load i32*, i32** %9, align 8
  %18 = load i32, i32* %10, align 4
  %19 = call i32 @atomic_xor_i32(i32* %17, i32 %18) #24
  br label %33

20:                                               ; preds = %2
  %21 = load i32, i32* %10, align 4
  %22 = call i32 @warp_reduce_xor_i32(i32 -1, i32 %21) #24
  store i32 %22, i32* %12, align 4
  %23 = call i32 @thread_idx() #24
  %24 = call i32 @warp_size() #24
  %25 = sub nsw i32 %24, 1
  %26 = and i32 %23, %25
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %28, label %32

28:                                               ; preds = %20
  %29 = load i32*, i32** %9, align 8
  %30 = load i32, i32* %12, align 4
  %31 = call i32 @atomic_xor_i32(i32* %29, i32 %30) #24
  br label %32

32:                                               ; preds = %28, %20
  br label %33

33:                                               ; preds = %32, %16
  %34 = load i32, i32* %10, align 4
  ret i32 %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @clear_list(%struct.LLVMRuntime* %0, %struct.StructMeta* %1, %struct.StructMeta* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %6 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %7 = alloca %struct.ListManager*, align 8, addrspace(5)
  %8 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %9 = addrspacecast %struct.StructMeta* addrspace(5)* %5 to %struct.StructMeta**
  %10 = addrspacecast %struct.StructMeta* addrspace(5)* %6 to %struct.StructMeta**
  %11 = addrspacecast %struct.ListManager* addrspace(5)* %7 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %8, align 8
  store %struct.StructMeta* %1, %struct.StructMeta** %9, align 8
  store %struct.StructMeta* %2, %struct.StructMeta** %10, align 8
  %12 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %8, align 8
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %12, i32 0, i32 13
  %14 = load %struct.StructMeta*, %struct.StructMeta** %10, align 8
  %15 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %14, i32 0, i32 0
  %16 = load i32, i32* %15, align 8
  %17 = sext i32 %16 to i64
  %18 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %13, i64 0, i64 %17
  %19 = load %struct.ListManager*, %struct.ListManager** %18, align 8
  store %struct.ListManager* %19, %struct.ListManager** %11, align 8
  %20 = load %struct.ListManager*, %struct.ListManager** %11, align 8
  call void @_ZN11ListManager5clearEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %20) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11ListManager5clearEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0) #2 comdat align 2 {
  %2 = alloca %struct.ListManager*, align 8, addrspace(5)
  %3 = addrspacecast %struct.ListManager* addrspace(5)* %2 to %struct.ListManager**
  store %struct.ListManager* %0, %struct.ListManager** %3, align 8
  %4 = load %struct.ListManager*, %struct.ListManager** %3, align 8
  %5 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %4, i32 0, i32 5
  store i32 0, i32* %5, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @element_listgen_root(%struct.LLVMRuntime* %0, %struct.StructMeta* %1, %struct.StructMeta* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %6 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %7 = alloca %struct.ListManager*, align 8, addrspace(5)
  %8 = alloca %struct.ListManager*, align 8, addrspace(5)
  %9 = alloca i8* (i8*, i8*, i32)*, align 8, addrspace(5)
  %10 = alloca i32 (i8*, i8*)*, align 8, addrspace(5)
  %11 = alloca i8* (i8*)*, align 8, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca %struct.Element, align 8, addrspace(5)
  %15 = alloca i8*, align 8, addrspace(5)
  %16 = alloca i32, align 4, addrspace(5)
  %17 = alloca i32, align 4, addrspace(5)
  %18 = alloca i32, align 4, addrspace(5)
  %19 = alloca %struct.Element, align 8, addrspace(5)
  %20 = alloca i32, align 4, addrspace(5)
  %21 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %22 = addrspacecast %struct.StructMeta* addrspace(5)* %5 to %struct.StructMeta**
  %23 = addrspacecast %struct.StructMeta* addrspace(5)* %6 to %struct.StructMeta**
  %24 = addrspacecast %struct.ListManager* addrspace(5)* %7 to %struct.ListManager**
  %25 = addrspacecast %struct.ListManager* addrspace(5)* %8 to %struct.ListManager**
  %26 = addrspacecast i8* (i8*, i8*, i32)* addrspace(5)* %9 to i8* (i8*, i8*, i32)**
  %27 = addrspacecast i32 (i8*, i8*)* addrspace(5)* %10 to i32 (i8*, i8*)**
  %28 = addrspacecast i8* (i8*)* addrspace(5)* %11 to i8* (i8*)**
  %29 = addrspacecast i32 addrspace(5)* %12 to i32*
  %30 = addrspacecast i32 addrspace(5)* %13 to i32*
  %31 = addrspacecast %struct.Element addrspace(5)* %14 to %struct.Element*
  %32 = addrspacecast i8* addrspace(5)* %15 to i8**
  %33 = addrspacecast i32 addrspace(5)* %16 to i32*
  %34 = addrspacecast i32 addrspace(5)* %17 to i32*
  %35 = addrspacecast i32 addrspace(5)* %18 to i32*
  %36 = addrspacecast %struct.Element addrspace(5)* %19 to %struct.Element*
  %37 = addrspacecast i32 addrspace(5)* %20 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %21, align 8
  store %struct.StructMeta* %1, %struct.StructMeta** %22, align 8
  store %struct.StructMeta* %2, %struct.StructMeta** %23, align 8
  %38 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %21, align 8
  %39 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %38, i32 0, i32 13
  %40 = load %struct.StructMeta*, %struct.StructMeta** %22, align 8
  %41 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %40, i32 0, i32 0
  %42 = load i32, i32* %41, align 8
  %43 = sext i32 %42 to i64
  %44 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %39, i64 0, i64 %43
  %45 = load %struct.ListManager*, %struct.ListManager** %44, align 8
  store %struct.ListManager* %45, %struct.ListManager** %24, align 8
  %46 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %21, align 8
  %47 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %46, i32 0, i32 13
  %48 = load %struct.StructMeta*, %struct.StructMeta** %23, align 8
  %49 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %48, i32 0, i32 0
  %50 = load i32, i32* %49, align 8
  %51 = sext i32 %50 to i64
  %52 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %47, i64 0, i64 %51
  %53 = load %struct.ListManager*, %struct.ListManager** %52, align 8
  store %struct.ListManager* %53, %struct.ListManager** %25, align 8
  %54 = load %struct.StructMeta*, %struct.StructMeta** %22, align 8
  %55 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %54, i32 0, i32 3
  %56 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %55, align 8
  store i8* (i8*, i8*, i32)* %56, i8* (i8*, i8*, i32)** %26, align 8
  %57 = load %struct.StructMeta*, %struct.StructMeta** %23, align 8
  %58 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %57, i32 0, i32 6
  %59 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %58, align 8
  store i32 (i8*, i8*)* %59, i32 (i8*, i8*)** %27, align 8
  %60 = load %struct.StructMeta*, %struct.StructMeta** %23, align 8
  %61 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %60, i32 0, i32 4
  %62 = load i8* (i8*)*, i8* (i8*)** %61, align 8
  store i8* (i8*)* %62, i8* (i8*)** %28, align 8
  %63 = call i32 @block_dim() #24
  %64 = call i32 @block_idx() #24
  %65 = mul nsw i32 %63, %64
  %66 = call i32 @thread_idx() #24
  %67 = add nsw i32 %65, %66
  store i32 %67, i32* %29, align 4
  %68 = call i32 @grid_dim() #24
  %69 = call i32 @block_dim() #24
  %70 = mul nsw i32 %68, %69
  store i32 %70, i32* %30, align 4
  %71 = load %struct.ListManager*, %struct.ListManager** %24, align 8
  %72 = call nonnull align 8 dereferenceable(48) %struct.Element* @_ZN11ListManager3getI7ElementEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %71, i32 0) #24
  %73 = bitcast %struct.Element* %31 to i8*
  %74 = bitcast %struct.Element* %72 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %73, i8* align 8 %74, i64 48, i1 false)
  %75 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %26, align 8
  %76 = load %struct.StructMeta*, %struct.StructMeta** %22, align 8
  %77 = bitcast %struct.StructMeta* %76 to i8*
  %78 = getelementptr inbounds %struct.Element, %struct.Element* %31, i32 0, i32 0
  %79 = load i8*, i8** %78, align 8
  %80 = call i8* %75(i8* %77, i8* %79, i32 0) #24
  store i8* %80, i8** %32, align 8
  %81 = load i8* (i8*)*, i8* (i8*)** %28, align 8
  %82 = load i8*, i8** %32, align 8
  %83 = call i8* %81(i8* %82) #24
  store i8* %83, i8** %32, align 8
  %84 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %27, align 8
  %85 = load %struct.StructMeta*, %struct.StructMeta** %23, align 8
  %86 = bitcast %struct.StructMeta* %85 to i8*
  %87 = load i8*, i8** %32, align 8
  %88 = call i32 %84(i8* %86, i8* %87) #24
  store i32 %88, i32* %33, align 4
  %89 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %33, i32* nonnull align 4 dereferenceable(4) addrspacecast (i32 addrspace(4)* @_ZL31taichi_listgen_max_element_size to i32*)) #24
  %90 = load i32, i32* %89, align 4
  store i32 %90, i32* %34, align 4
  %91 = load i32, i32* %29, align 4
  store i32 %91, i32* %35, align 4
  br label %92

92:                                               ; preds = %120, %3
  %93 = load i32, i32* %35, align 4
  %94 = load i32, i32* %34, align 4
  %95 = mul nsw i32 %93, %94
  %96 = load i32, i32* %33, align 4
  %97 = icmp slt i32 %95, %96
  br i1 %97, label %98, label %124

98:                                               ; preds = %92
  %99 = load i8*, i8** %32, align 8
  %100 = getelementptr inbounds %struct.Element, %struct.Element* %36, i32 0, i32 0
  store i8* %99, i8** %100, align 8
  %101 = load i32, i32* %35, align 4
  %102 = load i32, i32* %34, align 4
  %103 = mul nsw i32 %101, %102
  %104 = getelementptr inbounds %struct.Element, %struct.Element* %36, i32 0, i32 1
  %105 = getelementptr inbounds [2 x i32], [2 x i32]* %104, i64 0, i64 0
  store i32 %103, i32* %105, align 8
  %106 = load i32, i32* %35, align 4
  %107 = add nsw i32 %106, 1
  %108 = load i32, i32* %34, align 4
  %109 = mul nsw i32 %107, %108
  store i32 %109, i32* %37, align 4
  %110 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %37, i32* nonnull align 4 dereferenceable(4) %33) #24
  %111 = load i32, i32* %110, align 4
  %112 = getelementptr inbounds %struct.Element, %struct.Element* %36, i32 0, i32 1
  %113 = getelementptr inbounds [2 x i32], [2 x i32]* %112, i64 0, i64 1
  store i32 %111, i32* %113, align 4
  %114 = getelementptr inbounds %struct.Element, %struct.Element* %31, i32 0, i32 2
  %115 = getelementptr inbounds %struct.Element, %struct.Element* %36, i32 0, i32 2
  %116 = bitcast %struct.PhysicalCoordinates* %115 to i8*
  %117 = bitcast %struct.PhysicalCoordinates* %114 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %116, i8* align 8 %117, i64 32, i1 false)
  %118 = load %struct.ListManager*, %struct.ListManager** %25, align 8
  %119 = bitcast %struct.Element* %36 to i8*
  call void @_ZN11ListManager6appendEPv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %118, i8* %119) #24
  br label %120

120:                                              ; preds = %98
  %121 = load i32, i32* %30, align 4
  %122 = load i32, i32* %35, align 4
  %123 = add nsw i32 %122, %121
  store i32 %123, i32* %35, align 4
  br label %92, !llvm.loop !41

124:                                              ; preds = %92
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 8 dereferenceable(48) %struct.Element* @_ZN11ListManager3getI7ElementEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca %struct.Element*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %struct.Element* addrspace(5)* %3 to %struct.Element**
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %9, i32 %10) #24
  %12 = bitcast i8* %11 to %struct.Element*
  ret %struct.Element* %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @element_listgen_nonroot(%struct.LLVMRuntime* %0, %struct.StructMeta* %1, %struct.StructMeta* %2) #2 {
  %4 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %5 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %6 = alloca %struct.StructMeta*, align 8, addrspace(5)
  %7 = alloca %struct.ListManager*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca %struct.ListManager*, align 8, addrspace(5)
  %10 = alloca void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, align 8, addrspace(5)
  %11 = alloca i32 (i8*, i8*, i32)*, align 8, addrspace(5)
  %12 = alloca i8* (i8*, i8*, i32)*, align 8, addrspace(5)
  %13 = alloca i32 (i8*, i8*)*, align 8, addrspace(5)
  %14 = alloca i8* (i8*)*, align 8, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = alloca i32, align 4, addrspace(5)
  %17 = alloca i32, align 4, addrspace(5)
  %18 = alloca i32, align 4, addrspace(5)
  %19 = alloca i32, align 4, addrspace(5)
  %20 = alloca %struct.Element, align 8, addrspace(5)
  %21 = alloca i32, align 4, addrspace(5)
  %22 = alloca i32, align 4, addrspace(5)
  %23 = alloca i32, align 4, addrspace(5)
  %24 = alloca %struct.PhysicalCoordinates, align 4, addrspace(5)
  %25 = alloca i8*, align 8, addrspace(5)
  %26 = alloca i32, align 4, addrspace(5)
  %27 = alloca i32, align 4, addrspace(5)
  %28 = alloca i32, align 4, addrspace(5)
  %29 = alloca %struct.Element, align 8, addrspace(5)
  %30 = alloca i32, align 4, addrspace(5)
  %31 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %4 to %struct.LLVMRuntime**
  %32 = addrspacecast %struct.StructMeta* addrspace(5)* %5 to %struct.StructMeta**
  %33 = addrspacecast %struct.StructMeta* addrspace(5)* %6 to %struct.StructMeta**
  %34 = addrspacecast %struct.ListManager* addrspace(5)* %7 to %struct.ListManager**
  %35 = addrspacecast i32 addrspace(5)* %8 to i32*
  %36 = addrspacecast %struct.ListManager* addrspace(5)* %9 to %struct.ListManager**
  %37 = addrspacecast void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* addrspace(5)* %10 to void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)**
  %38 = addrspacecast i32 (i8*, i8*, i32)* addrspace(5)* %11 to i32 (i8*, i8*, i32)**
  %39 = addrspacecast i8* (i8*, i8*, i32)* addrspace(5)* %12 to i8* (i8*, i8*, i32)**
  %40 = addrspacecast i32 (i8*, i8*)* addrspace(5)* %13 to i32 (i8*, i8*)**
  %41 = addrspacecast i8* (i8*)* addrspace(5)* %14 to i8* (i8*)**
  %42 = addrspacecast i32 addrspace(5)* %15 to i32*
  %43 = addrspacecast i32 addrspace(5)* %16 to i32*
  %44 = addrspacecast i32 addrspace(5)* %17 to i32*
  %45 = addrspacecast i32 addrspace(5)* %18 to i32*
  %46 = addrspacecast i32 addrspace(5)* %19 to i32*
  %47 = addrspacecast %struct.Element addrspace(5)* %20 to %struct.Element*
  %48 = addrspacecast i32 addrspace(5)* %21 to i32*
  %49 = addrspacecast i32 addrspace(5)* %22 to i32*
  %50 = addrspacecast i32 addrspace(5)* %23 to i32*
  %51 = addrspacecast %struct.PhysicalCoordinates addrspace(5)* %24 to %struct.PhysicalCoordinates*
  %52 = addrspacecast i8* addrspace(5)* %25 to i8**
  %53 = addrspacecast i32 addrspace(5)* %26 to i32*
  %54 = addrspacecast i32 addrspace(5)* %27 to i32*
  %55 = addrspacecast i32 addrspace(5)* %28 to i32*
  %56 = addrspacecast %struct.Element addrspace(5)* %29 to %struct.Element*
  %57 = addrspacecast i32 addrspace(5)* %30 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %31, align 8
  store %struct.StructMeta* %1, %struct.StructMeta** %32, align 8
  store %struct.StructMeta* %2, %struct.StructMeta** %33, align 8
  %58 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %31, align 8
  %59 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %58, i32 0, i32 13
  %60 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %61 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %60, i32 0, i32 0
  %62 = load i32, i32* %61, align 8
  %63 = sext i32 %62 to i64
  %64 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %59, i64 0, i64 %63
  %65 = load %struct.ListManager*, %struct.ListManager** %64, align 8
  store %struct.ListManager* %65, %struct.ListManager** %34, align 8
  %66 = load %struct.ListManager*, %struct.ListManager** %34, align 8
  %67 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %66) #24
  store i32 %67, i32* %35, align 4
  %68 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %31, align 8
  %69 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %68, i32 0, i32 13
  %70 = load %struct.StructMeta*, %struct.StructMeta** %33, align 8
  %71 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %70, i32 0, i32 0
  %72 = load i32, i32* %71, align 8
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %69, i64 0, i64 %73
  %75 = load %struct.ListManager*, %struct.ListManager** %74, align 8
  store %struct.ListManager* %75, %struct.ListManager** %36, align 8
  %76 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %77 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %76, i32 0, i32 7
  %78 = load void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %77, align 8
  store void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)* %78, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %37, align 8
  %79 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %80 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %79, i32 0, i32 5
  %81 = load i32 (i8*, i8*, i32)*, i32 (i8*, i8*, i32)** %80, align 8
  store i32 (i8*, i8*, i32)* %81, i32 (i8*, i8*, i32)** %38, align 8
  %82 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %83 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %82, i32 0, i32 3
  %84 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %83, align 8
  store i8* (i8*, i8*, i32)* %84, i8* (i8*, i8*, i32)** %39, align 8
  %85 = load %struct.StructMeta*, %struct.StructMeta** %33, align 8
  %86 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %85, i32 0, i32 6
  %87 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %86, align 8
  store i32 (i8*, i8*)* %87, i32 (i8*, i8*)** %40, align 8
  %88 = load %struct.StructMeta*, %struct.StructMeta** %33, align 8
  %89 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %88, i32 0, i32 4
  %90 = load i8* (i8*)*, i8* (i8*)** %89, align 8
  store i8* (i8*)* %90, i8* (i8*)** %41, align 8
  %91 = call i32 @block_idx() #24
  store i32 %91, i32* %42, align 4
  %92 = call i32 @grid_dim() #24
  store i32 %92, i32* %43, align 4
  %93 = call i32 @thread_idx() #24
  store i32 %93, i32* %44, align 4
  %94 = call i32 @block_dim() #24
  store i32 %94, i32* %45, align 4
  %95 = load i32, i32* %42, align 4
  store i32 %95, i32* %46, align 4
  br label %96

96:                                               ; preds = %182, %3
  %97 = load i32, i32* %46, align 4
  %98 = load i32, i32* %35, align 4
  %99 = icmp slt i32 %97, %98
  br i1 %99, label %100, label %186

100:                                              ; preds = %96
  %101 = load %struct.ListManager*, %struct.ListManager** %34, align 8
  %102 = load i32, i32* %46, align 4
  %103 = call nonnull align 8 dereferenceable(48) %struct.Element* @_ZN11ListManager3getI7ElementEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %101, i32 %102) #24
  %104 = bitcast %struct.Element* %47 to i8*
  %105 = bitcast %struct.Element* %103 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %104, i8* align 8 %105, i64 48, i1 false)
  %106 = getelementptr inbounds %struct.Element, %struct.Element* %47, i32 0, i32 1
  %107 = getelementptr inbounds [2 x i32], [2 x i32]* %106, i64 0, i64 0
  %108 = load i32, i32* %107, align 8
  %109 = load i32, i32* %44, align 4
  %110 = add nsw i32 %108, %109
  store i32 %110, i32* %48, align 4
  %111 = getelementptr inbounds %struct.Element, %struct.Element* %47, i32 0, i32 1
  %112 = getelementptr inbounds [2 x i32], [2 x i32]* %111, i64 0, i64 1
  %113 = load i32, i32* %112, align 4
  store i32 %113, i32* %49, align 4
  %114 = load i32, i32* %48, align 4
  store i32 %114, i32* %50, align 4
  br label %115

115:                                              ; preds = %177, %100
  %116 = load i32, i32* %50, align 4
  %117 = load i32, i32* %49, align 4
  %118 = icmp slt i32 %116, %117
  br i1 %118, label %119, label %181

119:                                              ; preds = %115
  %120 = load void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)*, void (%struct.PhysicalCoordinates*, %struct.PhysicalCoordinates*, i32)** %37, align 8
  %121 = getelementptr inbounds %struct.Element, %struct.Element* %47, i32 0, i32 2
  %122 = load i32, i32* %50, align 4
  call void %120(%struct.PhysicalCoordinates* %121, %struct.PhysicalCoordinates* %51, i32 %122) #24
  %123 = load i32 (i8*, i8*, i32)*, i32 (i8*, i8*, i32)** %38, align 8
  %124 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %125 = bitcast %struct.StructMeta* %124 to i8*
  %126 = getelementptr inbounds %struct.Element, %struct.Element* %47, i32 0, i32 0
  %127 = load i8*, i8** %126, align 8
  %128 = load i32, i32* %50, align 4
  %129 = call i32 %123(i8* %125, i8* %127, i32 %128) #24
  %130 = icmp ne i32 %129, 0
  br i1 %130, label %131, label %176

131:                                              ; preds = %119
  %132 = load i8* (i8*, i8*, i32)*, i8* (i8*, i8*, i32)** %39, align 8
  %133 = load %struct.StructMeta*, %struct.StructMeta** %32, align 8
  %134 = bitcast %struct.StructMeta* %133 to i8*
  %135 = getelementptr inbounds %struct.Element, %struct.Element* %47, i32 0, i32 0
  %136 = load i8*, i8** %135, align 8
  %137 = load i32, i32* %50, align 4
  %138 = call i8* %132(i8* %134, i8* %136, i32 %137) #24
  store i8* %138, i8** %52, align 8
  %139 = load i8* (i8*)*, i8* (i8*)** %41, align 8
  %140 = load i8*, i8** %52, align 8
  %141 = call i8* %139(i8* %140) #24
  store i8* %141, i8** %52, align 8
  %142 = load i32 (i8*, i8*)*, i32 (i8*, i8*)** %40, align 8
  %143 = load %struct.StructMeta*, %struct.StructMeta** %33, align 8
  %144 = bitcast %struct.StructMeta* %143 to i8*
  %145 = load i8*, i8** %52, align 8
  %146 = call i32 %142(i8* %144, i8* %145) #24
  store i32 %146, i32* %53, align 4
  %147 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %53, i32* nonnull align 4 dereferenceable(4) addrspacecast (i32 addrspace(4)* @_ZL31taichi_listgen_max_element_size to i32*)) #24
  %148 = load i32, i32* %147, align 4
  store i32 %148, i32* %54, align 4
  store i32 0, i32* %55, align 4
  br label %149

149:                                              ; preds = %171, %131
  %150 = load i32, i32* %55, align 4
  %151 = load i32, i32* %53, align 4
  %152 = icmp slt i32 %150, %151
  br i1 %152, label %153, label %175

153:                                              ; preds = %149
  %154 = load i8*, i8** %52, align 8
  %155 = getelementptr inbounds %struct.Element, %struct.Element* %56, i32 0, i32 0
  store i8* %154, i8** %155, align 8
  %156 = load i32, i32* %55, align 4
  %157 = getelementptr inbounds %struct.Element, %struct.Element* %56, i32 0, i32 1
  %158 = getelementptr inbounds [2 x i32], [2 x i32]* %157, i64 0, i64 0
  store i32 %156, i32* %158, align 8
  %159 = load i32, i32* %55, align 4
  %160 = load i32, i32* %54, align 4
  %161 = add nsw i32 %159, %160
  store i32 %161, i32* %57, align 4
  %162 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %57, i32* nonnull align 4 dereferenceable(4) %53) #24
  %163 = load i32, i32* %162, align 4
  %164 = getelementptr inbounds %struct.Element, %struct.Element* %56, i32 0, i32 1
  %165 = getelementptr inbounds [2 x i32], [2 x i32]* %164, i64 0, i64 1
  store i32 %163, i32* %165, align 4
  %166 = getelementptr inbounds %struct.Element, %struct.Element* %56, i32 0, i32 2
  %167 = bitcast %struct.PhysicalCoordinates* %166 to i8*
  %168 = bitcast %struct.PhysicalCoordinates* %51 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %167, i8* align 4 %168, i64 32, i1 false)
  %169 = load %struct.ListManager*, %struct.ListManager** %36, align 8
  %170 = bitcast %struct.Element* %56 to i8*
  call void @_ZN11ListManager6appendEPv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %169, i8* %170) #24
  br label %171

171:                                              ; preds = %153
  %172 = load i32, i32* %54, align 4
  %173 = load i32, i32* %55, align 4
  %174 = add nsw i32 %173, %172
  store i32 %174, i32* %55, align 4
  br label %149, !llvm.loop !42

175:                                              ; preds = %149
  br label %176

176:                                              ; preds = %175, %119
  br label %177

177:                                              ; preds = %176
  %178 = load i32, i32* %45, align 4
  %179 = load i32, i32* %50, align 4
  %180 = add nsw i32 %179, %178
  store i32 %180, i32* %50, align 4
  br label %115, !llvm.loop !43

181:                                              ; preds = %115
  br label %182

182:                                              ; preds = %181
  %183 = load i32, i32* %43, align 4
  %184 = load i32, i32* %46, align 4
  %185 = add nsw i32 %184, %183
  store i32 %185, i32* %46, align 4
  br label %96, !llvm.loop !44

186:                                              ; preds = %96
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  store %struct.ListManager* %0, %struct.ListManager** %5, align 8
  %6 = load %struct.ListManager*, %struct.ListManager** %5, align 8
  %7 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %6, i32 0, i32 5
  %8 = load i32, i32* %7, align 8
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @parallel_struct_for(%struct.RuntimeContext* %0, i32 %1, i32 %2, i32 %3, void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)* %4, i64 %5, i32 %6) #2 {
  %8 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i32, align 4, addrspace(5)
  %11 = alloca i32, align 4, addrspace(5)
  %12 = alloca void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)*, align 8, addrspace(5)
  %13 = alloca i64, align 8, addrspace(5)
  %14 = alloca i32, align 4, addrspace(5)
  %15 = alloca %struct.ListManager*, align 8, addrspace(5)
  %16 = alloca i32, align 4, addrspace(5)
  %17 = alloca i32, align 4, addrspace(5)
  %18 = alloca [1 x i8], align 8, addrspace(5)
  %19 = alloca i32, align 4, addrspace(5)
  %20 = alloca i32, align 4, addrspace(5)
  %21 = alloca i32, align 4, addrspace(5)
  %22 = alloca %struct.Element*, align 8, addrspace(5)
  %23 = alloca i32, align 4, addrspace(5)
  %24 = alloca i32, align 4, addrspace(5)
  %25 = addrspacecast %struct.RuntimeContext* addrspace(5)* %8 to %struct.RuntimeContext**
  %26 = addrspacecast i32 addrspace(5)* %9 to i32*
  %27 = addrspacecast i32 addrspace(5)* %10 to i32*
  %28 = addrspacecast i32 addrspace(5)* %11 to i32*
  %29 = addrspacecast void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)* addrspace(5)* %12 to void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)**
  %30 = addrspacecast i64 addrspace(5)* %13 to i64*
  %31 = addrspacecast i32 addrspace(5)* %14 to i32*
  %32 = addrspacecast %struct.ListManager* addrspace(5)* %15 to %struct.ListManager**
  %33 = addrspacecast i32 addrspace(5)* %16 to i32*
  %34 = addrspacecast i32 addrspace(5)* %17 to i32*
  %35 = addrspacecast [1 x i8] addrspace(5)* %18 to [1 x i8]*
  %36 = addrspacecast i32 addrspace(5)* %19 to i32*
  %37 = addrspacecast i32 addrspace(5)* %20 to i32*
  %38 = addrspacecast i32 addrspace(5)* %21 to i32*
  %39 = addrspacecast %struct.Element* addrspace(5)* %22 to %struct.Element**
  %40 = addrspacecast i32 addrspace(5)* %23 to i32*
  %41 = addrspacecast i32 addrspace(5)* %24 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %25, align 8
  store i32 %1, i32* %26, align 4
  store i32 %2, i32* %27, align 4
  store i32 %3, i32* %28, align 4
  store void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)* %4, void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)** %29, align 8
  store i64 %5, i64* %30, align 8
  store i32 %6, i32* %31, align 4
  %42 = load %struct.RuntimeContext*, %struct.RuntimeContext** %25, align 8
  %43 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %42, i32 0, i32 0
  %44 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %43, align 8
  %45 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %44, i32 0, i32 13
  %46 = load i32, i32* %26, align 4
  %47 = sext i32 %46 to i64
  %48 = getelementptr inbounds [1024 x %struct.ListManager*], [1024 x %struct.ListManager*]* %45, i64 0, i64 %47
  %49 = load %struct.ListManager*, %struct.ListManager** %48, align 8
  store %struct.ListManager* %49, %struct.ListManager** %32, align 8
  %50 = load %struct.ListManager*, %struct.ListManager** %32, align 8
  %51 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %50) #24
  store i32 %51, i32* %33, align 4
  %52 = call i32 @block_idx() #24
  store i32 %52, i32* %34, align 4
  store i32 1, i32* %28, align 4
  %53 = load i32, i32* %27, align 4
  %54 = load i32, i32* %28, align 4
  %55 = sdiv i32 %53, %54
  store i32 %55, i32* %36, align 4
  br label %56

56:                                               ; preds = %7, %105
  %57 = load i32, i32* %34, align 4
  %58 = load i32, i32* %28, align 4
  %59 = sdiv i32 %57, %58
  store i32 %59, i32* %37, align 4
  %60 = load i32, i32* %37, align 4
  %61 = load i32, i32* %33, align 4
  %62 = icmp sge i32 %60, %61
  br i1 %62, label %63, label %64

63:                                               ; preds = %56
  br label %109

64:                                               ; preds = %56
  %65 = load i32, i32* %34, align 4
  %66 = load i32, i32* %28, align 4
  %67 = srem i32 %65, %66
  store i32 %67, i32* %38, align 4
  %68 = load %struct.ListManager*, %struct.ListManager** %32, align 8
  %69 = load i32, i32* %37, align 4
  %70 = call nonnull align 8 dereferenceable(48) %struct.Element* @_ZN11ListManager3getI7ElementEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %68, i32 %69) #24
  store %struct.Element* %70, %struct.Element** %39, align 8
  %71 = load %struct.Element*, %struct.Element** %39, align 8
  %72 = getelementptr inbounds %struct.Element, %struct.Element* %71, i32 0, i32 1
  %73 = getelementptr inbounds [2 x i32], [2 x i32]* %72, i64 0, i64 0
  %74 = load i32, i32* %73, align 8
  %75 = load i32, i32* %38, align 4
  %76 = load i32, i32* %36, align 4
  %77 = mul nsw i32 %75, %76
  %78 = add nsw i32 %74, %77
  store i32 %78, i32* %40, align 4
  %79 = load %struct.Element*, %struct.Element** %39, align 8
  %80 = getelementptr inbounds %struct.Element, %struct.Element* %79, i32 0, i32 1
  %81 = getelementptr inbounds [2 x i32], [2 x i32]* %80, i64 0, i64 0
  %82 = load i32, i32* %81, align 8
  %83 = load i32, i32* %38, align 4
  %84 = add nsw i32 %83, 1
  %85 = load i32, i32* %36, align 4
  %86 = mul nsw i32 %84, %85
  %87 = add nsw i32 %82, %86
  store i32 %87, i32* %41, align 4
  %88 = load %struct.Element*, %struct.Element** %39, align 8
  %89 = getelementptr inbounds %struct.Element, %struct.Element* %88, i32 0, i32 1
  %90 = getelementptr inbounds [2 x i32], [2 x i32]* %89, i64 0, i64 1
  %91 = call nonnull align 4 dereferenceable(4) i32* @_ZSt3minIiEUa9enable_ifILb1EERKT_S2_S2_(i32* nonnull align 4 dereferenceable(4) %41, i32* nonnull align 4 dereferenceable(4) %90) #24
  %92 = load i32, i32* %91, align 4
  store i32 %92, i32* %41, align 4
  %93 = load i32, i32* %40, align 4
  %94 = load i32, i32* %41, align 4
  %95 = icmp slt i32 %93, %94
  br i1 %95, label %96, label %105

96:                                               ; preds = %64
  %97 = load void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)*, void (%struct.RuntimeContext*, i8*, %struct.Element*, i32, i32)** %29, align 8
  %98 = load %struct.RuntimeContext*, %struct.RuntimeContext** %25, align 8
  %99 = getelementptr inbounds [1 x i8], [1 x i8]* %35, i64 0, i64 0
  %100 = load %struct.ListManager*, %struct.ListManager** %32, align 8
  %101 = load i32, i32* %37, align 4
  %102 = call nonnull align 8 dereferenceable(48) %struct.Element* @_ZN11ListManager3getI7ElementEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %100, i32 %101) #24
  %103 = load i32, i32* %40, align 4
  %104 = load i32, i32* %41, align 4
  call void %97(%struct.RuntimeContext* %98, i8* %99, %struct.Element* %102, i32 %103, i32 %104) #24
  br label %105

105:                                              ; preds = %96, %64
  %106 = call i32 @grid_dim() #24
  %107 = load i32, i32* %34, align 4
  %108 = add nsw i32 %107, %106
  store i32 %108, i32* %34, align 4
  br label %56, !llvm.loop !45

109:                                              ; preds = %63
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @gpu_parallel_range_for(%struct.RuntimeContext* %0, i32 %1, i32 %2, void (%struct.RuntimeContext*, i8*)* %3, void (%struct.RuntimeContext*, i8*, i32)* %4, void (%struct.RuntimeContext*, i8*)* %5, i64 %6) #2 {
  %8 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i32, align 4, addrspace(5)
  %11 = alloca void (%struct.RuntimeContext*, i8*)*, align 8, addrspace(5)
  %12 = alloca void (%struct.RuntimeContext*, i8*, i32)*, align 8, addrspace(5)
  %13 = alloca void (%struct.RuntimeContext*, i8*)*, align 8, addrspace(5)
  %14 = alloca i64, align 8, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = alloca i8, align 1, addrspace(5)
  %17 = alloca i8*, align 8, addrspace(5)
  %18 = alloca %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X", align 1, addrspace(5)
  %19 = alloca %"struct.__HIP_Coordinates<__HIP_GridDim>::__X", align 1, addrspace(5)
  %20 = addrspacecast %struct.RuntimeContext* addrspace(5)* %8 to %struct.RuntimeContext**
  %21 = addrspacecast i32 addrspace(5)* %9 to i32*
  %22 = addrspacecast i32 addrspace(5)* %10 to i32*
  %23 = addrspacecast void (%struct.RuntimeContext*, i8*)* addrspace(5)* %11 to void (%struct.RuntimeContext*, i8*)**
  %24 = addrspacecast void (%struct.RuntimeContext*, i8*, i32)* addrspace(5)* %12 to void (%struct.RuntimeContext*, i8*, i32)**
  %25 = addrspacecast void (%struct.RuntimeContext*, i8*)* addrspace(5)* %13 to void (%struct.RuntimeContext*, i8*)**
  %26 = addrspacecast i64 addrspace(5)* %14 to i64*
  %27 = addrspacecast i32 addrspace(5)* %15 to i32*
  %28 = addrspacecast i8 addrspace(5)* %16 to i8*
  %29 = addrspacecast i8* addrspace(5)* %17 to i8**
  %30 = addrspacecast %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X" addrspace(5)* %18 to %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"*
  %31 = addrspacecast %"struct.__HIP_Coordinates<__HIP_GridDim>::__X" addrspace(5)* %19 to %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %20, align 8
  store i32 %1, i32* %21, align 4
  store i32 %2, i32* %22, align 4
  store void (%struct.RuntimeContext*, i8*)* %3, void (%struct.RuntimeContext*, i8*)** %23, align 8
  store void (%struct.RuntimeContext*, i8*, i32)* %4, void (%struct.RuntimeContext*, i8*, i32)** %24, align 8
  store void (%struct.RuntimeContext*, i8*)* %5, void (%struct.RuntimeContext*, i8*)** %25, align 8
  store i64 %6, i64* %26, align 8
  %32 = call i32 @_ZNK17__HIP_CoordinatesI15__HIP_ThreadIdxE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"* nonnull align 1 dereferenceable(1) addrspacecast (%"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X" addrspace(4)* @_ZN17__HIP_CoordinatesI15__HIP_ThreadIdxE1xE to %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"*)) #24
  %33 = call i32 @_ZNK17__HIP_CoordinatesI14__HIP_BlockDimE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"* nonnull align 1 dereferenceable(1) addrspacecast (%"struct.__HIP_Coordinates<__HIP_BlockDim>::__X" addrspace(4)* @_ZN17__HIP_CoordinatesI14__HIP_BlockDimE1xE to %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"*)) #24
  %34 = call i32 @_ZNK17__HIP_CoordinatesI14__HIP_BlockIdxE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"* nonnull align 1 dereferenceable(1) addrspacecast (%"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X" addrspace(4)* @_ZN17__HIP_CoordinatesI14__HIP_BlockIdxE1xE to %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"*)) #24
  %35 = mul i32 %33, %34
  %36 = add i32 %32, %35
  %37 = load i32, i32* %21, align 4
  %38 = add i32 %36, %37
  store i32 %38, i32* %27, align 4
  store i8* %28, i8** %29, align 8
  %39 = load void (%struct.RuntimeContext*, i8*)*, void (%struct.RuntimeContext*, i8*)** %23, align 8
  %40 = icmp ne void (%struct.RuntimeContext*, i8*)* %39, null
  br i1 %40, label %41, label %45

41:                                               ; preds = %7
  %42 = load void (%struct.RuntimeContext*, i8*)*, void (%struct.RuntimeContext*, i8*)** %23, align 8
  %43 = load %struct.RuntimeContext*, %struct.RuntimeContext** %20, align 8
  %44 = load i8*, i8** %29, align 8
  call void %42(%struct.RuntimeContext* %43, i8* %44) #24
  br label %45

45:                                               ; preds = %41, %7
  br label %46

46:                                               ; preds = %50, %45
  %47 = load i32, i32* %27, align 4
  %48 = load i32, i32* %22, align 4
  %49 = icmp slt i32 %47, %48
  br i1 %49, label %50, label %58

50:                                               ; preds = %46
  %51 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %24, align 8
  %52 = load %struct.RuntimeContext*, %struct.RuntimeContext** %20, align 8
  %53 = load i8*, i8** %29, align 8
  %54 = load i32, i32* %27, align 4
  call void %51(%struct.RuntimeContext* %52, i8* %53, i32 %54) #24
  %55 = call i32 @_ZmlN17__HIP_CoordinatesI14__HIP_BlockDimE3__XENS_I13__HIP_GridDimE3__XE() #24
  %56 = load i32, i32* %27, align 4
  %57 = add i32 %56, %55
  store i32 %57, i32* %27, align 4
  br label %46, !llvm.loop !46

58:                                               ; preds = %46
  %59 = load void (%struct.RuntimeContext*, i8*)*, void (%struct.RuntimeContext*, i8*)** %25, align 8
  %60 = icmp ne void (%struct.RuntimeContext*, i8*)* %59, null
  br i1 %60, label %61, label %65

61:                                               ; preds = %58
  %62 = load void (%struct.RuntimeContext*, i8*)*, void (%struct.RuntimeContext*, i8*)** %25, align 8
  %63 = load %struct.RuntimeContext*, %struct.RuntimeContext** %20, align 8
  %64 = load i8*, i8** %29, align 8
  call void %62(%struct.RuntimeContext* %63, i8* %64) #24
  br label %65

65:                                               ; preds = %61, %58
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK17__HIP_CoordinatesI15__HIP_ThreadIdxE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"* nonnull align 1 dereferenceable(1) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"*, align 8, addrspace(5)
  %4 = alloca %struct.__HIP_ThreadIdx, align 1, addrspace(5)
  %5 = addrspacecast i32 addrspace(5)* %2 to i32*
  %6 = addrspacecast %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"* addrspace(5)* %3 to %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"**
  %7 = addrspacecast %struct.__HIP_ThreadIdx addrspace(5)* %4 to %struct.__HIP_ThreadIdx*
  store %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"* %0, %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"** %6, align 8
  %8 = load %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"*, %"struct.__HIP_Coordinates<__HIP_ThreadIdx>::__X"** %6, align 8
  %9 = call i32 @_ZNK15__HIP_ThreadIdxclEj(%struct.__HIP_ThreadIdx* nonnull align 1 dereferenceable(1) %7, i32 0) #24
  ret i32 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK17__HIP_CoordinatesI14__HIP_BlockDimE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"* nonnull align 1 dereferenceable(1) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"*, align 8, addrspace(5)
  %4 = alloca %struct.__HIP_BlockDim, align 1, addrspace(5)
  %5 = addrspacecast i32 addrspace(5)* %2 to i32*
  %6 = addrspacecast %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"* addrspace(5)* %3 to %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"**
  %7 = addrspacecast %struct.__HIP_BlockDim addrspace(5)* %4 to %struct.__HIP_BlockDim*
  store %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"* %0, %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"** %6, align 8
  %8 = load %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"*, %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"** %6, align 8
  %9 = call i32 @_ZNK14__HIP_BlockDimclEj(%struct.__HIP_BlockDim* nonnull align 1 dereferenceable(1) %7, i32 0) #24
  ret i32 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK17__HIP_CoordinatesI14__HIP_BlockIdxE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"* nonnull align 1 dereferenceable(1) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"*, align 8, addrspace(5)
  %4 = alloca %struct.__HIP_BlockIdx, align 1, addrspace(5)
  %5 = addrspacecast i32 addrspace(5)* %2 to i32*
  %6 = addrspacecast %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"* addrspace(5)* %3 to %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"**
  %7 = addrspacecast %struct.__HIP_BlockIdx addrspace(5)* %4 to %struct.__HIP_BlockIdx*
  store %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"* %0, %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"** %6, align 8
  %8 = load %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"*, %"struct.__HIP_Coordinates<__HIP_BlockIdx>::__X"** %6, align 8
  %9 = call i32 @_ZNK14__HIP_BlockIdxclEj(%struct.__HIP_BlockIdx* nonnull align 1 dereferenceable(1) %7, i32 0) #24
  ret i32 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZmlN17__HIP_CoordinatesI14__HIP_BlockDimE3__XENS_I13__HIP_GridDimE3__XE() #2 comdat {
  %1 = alloca i32, align 4, addrspace(5)
  %2 = alloca %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X", align 1, addrspace(5)
  %3 = alloca %"struct.__HIP_Coordinates<__HIP_GridDim>::__X", align 1, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %1 to i32*
  %5 = addrspacecast %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X" addrspace(5)* %2 to %"struct.__HIP_Coordinates<__HIP_BlockDim>::__X"*
  %6 = addrspacecast %"struct.__HIP_Coordinates<__HIP_GridDim>::__X" addrspace(5)* %3 to %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"*
  %7 = call i64 @__ockl_get_global_size(i32 0) #17
  %8 = trunc i64 %7 to i32
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @gpu_parallel_mesh_for(%struct.RuntimeContext* %0, i32 %1, void (%struct.RuntimeContext*, i8*, i32)* %2, void (%struct.RuntimeContext*, i8*, i32)* %3, void (%struct.RuntimeContext*, i8*, i32)* %4, i64 %5) #2 {
  %7 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca void (%struct.RuntimeContext*, i8*, i32)*, align 8, addrspace(5)
  %10 = alloca void (%struct.RuntimeContext*, i8*, i32)*, align 8, addrspace(5)
  %11 = alloca void (%struct.RuntimeContext*, i8*, i32)*, align 8, addrspace(5)
  %12 = alloca i64, align 8, addrspace(5)
  %13 = alloca i8, align 1, addrspace(5)
  %14 = alloca i8*, align 8, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = addrspacecast %struct.RuntimeContext* addrspace(5)* %7 to %struct.RuntimeContext**
  %17 = addrspacecast i32 addrspace(5)* %8 to i32*
  %18 = addrspacecast void (%struct.RuntimeContext*, i8*, i32)* addrspace(5)* %9 to void (%struct.RuntimeContext*, i8*, i32)**
  %19 = addrspacecast void (%struct.RuntimeContext*, i8*, i32)* addrspace(5)* %10 to void (%struct.RuntimeContext*, i8*, i32)**
  %20 = addrspacecast void (%struct.RuntimeContext*, i8*, i32)* addrspace(5)* %11 to void (%struct.RuntimeContext*, i8*, i32)**
  %21 = addrspacecast i64 addrspace(5)* %12 to i64*
  %22 = addrspacecast i8 addrspace(5)* %13 to i8*
  %23 = addrspacecast i8* addrspace(5)* %14 to i8**
  %24 = addrspacecast i32 addrspace(5)* %15 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %16, align 8
  store i32 %1, i32* %17, align 4
  store void (%struct.RuntimeContext*, i8*, i32)* %2, void (%struct.RuntimeContext*, i8*, i32)** %18, align 8
  store void (%struct.RuntimeContext*, i8*, i32)* %3, void (%struct.RuntimeContext*, i8*, i32)** %19, align 8
  store void (%struct.RuntimeContext*, i8*, i32)* %4, void (%struct.RuntimeContext*, i8*, i32)** %20, align 8
  store i64 %5, i64* %21, align 8
  store i8* %22, i8** %23, align 8
  %25 = call i32 @block_idx() #24
  store i32 %25, i32* %24, align 4
  br label %26

26:                                               ; preds = %51, %6
  %27 = load i32, i32* %24, align 4
  %28 = load i32, i32* %17, align 4
  %29 = icmp slt i32 %27, %28
  br i1 %29, label %30, label %55

30:                                               ; preds = %26
  %31 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %18, align 8
  %32 = icmp ne void (%struct.RuntimeContext*, i8*, i32)* %31, null
  br i1 %32, label %33, label %38

33:                                               ; preds = %30
  %34 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %18, align 8
  %35 = load %struct.RuntimeContext*, %struct.RuntimeContext** %16, align 8
  %36 = load i8*, i8** %23, align 8
  %37 = load i32, i32* %24, align 4
  call void %34(%struct.RuntimeContext* %35, i8* %36, i32 %37) #24
  br label %38

38:                                               ; preds = %33, %30
  %39 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %19, align 8
  %40 = load %struct.RuntimeContext*, %struct.RuntimeContext** %16, align 8
  %41 = load i8*, i8** %23, align 8
  %42 = load i32, i32* %24, align 4
  call void %39(%struct.RuntimeContext* %40, i8* %41, i32 %42) #24
  %43 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %20, align 8
  %44 = icmp ne void (%struct.RuntimeContext*, i8*, i32)* %43, null
  br i1 %44, label %45, label %50

45:                                               ; preds = %38
  %46 = load void (%struct.RuntimeContext*, i8*, i32)*, void (%struct.RuntimeContext*, i8*, i32)** %20, align 8
  %47 = load %struct.RuntimeContext*, %struct.RuntimeContext** %16, align 8
  %48 = load i8*, i8** %23, align 8
  %49 = load i32, i32* %24, align 4
  call void %46(%struct.RuntimeContext* %47, i8* %48, i32 %49) #24
  br label %50

50:                                               ; preds = %45, %38
  br label %51

51:                                               ; preds = %50
  %52 = call i32 @_ZNK17__HIP_CoordinatesI13__HIP_GridDimE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_GridDim>::__X"* nonnull align 1 dereferenceable(1) addrspacecast (%"struct.__HIP_Coordinates<__HIP_GridDim>::__X" addrspace(4)* @_ZN17__HIP_CoordinatesI13__HIP_GridDimE1xE to %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"*)) #24
  %53 = load i32, i32* %24, align 4
  %54 = add i32 %53, %52
  store i32 %54, i32* %24, align 4
  br label %26, !llvm.loop !47

55:                                               ; preds = %26
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK17__HIP_CoordinatesI13__HIP_GridDimE3__XcvjEv(%"struct.__HIP_Coordinates<__HIP_GridDim>::__X"* nonnull align 1 dereferenceable(1) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"*, align 8, addrspace(5)
  %4 = alloca %struct.__HIP_GridDim, align 1, addrspace(5)
  %5 = addrspacecast i32 addrspace(5)* %2 to i32*
  %6 = addrspacecast %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"* addrspace(5)* %3 to %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"**
  %7 = addrspacecast %struct.__HIP_GridDim addrspace(5)* %4 to %struct.__HIP_GridDim*
  store %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"* %0, %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"** %6, align 8
  %8 = load %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"*, %"struct.__HIP_Coordinates<__HIP_GridDim>::__X"** %6, align 8
  %9 = call i32 @_ZNK13__HIP_GridDimclEj(%struct.__HIP_GridDim* nonnull align 1 dereferenceable(1) %7, i32 0) #24
  ret i32 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @linear_thread_idx(%struct.RuntimeContext* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = call i32 @block_idx() #24
  %7 = call i32 @block_dim() #24
  %8 = mul nsw i32 %6, %7
  %9 = call i32 @thread_idx() #24
  %10 = add nsw i32 %8, %9
  ret i32 %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @DenseMeta_get_morton_dim(%struct.DenseMeta* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.DenseMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.DenseMeta* addrspace(5)* %3 to %struct.DenseMeta**
  store %struct.DenseMeta* %0, %struct.DenseMeta** %5, align 8
  %6 = load %struct.DenseMeta*, %struct.DenseMeta** %5, align 8
  %7 = getelementptr inbounds %struct.DenseMeta, %struct.DenseMeta* %6, i32 0, i32 1
  %8 = load i32, i32* %7, align 8
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32* @DenseMeta_get_ptr_morton_dim(%struct.DenseMeta* %0) #2 {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca %struct.DenseMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast %struct.DenseMeta* addrspace(5)* %3 to %struct.DenseMeta**
  store %struct.DenseMeta* %0, %struct.DenseMeta** %5, align 8
  %6 = load %struct.DenseMeta*, %struct.DenseMeta** %5, align 8
  %7 = getelementptr inbounds %struct.DenseMeta, %struct.DenseMeta* %6, i32 0, i32 1
  ret i32* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @DenseMeta_set_morton_dim(%struct.DenseMeta* %0, i32 %1) #2 {
  %3 = alloca %struct.DenseMeta*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.DenseMeta* addrspace(5)* %3 to %struct.DenseMeta**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.DenseMeta* %0, %struct.DenseMeta** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load i32, i32* %6, align 4
  %8 = load %struct.DenseMeta*, %struct.DenseMeta** %5, align 8
  %9 = getelementptr inbounds %struct.DenseMeta, %struct.DenseMeta* %8, i32 0, i32 1
  store i32 %7, i32* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @Dense_get_num_elements(i8* %0, i8* %1) #2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast i8* addrspace(5)* %4 to i8**
  %8 = addrspacecast i8* addrspace(5)* %5 to i8**
  store i8* %0, i8** %7, align 8
  store i8* %1, i8** %8, align 8
  %9 = load i8*, i8** %7, align 8
  %10 = bitcast i8* %9 to %struct.StructMeta*
  %11 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %10, i32 0, i32 2
  %12 = load i64, i64* %11, align 8
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @Dense_activate(i8* %0, i8* %1, i32 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = addrspacecast i8* addrspace(5)* %4 to i8**
  %8 = addrspacecast i8* addrspace(5)* %5 to i8**
  %9 = addrspacecast i32 addrspace(5)* %6 to i32*
  store i8* %0, i8** %7, align 8
  store i8* %1, i8** %8, align 8
  store i32 %2, i32* %9, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @Dense_is_active(i8* %0, i8* %1, i32 %2) #2 {
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i8* addrspace(5)* %5 to i8**
  %10 = addrspacecast i8* addrspace(5)* %6 to i8**
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i8* %0, i8** %9, align 8
  store i8* %1, i8** %10, align 8
  store i32 %2, i32* %11, align 4
  ret i32 1
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @Dense_lookup_element(i8* %0, i8* %1, i32 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast i8* addrspace(5)* %5 to i8**
  %10 = addrspacecast i8* addrspace(5)* %6 to i8**
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i8* %0, i8** %9, align 8
  store i8* %1, i8** %10, align 8
  store i32 %2, i32* %11, align 4
  %12 = load i8*, i8** %10, align 8
  %13 = load i8*, i8** %9, align 8
  %14 = bitcast i8* %13 to %struct.StructMeta*
  %15 = getelementptr inbounds %struct.StructMeta, %struct.StructMeta* %14, i32 0, i32 1
  %16 = load i64, i64* %15, align 8
  %17 = load i32, i32* %11, align 4
  %18 = sext i32 %17 to i64
  %19 = mul i64 %16, %18
  %20 = getelementptr inbounds i8, i8* %12, i64 %19
  ret i8* %20
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @DynamicMeta_get_chunk_size(%struct.DynamicMeta* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.DynamicMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.DynamicMeta* addrspace(5)* %3 to %struct.DynamicMeta**
  store %struct.DynamicMeta* %0, %struct.DynamicMeta** %5, align 8
  %6 = load %struct.DynamicMeta*, %struct.DynamicMeta** %5, align 8
  %7 = getelementptr inbounds %struct.DynamicMeta, %struct.DynamicMeta* %6, i32 0, i32 1
  %8 = load i32, i32* %7, align 8
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32* @DynamicMeta_get_ptr_chunk_size(%struct.DynamicMeta* %0) #2 {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca %struct.DynamicMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast %struct.DynamicMeta* addrspace(5)* %3 to %struct.DynamicMeta**
  store %struct.DynamicMeta* %0, %struct.DynamicMeta** %5, align 8
  %6 = load %struct.DynamicMeta*, %struct.DynamicMeta** %5, align 8
  %7 = getelementptr inbounds %struct.DynamicMeta, %struct.DynamicMeta* %6, i32 0, i32 1
  ret i32* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @DynamicMeta_set_chunk_size(%struct.DynamicMeta* %0, i32 %1) #2 {
  %3 = alloca %struct.DynamicMeta*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.DynamicMeta* addrspace(5)* %3 to %struct.DynamicMeta**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.DynamicMeta* %0, %struct.DynamicMeta** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load i32, i32* %6, align 4
  %8 = load %struct.DynamicMeta*, %struct.DynamicMeta** %5, align 8
  %9 = getelementptr inbounds %struct.DynamicMeta, %struct.DynamicMeta* %8, i32 0, i32 1
  store i32 %7, i32* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @PointerMeta_get__(%struct.PointerMeta* %0) #2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca %struct.PointerMeta*, align 8, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast %struct.PointerMeta* addrspace(5)* %3 to %struct.PointerMeta**
  store %struct.PointerMeta* %0, %struct.PointerMeta** %5, align 8
  %6 = load %struct.PointerMeta*, %struct.PointerMeta** %5, align 8
  %7 = getelementptr inbounds %struct.PointerMeta, %struct.PointerMeta* %6, i32 0, i32 1
  %8 = load i8, i8* %7, align 8
  %9 = trunc i8 %8 to i1
  ret i1 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @PointerMeta_get_ptr__(%struct.PointerMeta* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.PointerMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* addrspace(5)* %2 to i8**
  %5 = addrspacecast %struct.PointerMeta* addrspace(5)* %3 to %struct.PointerMeta**
  store %struct.PointerMeta* %0, %struct.PointerMeta** %5, align 8
  %6 = load %struct.PointerMeta*, %struct.PointerMeta** %5, align 8
  %7 = getelementptr inbounds %struct.PointerMeta, %struct.PointerMeta* %6, i32 0, i32 1
  ret i8* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @PointerMeta_set__(%struct.PointerMeta* %0, i1 zeroext %1) #2 {
  %3 = alloca %struct.PointerMeta*, align 8, addrspace(5)
  %4 = alloca i8, align 1, addrspace(5)
  %5 = addrspacecast %struct.PointerMeta* addrspace(5)* %3 to %struct.PointerMeta**
  %6 = addrspacecast i8 addrspace(5)* %4 to i8*
  store %struct.PointerMeta* %0, %struct.PointerMeta** %5, align 8
  %7 = zext i1 %1 to i8
  store i8 %7, i8* %6, align 1
  %8 = load i8, i8* %6, align 1
  %9 = trunc i8 %8 to i1
  %10 = load %struct.PointerMeta*, %struct.PointerMeta** %5, align 8
  %11 = getelementptr inbounds %struct.PointerMeta, %struct.PointerMeta* %10, i32 0, i32 1
  %12 = zext i1 %9 to i8
  store i8 %12, i8* %11, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @RootMeta_get_tag(%struct.RootMeta* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.RootMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32 addrspace(5)* %2 to i32*
  %5 = addrspacecast %struct.RootMeta* addrspace(5)* %3 to %struct.RootMeta**
  store %struct.RootMeta* %0, %struct.RootMeta** %5, align 8
  %6 = load %struct.RootMeta*, %struct.RootMeta** %5, align 8
  %7 = getelementptr inbounds %struct.RootMeta, %struct.RootMeta* %6, i32 0, i32 1
  %8 = load i32, i32* %7, align 8
  ret i32 %8
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32* @RootMeta_get_ptr_tag(%struct.RootMeta* %0) #2 {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca %struct.RootMeta*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast %struct.RootMeta* addrspace(5)* %3 to %struct.RootMeta**
  store %struct.RootMeta* %0, %struct.RootMeta** %5, align 8
  %6 = load %struct.RootMeta*, %struct.RootMeta** %5, align 8
  %7 = getelementptr inbounds %struct.RootMeta, %struct.RootMeta* %6, i32 0, i32 1
  ret i32* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @RootMeta_set_tag(%struct.RootMeta* %0, i32 %1) #2 {
  %3 = alloca %struct.RootMeta*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.RootMeta* addrspace(5)* %3 to %struct.RootMeta**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.RootMeta* %0, %struct.RootMeta** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load i32, i32* %6, align 4
  %8 = load %struct.RootMeta*, %struct.RootMeta** %5, align 8
  %9 = getelementptr inbounds %struct.RootMeta, %struct.RootMeta* %8, i32 0, i32 1
  store i32 %7, i32* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden zeroext i1 @BitmaskedMeta_get__(%struct.BitmaskedMeta* %0) #2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca %struct.BitmaskedMeta*, align 8, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast %struct.BitmaskedMeta* addrspace(5)* %3 to %struct.BitmaskedMeta**
  store %struct.BitmaskedMeta* %0, %struct.BitmaskedMeta** %5, align 8
  %6 = load %struct.BitmaskedMeta*, %struct.BitmaskedMeta** %5, align 8
  %7 = getelementptr inbounds %struct.BitmaskedMeta, %struct.BitmaskedMeta* %6, i32 0, i32 1
  %8 = load i8, i8* %7, align 8
  %9 = trunc i8 %8 to i1
  ret i1 %9
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @BitmaskedMeta_get_ptr__(%struct.BitmaskedMeta* %0) #2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.BitmaskedMeta*, align 8, addrspace(5)
  %4 = addrspacecast i8* addrspace(5)* %2 to i8**
  %5 = addrspacecast %struct.BitmaskedMeta* addrspace(5)* %3 to %struct.BitmaskedMeta**
  store %struct.BitmaskedMeta* %0, %struct.BitmaskedMeta** %5, align 8
  %6 = load %struct.BitmaskedMeta*, %struct.BitmaskedMeta** %5, align 8
  %7 = getelementptr inbounds %struct.BitmaskedMeta, %struct.BitmaskedMeta* %6, i32 0, i32 1
  ret i8* %7
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @BitmaskedMeta_set__(%struct.BitmaskedMeta* %0, i1 zeroext %1) #2 {
  %3 = alloca %struct.BitmaskedMeta*, align 8, addrspace(5)
  %4 = alloca i8, align 1, addrspace(5)
  %5 = addrspacecast %struct.BitmaskedMeta* addrspace(5)* %3 to %struct.BitmaskedMeta**
  %6 = addrspacecast i8 addrspace(5)* %4 to i8*
  store %struct.BitmaskedMeta* %0, %struct.BitmaskedMeta** %5, align 8
  %7 = zext i1 %1 to i8
  store i8 %7, i8* %6, align 1
  %8 = load i8, i8* %6, align 1
  %9 = trunc i8 %8 to i1
  %10 = load %struct.BitmaskedMeta*, %struct.BitmaskedMeta** %5, align 8
  %11 = getelementptr inbounds %struct.BitmaskedMeta, %struct.BitmaskedMeta* %10, i32 0, i32 1
  %12 = zext i1 %9 to i8
  store i8 %12, i8* %11, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @_ZN11ListManager11touch_chunkEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32 %1) #2 align 2 {
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca %class.anon.4, align 8, addrspace(5)
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  %8 = addrspacecast %class.anon.4 addrspace(5)* %5 to %class.anon.4*
  store %struct.ListManager* %0, %struct.ListManager** %6, align 8
  store i32 %1, i32* %7, align 4
  %9 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %10 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 6
  %11 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %10, align 8
  %12 = load i32, i32* %7, align 4
  %13 = sext i32 %12 to i64
  %14 = icmp ult i64 %13, 131072
  %15 = zext i1 %14 to i32
  call void @taichi_assert_runtime(%struct.LLVMRuntime* %11, i32 %15, i8* getelementptr inbounds ([28 x i8], [28 x i8]* addrspacecast ([28 x i8] addrspace(4)* @.str.5 to [28 x i8]*), i64 0, i64 0)) #24
  %16 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 0
  %17 = load i32, i32* %7, align 4
  %18 = sext i32 %17 to i64
  %19 = getelementptr inbounds [131072 x i8*], [131072 x i8*]* %16, i64 0, i64 %18
  %20 = load i8*, i8** %19, align 8
  %21 = icmp ne i8* %20, null
  br i1 %21, label %27, label %22

22:                                               ; preds = %2
  %23 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 4
  %24 = bitcast i32* %23 to i8*
  %25 = getelementptr inbounds %class.anon.4, %class.anon.4* %8, i32 0, i32 0
  store %struct.ListManager* %9, %struct.ListManager** %25, align 8
  %26 = getelementptr inbounds %class.anon.4, %class.anon.4* %8, i32 0, i32 1
  store i32* %7, i32** %26, align 8
  call void @_Z11locked_taskIZN11ListManager11touch_chunkEiEUlvE_EvPvRKT_(i8* %24, %class.anon.4* nonnull align 8 dereferenceable(16) %8) #24
  br label %27

27:                                               ; preds = %22, %2
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZN11ListManager11touch_chunkEiEUlvE_EvPvRKT_(i8* %0, %class.anon.4* nonnull align 8 dereferenceable(16) %1) #2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %class.anon.4*, align 8, addrspace(5)
  %5 = alloca %class.anon.20, align 1, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %class.anon.4* addrspace(5)* %4 to %class.anon.4**
  %8 = addrspacecast %class.anon.20 addrspace(5)* %5 to %class.anon.20*
  store i8* %0, i8** %6, align 8
  store %class.anon.4* %1, %class.anon.4** %7, align 8
  %9 = load i8*, i8** %6, align 8
  %10 = load %class.anon.4*, %class.anon.4** %7, align 8
  call void @_Z11locked_taskIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EvS3_S6_RKT0_(i8* %9, %class.anon.4* nonnull align 8 dereferenceable(16) %10, %class.anon.20* nonnull align 1 dereferenceable(1) %8) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i8* @_ZN11ListManager8allocateEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0) #2 align 2 {
  %2 = alloca i8*, align 8, addrspace(5)
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast i8* addrspace(5)* %2 to i8**
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %6, align 8
  %8 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %9 = call i32 @_ZN11ListManager19reserve_new_elementEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %8) #24
  store i32 %9, i32* %7, align 4
  %10 = load i32, i32* %7, align 4
  %11 = call i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %8, i32 %10) #24
  ret i8* %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZN11ListManager19reserve_new_elementEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0) #2 comdat align 2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %2 to i32*
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %8 = addrspacecast i32 addrspace(5)* %4 to i32*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  %10 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %11 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %10, i32 0, i32 5
  %12 = call i32 @atomic_add_i32(i32* %11, i32 1) #24
  store i32 %12, i32* %8, align 4
  %13 = load i32, i32* %8, align 4
  %14 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %10, i32 0, i32 3
  %15 = load i32, i32* %14, align 8
  %16 = ashr i32 %13, %15
  store i32 %16, i32* %9, align 4
  %17 = load i32, i32* %9, align 4
  call void @_ZN11ListManager11touch_chunkEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %10, i32 %17) #24
  %18 = load i32, i32* %8, align 4
  ret i32 %18
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %10 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 0
  %11 = load i32, i32* %8, align 4
  %12 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 3
  %13 = load i32, i32* %12, align 8
  %14 = ashr i32 %11, %13
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds [131072 x i8*], [131072 x i8*]* %10, i64 0, i64 %15
  %17 = load i8*, i8** %16, align 8
  %18 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 1
  %19 = load i64, i64* %18, align 8
  %20 = load i32, i32* %8, align 4
  %21 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %9, i32 0, i32 3
  %22 = load i32, i32* %21, align 8
  %23 = shl i32 1, %22
  %24 = sub nsw i32 %23, 1
  %25 = and i32 %20, %24
  %26 = sext i32 %25 to i64
  %27 = mul i64 %19, %26
  %28 = getelementptr inbounds i8, i8* %17, i64 %27
  ret i8* %28
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @node_gc(%struct.LLVMRuntime* %0, i32 %1) #2 {
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %5, align 8
  %8 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %7, i32 0, i32 14
  %9 = load i32, i32* %6, align 4
  %10 = sext i32 %9 to i64
  %11 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %8, i64 0, i64 %10
  %12 = load %struct.NodeManager*, %struct.NodeManager** %11, align 8
  call void @_ZN11NodeManager9gc_serialEv(%struct.NodeManager* nonnull align 8 dereferenceable(52) %12) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11NodeManager9gc_serialEv(%struct.NodeManager* nonnull align 8 dereferenceable(52) %0) #2 comdat align 2 {
  %2 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i8*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = addrspacecast %struct.NodeManager* addrspace(5)* %2 to %struct.NodeManager**
  %10 = addrspacecast i32 addrspace(5)* %3 to i32*
  %11 = addrspacecast i32 addrspace(5)* %4 to i32*
  %12 = addrspacecast i32 addrspace(5)* %5 to i32*
  %13 = addrspacecast i32 addrspace(5)* %6 to i32*
  %14 = addrspacecast i8* addrspace(5)* %7 to i8**
  %15 = addrspacecast i32 addrspace(5)* %8 to i32*
  store %struct.NodeManager* %0, %struct.NodeManager** %9, align 8
  %16 = load %struct.NodeManager*, %struct.NodeManager** %9, align 8
  %17 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 4
  %18 = load i32, i32* %17, align 4
  store i32 %18, i32* %10, align 4
  br label %19

19:                                               ; preds = %38, %1
  %20 = load i32, i32* %10, align 4
  %21 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %22 = load %struct.ListManager*, %struct.ListManager** %21, align 8
  %23 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %22) #24
  %24 = icmp slt i32 %20, %23
  br i1 %24, label %25, label %41

25:                                               ; preds = %19
  %26 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %27 = load %struct.ListManager*, %struct.ListManager** %26, align 8
  %28 = load i32, i32* %10, align 4
  %29 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %27, i32 %28) #24
  %30 = load i32, i32* %29, align 4
  %31 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %32 = load %struct.ListManager*, %struct.ListManager** %31, align 8
  %33 = load i32, i32* %10, align 4
  %34 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 4
  %35 = load i32, i32* %34, align 4
  %36 = sub nsw i32 %33, %35
  %37 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %32, i32 %36) #24
  store i32 %30, i32* %37, align 4
  br label %38

38:                                               ; preds = %25
  %39 = load i32, i32* %10, align 4
  %40 = add nsw i32 %39, 1
  store i32 %40, i32* %10, align 4
  br label %19, !llvm.loop !48

41:                                               ; preds = %19
  %42 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %43 = load %struct.ListManager*, %struct.ListManager** %42, align 8
  %44 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %43) #24
  %45 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 4
  %46 = load i32, i32* %45, align 4
  %47 = sub nsw i32 %44, %46
  %48 = call i32 @max_i32(i32 %47, i32 0) #24
  store i32 %48, i32* %11, align 4
  %49 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 4
  store i32 0, i32* %49, align 4
  %50 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %51 = load %struct.ListManager*, %struct.ListManager** %50, align 8
  %52 = load i32, i32* %11, align 4
  call void @_ZN11ListManager6resizeEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %51, i32 %52) #24
  store i32 0, i32* %12, align 4
  br label %53

53:                                               ; preds = %85, %41
  %54 = load i32, i32* %12, align 4
  %55 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 6
  %56 = load %struct.ListManager*, %struct.ListManager** %55, align 8
  %57 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %56) #24
  %58 = icmp slt i32 %54, %57
  br i1 %58, label %59, label %88

59:                                               ; preds = %53
  %60 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 6
  %61 = load %struct.ListManager*, %struct.ListManager** %60, align 8
  %62 = load i32, i32* %12, align 4
  %63 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %61, i32 %62) #24
  %64 = load i32, i32* %63, align 4
  store i32 %64, i32* %13, align 4
  %65 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 7
  %66 = load %struct.ListManager*, %struct.ListManager** %65, align 8
  %67 = load i32, i32* %13, align 4
  %68 = call i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %66, i32 %67) #24
  store i8* %68, i8** %14, align 8
  store i32 0, i32* %15, align 4
  br label %69

69:                                               ; preds = %79, %59
  %70 = load i32, i32* %15, align 4
  %71 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 2
  %72 = load i32, i32* %71, align 4
  %73 = icmp slt i32 %70, %72
  br i1 %73, label %74, label %82

74:                                               ; preds = %69
  %75 = load i8*, i8** %14, align 8
  %76 = load i32, i32* %15, align 4
  %77 = sext i32 %76 to i64
  %78 = getelementptr inbounds i8, i8* %75, i64 %77
  store i8 0, i8* %78, align 1
  br label %79

79:                                               ; preds = %74
  %80 = load i32, i32* %15, align 4
  %81 = add nsw i32 %80, 1
  store i32 %81, i32* %15, align 4
  br label %69, !llvm.loop !49

82:                                               ; preds = %69
  %83 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 5
  %84 = load %struct.ListManager*, %struct.ListManager** %83, align 8
  call void @_ZN11ListManager9push_backIiEEvRKT_(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %84, i32* nonnull align 4 dereferenceable(4) %13) #24
  br label %85

85:                                               ; preds = %82
  %86 = load i32, i32* %12, align 4
  %87 = add nsw i32 %86, 1
  store i32 %87, i32* %12, align 4
  br label %53, !llvm.loop !50

88:                                               ; preds = %53
  %89 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %16, i32 0, i32 6
  %90 = load %struct.ListManager*, %struct.ListManager** %89, align 8
  call void @_ZN11ListManager5clearEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %90) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @gc_parallel_0(%struct.RuntimeContext* %0, i32 %1) #2 {
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %7 = alloca %struct.ListManager*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i32, align 4, addrspace(5)
  %11 = alloca i32, align 4, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %14 = addrspacecast i32 addrspace(5)* %4 to i32*
  %15 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %16 = addrspacecast %struct.NodeManager* addrspace(5)* %6 to %struct.NodeManager**
  %17 = addrspacecast %struct.ListManager* addrspace(5)* %7 to %struct.ListManager**
  %18 = addrspacecast i32 addrspace(5)* %8 to i32*
  %19 = addrspacecast i32 addrspace(5)* %9 to i32*
  %20 = addrspacecast i32 addrspace(5)* %10 to i32*
  %21 = addrspacecast i32 addrspace(5)* %11 to i32*
  %22 = addrspacecast i32 addrspace(5)* %12 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %13, align 8
  store i32 %1, i32* %14, align 4
  %23 = load %struct.RuntimeContext*, %struct.RuntimeContext** %13, align 8
  %24 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %23, i32 0, i32 0
  %25 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  store %struct.LLVMRuntime* %25, %struct.LLVMRuntime** %15, align 8
  %26 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %15, align 8
  %27 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %26, i32 0, i32 14
  %28 = load i32, i32* %14, align 4
  %29 = sext i32 %28 to i64
  %30 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %27, i64 0, i64 %29
  %31 = load %struct.NodeManager*, %struct.NodeManager** %30, align 8
  store %struct.NodeManager* %31, %struct.NodeManager** %16, align 8
  %32 = load %struct.NodeManager*, %struct.NodeManager** %16, align 8
  %33 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %32, i32 0, i32 5
  %34 = load %struct.ListManager*, %struct.ListManager** %33, align 8
  store %struct.ListManager* %34, %struct.ListManager** %17, align 8
  %35 = load %struct.ListManager*, %struct.ListManager** %17, align 8
  %36 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %35) #24
  store i32 %36, i32* %18, align 4
  %37 = load %struct.NodeManager*, %struct.NodeManager** %16, align 8
  %38 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %37, i32 0, i32 4
  %39 = load i32, i32* %38, align 4
  store i32 %39, i32* %19, align 4
  %40 = load %struct.RuntimeContext*, %struct.RuntimeContext** %13, align 8
  %41 = call i32 @linear_thread_idx(%struct.RuntimeContext* %40) #24
  store i32 %41, i32* %20, align 4
  %42 = load i32, i32* %19, align 4
  %43 = mul nsw i32 %42, 2
  %44 = load i32, i32* %18, align 4
  %45 = icmp sgt i32 %43, %44
  br i1 %45, label %46, label %70

46:                                               ; preds = %2
  %47 = load i32, i32* %18, align 4
  %48 = load i32, i32* %19, align 4
  %49 = sub nsw i32 %47, %48
  store i32 %49, i32* %21, align 4
  br label %50

50:                                               ; preds = %54, %46
  %51 = load i32, i32* %20, align 4
  %52 = load i32, i32* %21, align 4
  %53 = icmp slt i32 %51, %52
  br i1 %53, label %54, label %69

54:                                               ; preds = %50
  %55 = load %struct.ListManager*, %struct.ListManager** %17, align 8
  %56 = load i32, i32* %19, align 4
  %57 = load i32, i32* %20, align 4
  %58 = add nsw i32 %56, %57
  %59 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %55, i32 %58) #24
  %60 = load i32, i32* %59, align 4
  %61 = load %struct.ListManager*, %struct.ListManager** %17, align 8
  %62 = load i32, i32* %20, align 4
  %63 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %61, i32 %62) #24
  store i32 %60, i32* %63, align 4
  %64 = call i32 @grid_dim() #24
  %65 = call i32 @block_dim() #24
  %66 = mul nsw i32 %64, %65
  %67 = load i32, i32* %20, align 4
  %68 = add nsw i32 %67, %66
  store i32 %68, i32* %20, align 4
  br label %50, !llvm.loop !51

69:                                               ; preds = %50
  br label %94

70:                                               ; preds = %2
  %71 = load i32, i32* %19, align 4
  store i32 %71, i32* %22, align 4
  br label %72

72:                                               ; preds = %76, %70
  %73 = load i32, i32* %20, align 4
  %74 = load i32, i32* %22, align 4
  %75 = icmp slt i32 %73, %74
  br i1 %75, label %76, label %93

76:                                               ; preds = %72
  %77 = load %struct.ListManager*, %struct.ListManager** %17, align 8
  %78 = load i32, i32* %18, align 4
  %79 = load i32, i32* %22, align 4
  %80 = sub nsw i32 %78, %79
  %81 = load i32, i32* %20, align 4
  %82 = add nsw i32 %80, %81
  %83 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %77, i32 %82) #24
  %84 = load i32, i32* %83, align 4
  %85 = load %struct.ListManager*, %struct.ListManager** %17, align 8
  %86 = load i32, i32* %20, align 4
  %87 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %85, i32 %86) #24
  store i32 %84, i32* %87, align 4
  %88 = call i32 @grid_dim() #24
  %89 = call i32 @block_dim() #24
  %90 = mul nsw i32 %88, %89
  %91 = load i32, i32* %20, align 4
  %92 = add nsw i32 %91, %90
  store i32 %92, i32* %20, align 4
  br label %72, !llvm.loop !52

93:                                               ; preds = %72
  br label %94

94:                                               ; preds = %93, %69
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i32*, align 8, addrspace(5)
  %4 = alloca %struct.ListManager*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32* addrspace(5)* %3 to i32**
  %7 = addrspacecast %struct.ListManager* addrspace(5)* %4 to %struct.ListManager**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %9, i32 %10) #24
  %12 = bitcast i8* %11 to i32*
  ret i32* %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @gc_parallel_1(%struct.RuntimeContext* %0, i32 %1) #2 {
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %7 = alloca %struct.ListManager*, align 8, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %10 = addrspacecast i32 addrspace(5)* %4 to i32*
  %11 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %12 = addrspacecast %struct.NodeManager* addrspace(5)* %6 to %struct.NodeManager**
  %13 = addrspacecast %struct.ListManager* addrspace(5)* %7 to %struct.ListManager**
  %14 = addrspacecast i32 addrspace(5)* %8 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %9, align 8
  store i32 %1, i32* %10, align 4
  %15 = load %struct.RuntimeContext*, %struct.RuntimeContext** %9, align 8
  %16 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %15, i32 0, i32 0
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %16, align 8
  store %struct.LLVMRuntime* %17, %struct.LLVMRuntime** %11, align 8
  %18 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %11, align 8
  %19 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %18, i32 0, i32 14
  %20 = load i32, i32* %10, align 4
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %19, i64 0, i64 %21
  %23 = load %struct.NodeManager*, %struct.NodeManager** %22, align 8
  store %struct.NodeManager* %23, %struct.NodeManager** %12, align 8
  %24 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %25 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %24, i32 0, i32 5
  %26 = load %struct.ListManager*, %struct.ListManager** %25, align 8
  store %struct.ListManager* %26, %struct.ListManager** %13, align 8
  %27 = load %struct.ListManager*, %struct.ListManager** %13, align 8
  %28 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %27) #24
  %29 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %30 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %29, i32 0, i32 4
  %31 = load i32, i32* %30, align 4
  %32 = sub nsw i32 %28, %31
  %33 = call i32 @max_i32(i32 %32, i32 0) #24
  store i32 %33, i32* %14, align 4
  %34 = load %struct.ListManager*, %struct.ListManager** %13, align 8
  %35 = load i32, i32* %14, align 4
  call void @_ZN11ListManager6resizeEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %34, i32 %35) #24
  %36 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %37 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %36, i32 0, i32 4
  store i32 0, i32* %37, align 4
  %38 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %39 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %38, i32 0, i32 6
  %40 = load %struct.ListManager*, %struct.ListManager** %39, align 8
  %41 = call i32 @_ZN11ListManager4sizeEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %40) #24
  %42 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %43 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %42, i32 0, i32 8
  store i32 %41, i32* %43, align 8
  %44 = load %struct.NodeManager*, %struct.NodeManager** %12, align 8
  %45 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %44, i32 0, i32 6
  %46 = load %struct.ListManager*, %struct.ListManager** %45, align 8
  call void @_ZN11ListManager5clearEv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %46) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11ListManager6resizeEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %6 = addrspacecast i32 addrspace(5)* %4 to i32*
  store %struct.ListManager* %0, %struct.ListManager** %5, align 8
  store i32 %1, i32* %6, align 4
  %7 = load %struct.ListManager*, %struct.ListManager** %5, align 8
  %8 = load i32, i32* %6, align 4
  %9 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %7, i32 0, i32 5
  store i32 %8, i32* %9, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @gc_parallel_2(%struct.RuntimeContext* %0, i32 %1) #2 {
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %6 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca %struct.ListManager*, align 8, addrspace(5)
  %9 = alloca %struct.ListManager*, align 8, addrspace(5)
  %10 = alloca %struct.ListManager*, align 8, addrspace(5)
  %11 = alloca i32, align 4, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca i8*, align 8, addrspace(5)
  %15 = alloca i8*, align 8, addrspace(5)
  %16 = alloca i8*, align 8, addrspace(5)
  %17 = alloca i8*, align 8, addrspace(5)
  %18 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %19 = addrspacecast i32 addrspace(5)* %4 to i32*
  %20 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %5 to %struct.LLVMRuntime**
  %21 = addrspacecast %struct.NodeManager* addrspace(5)* %6 to %struct.NodeManager**
  %22 = addrspacecast i32 addrspace(5)* %7 to i32*
  %23 = addrspacecast %struct.ListManager* addrspace(5)* %8 to %struct.ListManager**
  %24 = addrspacecast %struct.ListManager* addrspace(5)* %9 to %struct.ListManager**
  %25 = addrspacecast %struct.ListManager* addrspace(5)* %10 to %struct.ListManager**
  %26 = addrspacecast i32 addrspace(5)* %11 to i32*
  %27 = addrspacecast i32 addrspace(5)* %12 to i32*
  %28 = addrspacecast i32 addrspace(5)* %13 to i32*
  %29 = addrspacecast i8* addrspace(5)* %14 to i8**
  %30 = addrspacecast i8* addrspace(5)* %15 to i8**
  %31 = addrspacecast i8* addrspace(5)* %16 to i8**
  %32 = addrspacecast i8* addrspace(5)* %17 to i8**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %18, align 8
  store i32 %1, i32* %19, align 4
  %33 = load %struct.RuntimeContext*, %struct.RuntimeContext** %18, align 8
  %34 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %33, i32 0, i32 0
  %35 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %34, align 8
  store %struct.LLVMRuntime* %35, %struct.LLVMRuntime** %20, align 8
  %36 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %20, align 8
  %37 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %36, i32 0, i32 14
  %38 = load i32, i32* %19, align 4
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds [1024 x %struct.NodeManager*], [1024 x %struct.NodeManager*]* %37, i64 0, i64 %39
  %41 = load %struct.NodeManager*, %struct.NodeManager** %40, align 8
  store %struct.NodeManager* %41, %struct.NodeManager** %21, align 8
  %42 = load %struct.NodeManager*, %struct.NodeManager** %21, align 8
  %43 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %42, i32 0, i32 8
  %44 = load i32, i32* %43, align 8
  store i32 %44, i32* %22, align 4
  %45 = load %struct.NodeManager*, %struct.NodeManager** %21, align 8
  %46 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %45, i32 0, i32 5
  %47 = load %struct.ListManager*, %struct.ListManager** %46, align 8
  store %struct.ListManager* %47, %struct.ListManager** %23, align 8
  %48 = load %struct.NodeManager*, %struct.NodeManager** %21, align 8
  %49 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %48, i32 0, i32 6
  %50 = load %struct.ListManager*, %struct.ListManager** %49, align 8
  store %struct.ListManager* %50, %struct.ListManager** %24, align 8
  %51 = load %struct.NodeManager*, %struct.NodeManager** %21, align 8
  %52 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %51, i32 0, i32 7
  %53 = load %struct.ListManager*, %struct.ListManager** %52, align 8
  store %struct.ListManager* %53, %struct.ListManager** %25, align 8
  %54 = load %struct.NodeManager*, %struct.NodeManager** %21, align 8
  %55 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %54, i32 0, i32 2
  %56 = load i32, i32* %55, align 4
  store i32 %56, i32* %26, align 4
  %57 = call i32 @block_idx() #24
  store i32 %57, i32* %27, align 4
  br label %58

58:                                               ; preds = %135, %2
  %59 = load i32, i32* %27, align 4
  %60 = load i32, i32* %22, align 4
  %61 = icmp slt i32 %59, %60
  br i1 %61, label %62, label %139

62:                                               ; preds = %58
  %63 = load %struct.ListManager*, %struct.ListManager** %24, align 8
  %64 = load i32, i32* %27, align 4
  %65 = call nonnull align 4 dereferenceable(4) i32* @_ZN11ListManager3getIiEERT_i(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %63, i32 %64) #24
  %66 = load i32, i32* %65, align 4
  store i32 %66, i32* %28, align 4
  %67 = load %struct.ListManager*, %struct.ListManager** %25, align 8
  %68 = load i32, i32* %28, align 4
  %69 = call i8* @_ZN11ListManager15get_element_ptrEi(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %67, i32 %68) #24
  store i8* %69, i8** %29, align 8
  %70 = call i32 @thread_idx() #24
  %71 = icmp eq i32 %70, 0
  br i1 %71, label %72, label %74

72:                                               ; preds = %62
  %73 = load %struct.ListManager*, %struct.ListManager** %23, align 8
  call void @_ZN11ListManager9push_backIiEEvRKT_(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %73, i32* nonnull align 4 dereferenceable(4) %28) #24
  br label %74

74:                                               ; preds = %72, %62
  %75 = load i8*, i8** %29, align 8
  %76 = load i32, i32* %26, align 4
  %77 = sext i32 %76 to i64
  %78 = getelementptr inbounds i8, i8* %75, i64 %77
  store i8* %78, i8** %30, align 8
  %79 = load i8*, i8** %29, align 8
  %80 = ptrtoint i8* %79 to i64
  %81 = urem i64 %80, 4
  %82 = icmp ne i64 %81, 0
  br i1 %82, label %83, label %107

83:                                               ; preds = %74
  %84 = load i8*, i8** %29, align 8
  %85 = getelementptr inbounds i8, i8* %84, i64 4
  %86 = load i8*, i8** %29, align 8
  %87 = ptrtoint i8* %86 to i64
  %88 = urem i64 %87, 4
  %89 = sub i64 0, %88
  %90 = getelementptr inbounds i8, i8* %85, i64 %89
  store i8* %90, i8** %31, align 8
  %91 = call i32 @thread_idx() #24
  %92 = icmp eq i32 %91, 0
  br i1 %92, label %93, label %105

93:                                               ; preds = %83
  %94 = load i8*, i8** %29, align 8
  store i8* %94, i8** %32, align 8
  br label %95

95:                                               ; preds = %101, %93
  %96 = load i8*, i8** %32, align 8
  %97 = load i8*, i8** %31, align 8
  %98 = icmp ult i8* %96, %97
  br i1 %98, label %99, label %104

99:                                               ; preds = %95
  %100 = load i8*, i8** %32, align 8
  store i8 0, i8* %100, align 1
  br label %101

101:                                              ; preds = %99
  %102 = load i8*, i8** %32, align 8
  %103 = getelementptr inbounds i8, i8* %102, i32 1
  store i8* %103, i8** %32, align 8
  br label %95, !llvm.loop !53

104:                                              ; preds = %95
  br label %105

105:                                              ; preds = %104, %83
  %106 = load i8*, i8** %31, align 8
  store i8* %106, i8** %29, align 8
  br label %107

107:                                              ; preds = %105, %74
  %108 = call i32 @thread_idx() #24
  %109 = sext i32 %108 to i64
  %110 = mul i64 %109, 4
  %111 = load i8*, i8** %29, align 8
  %112 = getelementptr inbounds i8, i8* %111, i64 %110
  store i8* %112, i8** %29, align 8
  br label %113

113:                                              ; preds = %118, %107
  %114 = load i8*, i8** %29, align 8
  %115 = getelementptr inbounds i8, i8* %114, i64 4
  %116 = load i8*, i8** %30, align 8
  %117 = icmp ule i8* %115, %116
  br i1 %117, label %118, label %126

118:                                              ; preds = %113
  %119 = load i8*, i8** %29, align 8
  %120 = bitcast i8* %119 to i32*
  store i32 0, i32* %120, align 4
  %121 = call i32 @block_dim() #24
  %122 = sext i32 %121 to i64
  %123 = mul i64 4, %122
  %124 = load i8*, i8** %29, align 8
  %125 = getelementptr inbounds i8, i8* %124, i64 %123
  store i8* %125, i8** %29, align 8
  br label %113, !llvm.loop !54

126:                                              ; preds = %113
  br label %127

127:                                              ; preds = %131, %126
  %128 = load i8*, i8** %29, align 8
  %129 = load i8*, i8** %30, align 8
  %130 = icmp ult i8* %128, %129
  br i1 %130, label %131, label %135

131:                                              ; preds = %127
  %132 = load i8*, i8** %29, align 8
  store i8 0, i8* %132, align 1
  %133 = load i8*, i8** %29, align 8
  %134 = getelementptr inbounds i8, i8* %133, i32 1
  store i8* %134, i8** %29, align 8
  br label %127, !llvm.loop !55

135:                                              ; preds = %127
  %136 = call i32 @grid_dim() #24
  %137 = load i32, i32* %27, align 4
  %138 = add nsw i32 %137, %136
  store i32 %138, i32* %27, align 4
  br label %58, !llvm.loop !56

139:                                              ; preds = %58
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden void @_ZN11ListManager9push_backIiEEvRKT_(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, i32* nonnull align 4 dereferenceable(4) %1) #2 comdat align 2 {
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %6 = addrspacecast i32* addrspace(5)* %4 to i32**
  store %struct.ListManager* %0, %struct.ListManager** %5, align 8
  store i32* %1, i32** %6, align 8
  %7 = load %struct.ListManager*, %struct.ListManager** %5, align 8
  %8 = load i32*, i32** %6, align 8
  %9 = bitcast i32* %8 to i8*
  call void @_ZN11ListManager6appendEPv(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %7, i8* %9) #24
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden i32 @rand_u32(%struct.RuntimeContext* %0) #2 {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = alloca %struct.RandState*, align 8, addrspace(5)
  %5 = alloca i32*, align 8, addrspace(5)
  %6 = alloca i32*, align 8, addrspace(5)
  %7 = alloca i32*, align 8, addrspace(5)
  %8 = alloca i32*, align 8, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = addrspacecast i32 addrspace(5)* %2 to i32*
  %11 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  %12 = addrspacecast %struct.RandState* addrspace(5)* %4 to %struct.RandState**
  %13 = addrspacecast i32* addrspace(5)* %5 to i32**
  %14 = addrspacecast i32* addrspace(5)* %6 to i32**
  %15 = addrspacecast i32* addrspace(5)* %7 to i32**
  %16 = addrspacecast i32* addrspace(5)* %8 to i32**
  %17 = addrspacecast i32 addrspace(5)* %9 to i32*
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %11, align 8
  %18 = load %struct.RuntimeContext*, %struct.RuntimeContext** %11, align 8
  %19 = getelementptr inbounds %struct.RuntimeContext, %struct.RuntimeContext* %18, i32 0, i32 0
  %20 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %19, align 8
  %21 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %20, i32 0, i32 17
  %22 = load %struct.RandState*, %struct.RandState** %21, align 8
  %23 = load %struct.RuntimeContext*, %struct.RuntimeContext** %11, align 8
  %24 = call i32 @linear_thread_idx(%struct.RuntimeContext* %23) #24
  %25 = sext i32 %24 to i64
  %26 = getelementptr inbounds %struct.RandState, %struct.RandState* %22, i64 %25
  store %struct.RandState* %26, %struct.RandState** %12, align 8
  %27 = load %struct.RandState*, %struct.RandState** %12, align 8
  %28 = getelementptr inbounds %struct.RandState, %struct.RandState* %27, i32 0, i32 0
  store i32* %28, i32** %13, align 8
  %29 = load %struct.RandState*, %struct.RandState** %12, align 8
  %30 = getelementptr inbounds %struct.RandState, %struct.RandState* %29, i32 0, i32 1
  store i32* %30, i32** %14, align 8
  %31 = load %struct.RandState*, %struct.RandState** %12, align 8
  %32 = getelementptr inbounds %struct.RandState, %struct.RandState* %31, i32 0, i32 2
  store i32* %32, i32** %15, align 8
  %33 = load %struct.RandState*, %struct.RandState** %12, align 8
  %34 = getelementptr inbounds %struct.RandState, %struct.RandState* %33, i32 0, i32 3
  store i32* %34, i32** %16, align 8
  %35 = load i32*, i32** %13, align 8
  %36 = load i32, i32* %35, align 4
  %37 = load i32*, i32** %13, align 8
  %38 = load i32, i32* %37, align 4
  %39 = shl i32 %38, 11
  %40 = xor i32 %36, %39
  store i32 %40, i32* %17, align 4
  %41 = load i32*, i32** %14, align 8
  %42 = load i32, i32* %41, align 4
  %43 = load i32*, i32** %13, align 8
  store i32 %42, i32* %43, align 4
  %44 = load i32*, i32** %15, align 8
  %45 = load i32, i32* %44, align 4
  %46 = load i32*, i32** %14, align 8
  store i32 %45, i32* %46, align 4
  %47 = load i32*, i32** %16, align 8
  %48 = load i32, i32* %47, align 4
  %49 = load i32*, i32** %15, align 8
  store i32 %48, i32* %49, align 4
  %50 = load i32*, i32** %16, align 8
  %51 = load i32, i32* %50, align 4
  %52 = load i32*, i32** %16, align 8
  %53 = load i32, i32* %52, align 4
  %54 = lshr i32 %53, 19
  %55 = xor i32 %51, %54
  %56 = load i32, i32* %17, align 4
  %57 = load i32, i32* %17, align 4
  %58 = lshr i32 %57, 8
  %59 = xor i32 %56, %58
  %60 = xor i32 %55, %59
  %61 = load i32*, i32** %16, align 8
  store i32 %60, i32* %61, align 4
  %62 = load i32*, i32** %16, align 8
  %63 = load i32, i32* %62, align 4
  %64 = mul i32 %63, 1000000007
  ret i32 %64
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden float @rand_f32(%struct.RuntimeContext* %0) #2 {
  %2 = alloca float, align 4, addrspace(5)
  %3 = alloca %struct.RuntimeContext*, align 8, addrspace(5)
  %4 = addrspacecast float addrspace(5)* %2 to float*
  %5 = addrspacecast %struct.RuntimeContext* addrspace(5)* %3 to %struct.RuntimeContext**
  store %struct.RuntimeContext* %0, %struct.RuntimeContext** %5, align 8
  %6 = load %struct.RuntimeContext*, %struct.RuntimeContext** %5, align 8
  %7 = call i32 @rand_u32(%struct.RuntimeContext* %6) #24
  %8 = lshr i32 %7, 8
  %9 = uitofp i32 %8 to float
  %10 = fmul contract float %9, 0x3E70000000000000
  ret float %10
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @set_mask_b8(i8* %0, i64 %1, i8 zeroext %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i8, align 1, addrspace(5)
  %7 = alloca i8, align 1, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i8 addrspace(5)* %6 to i8*
  %11 = addrspacecast i8 addrspace(5)* %7 to i8*
  store i8* %0, i8** %8, align 8
  store i64 %1, i64* %9, align 8
  store i8 %2, i8* %10, align 1
  %12 = load i64, i64* %9, align 8
  %13 = trunc i64 %12 to i8
  store i8 %13, i8* %11, align 1
  %14 = load i8*, i8** %8, align 8
  %15 = load i8, i8* %14, align 1
  %16 = zext i8 %15 to i32
  %17 = load i8, i8* %11, align 1
  %18 = zext i8 %17 to i32
  %19 = xor i32 %18, -1
  %20 = and i32 %16, %19
  %21 = sext i32 %20 to i64
  %22 = load i8, i8* %10, align 1
  %23 = zext i8 %22 to i64
  %24 = load i64, i64* %9, align 8
  %25 = and i64 %23, %24
  %26 = or i64 %21, %25
  %27 = trunc i64 %26 to i8
  %28 = load i8*, i8** %8, align 8
  store i8 %27, i8* %28, align 1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @atomic_set_mask_b8(i8* %0, i64 %1, i8 zeroext %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i8, align 1, addrspace(5)
  %7 = alloca i8, align 1, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = alloca i8, align 1, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = addrspacecast i8* addrspace(5)* %4 to i8**
  %12 = addrspacecast i64 addrspace(5)* %5 to i64*
  %13 = addrspacecast i8 addrspace(5)* %6 to i8*
  %14 = addrspacecast i8 addrspace(5)* %7 to i8*
  %15 = addrspacecast i8 addrspace(5)* %8 to i8*
  %16 = addrspacecast i8 addrspace(5)* %9 to i8*
  %17 = addrspacecast i8 addrspace(5)* %10 to i8*
  store i8* %0, i8** %11, align 8
  store i64 %1, i64* %12, align 8
  store i8 %2, i8* %13, align 1
  %18 = load i64, i64* %12, align 8
  %19 = trunc i64 %18 to i8
  store i8 %19, i8* %14, align 1
  store i8 0, i8* %15, align 1
  %20 = load i8*, i8** %11, align 8
  %21 = load i8, i8* %20, align 1
  store i8 %21, i8* %16, align 1
  br label %22

22:                                               ; preds = %46, %3
  %23 = load i8*, i8** %11, align 8
  %24 = load i8, i8* %23, align 1
  store i8 %24, i8* %16, align 1
  %25 = load i8, i8* %16, align 1
  %26 = zext i8 %25 to i32
  %27 = load i8, i8* %14, align 1
  %28 = zext i8 %27 to i32
  %29 = xor i32 %28, -1
  %30 = and i32 %26, %29
  %31 = sext i32 %30 to i64
  %32 = load i8, i8* %13, align 1
  %33 = zext i8 %32 to i64
  %34 = load i64, i64* %12, align 8
  %35 = and i64 %33, %34
  %36 = or i64 %31, %35
  %37 = trunc i64 %36 to i8
  store i8 %37, i8* %15, align 1
  br label %38

38:                                               ; preds = %22
  %39 = load i8*, i8** %11, align 8
  %40 = load i8, i8* %16, align 1
  %41 = load i8, i8* %15, align 1
  %42 = cmpxchg weak i8* %39, i8 %40, i8 %41 seq_cst seq_cst, align 1
  %43 = extractvalue { i8, i1 } %42, 0
  %44 = extractvalue { i8, i1 } %42, 1
  br i1 %44, label %46, label %45

45:                                               ; preds = %38
  store i8 %43, i8* %16, align 1
  br label %46

46:                                               ; preds = %45, %38
  %47 = zext i1 %44 to i8
  store i8 %47, i8* %17, align 1
  %48 = load i8, i8* %17, align 1
  %49 = trunc i8 %48 to i1
  %50 = xor i1 %49, true
  br i1 %50, label %22, label %51, !llvm.loop !57

51:                                               ; preds = %46
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @set_mask_b16(i16* %0, i64 %1, i16 zeroext %2) #2 {
  %4 = alloca i16*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i16, align 2, addrspace(5)
  %7 = alloca i16, align 2, addrspace(5)
  %8 = addrspacecast i16* addrspace(5)* %4 to i16**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i16 addrspace(5)* %6 to i16*
  %11 = addrspacecast i16 addrspace(5)* %7 to i16*
  store i16* %0, i16** %8, align 8
  store i64 %1, i64* %9, align 8
  store i16 %2, i16* %10, align 2
  %12 = load i64, i64* %9, align 8
  %13 = trunc i64 %12 to i16
  store i16 %13, i16* %11, align 2
  %14 = load i16*, i16** %8, align 8
  %15 = load i16, i16* %14, align 2
  %16 = zext i16 %15 to i32
  %17 = load i16, i16* %11, align 2
  %18 = zext i16 %17 to i32
  %19 = xor i32 %18, -1
  %20 = and i32 %16, %19
  %21 = sext i32 %20 to i64
  %22 = load i16, i16* %10, align 2
  %23 = zext i16 %22 to i64
  %24 = load i64, i64* %9, align 8
  %25 = and i64 %23, %24
  %26 = or i64 %21, %25
  %27 = trunc i64 %26 to i16
  %28 = load i16*, i16** %8, align 8
  store i16 %27, i16* %28, align 2
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @atomic_set_mask_b16(i16* %0, i64 %1, i16 zeroext %2) #2 {
  %4 = alloca i16*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i16, align 2, addrspace(5)
  %7 = alloca i16, align 2, addrspace(5)
  %8 = alloca i16, align 2, addrspace(5)
  %9 = alloca i16, align 2, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = addrspacecast i16* addrspace(5)* %4 to i16**
  %12 = addrspacecast i64 addrspace(5)* %5 to i64*
  %13 = addrspacecast i16 addrspace(5)* %6 to i16*
  %14 = addrspacecast i16 addrspace(5)* %7 to i16*
  %15 = addrspacecast i16 addrspace(5)* %8 to i16*
  %16 = addrspacecast i16 addrspace(5)* %9 to i16*
  %17 = addrspacecast i8 addrspace(5)* %10 to i8*
  store i16* %0, i16** %11, align 8
  store i64 %1, i64* %12, align 8
  store i16 %2, i16* %13, align 2
  %18 = load i64, i64* %12, align 8
  %19 = trunc i64 %18 to i16
  store i16 %19, i16* %14, align 2
  store i16 0, i16* %15, align 2
  %20 = load i16*, i16** %11, align 8
  %21 = load i16, i16* %20, align 2
  store i16 %21, i16* %16, align 2
  br label %22

22:                                               ; preds = %46, %3
  %23 = load i16*, i16** %11, align 8
  %24 = load i16, i16* %23, align 2
  store i16 %24, i16* %16, align 2
  %25 = load i16, i16* %16, align 2
  %26 = zext i16 %25 to i32
  %27 = load i16, i16* %14, align 2
  %28 = zext i16 %27 to i32
  %29 = xor i32 %28, -1
  %30 = and i32 %26, %29
  %31 = sext i32 %30 to i64
  %32 = load i16, i16* %13, align 2
  %33 = zext i16 %32 to i64
  %34 = load i64, i64* %12, align 8
  %35 = and i64 %33, %34
  %36 = or i64 %31, %35
  %37 = trunc i64 %36 to i16
  store i16 %37, i16* %15, align 2
  br label %38

38:                                               ; preds = %22
  %39 = load i16*, i16** %11, align 8
  %40 = load i16, i16* %16, align 2
  %41 = load i16, i16* %15, align 2
  %42 = cmpxchg weak i16* %39, i16 %40, i16 %41 seq_cst seq_cst, align 2
  %43 = extractvalue { i16, i1 } %42, 0
  %44 = extractvalue { i16, i1 } %42, 1
  br i1 %44, label %46, label %45

45:                                               ; preds = %38
  store i16 %43, i16* %16, align 2
  br label %46

46:                                               ; preds = %45, %38
  %47 = zext i1 %44 to i8
  store i8 %47, i8* %17, align 1
  %48 = load i8, i8* %17, align 1
  %49 = trunc i8 %48 to i1
  %50 = xor i1 %49, true
  br i1 %50, label %22, label %51, !llvm.loop !58

51:                                               ; preds = %46
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @set_mask_b32(i32* %0, i64 %1, i32 %2) #2 {
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = addrspacecast i32* addrspace(5)* %4 to i32**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i32 addrspace(5)* %6 to i32*
  %11 = addrspacecast i32 addrspace(5)* %7 to i32*
  store i32* %0, i32** %8, align 8
  store i64 %1, i64* %9, align 8
  store i32 %2, i32* %10, align 4
  %12 = load i64, i64* %9, align 8
  %13 = trunc i64 %12 to i32
  store i32 %13, i32* %11, align 4
  %14 = load i32*, i32** %8, align 8
  %15 = load i32, i32* %14, align 4
  %16 = load i32, i32* %11, align 4
  %17 = xor i32 %16, -1
  %18 = and i32 %15, %17
  %19 = zext i32 %18 to i64
  %20 = load i32, i32* %10, align 4
  %21 = zext i32 %20 to i64
  %22 = load i64, i64* %9, align 8
  %23 = and i64 %21, %22
  %24 = or i64 %19, %23
  %25 = trunc i64 %24 to i32
  %26 = load i32*, i32** %8, align 8
  store i32 %25, i32* %26, align 4
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @atomic_set_mask_b32(i32* %0, i64 %1, i32 %2) #2 {
  %4 = alloca i32*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = addrspacecast i32* addrspace(5)* %4 to i32**
  %12 = addrspacecast i64 addrspace(5)* %5 to i64*
  %13 = addrspacecast i32 addrspace(5)* %6 to i32*
  %14 = addrspacecast i32 addrspace(5)* %7 to i32*
  %15 = addrspacecast i32 addrspace(5)* %8 to i32*
  %16 = addrspacecast i32 addrspace(5)* %9 to i32*
  %17 = addrspacecast i8 addrspace(5)* %10 to i8*
  store i32* %0, i32** %11, align 8
  store i64 %1, i64* %12, align 8
  store i32 %2, i32* %13, align 4
  %18 = load i64, i64* %12, align 8
  %19 = trunc i64 %18 to i32
  store i32 %19, i32* %14, align 4
  store i32 0, i32* %15, align 4
  %20 = load i32*, i32** %11, align 8
  %21 = load i32, i32* %20, align 4
  store i32 %21, i32* %16, align 4
  br label %22

22:                                               ; preds = %44, %3
  %23 = load i32*, i32** %11, align 8
  %24 = load i32, i32* %23, align 4
  store i32 %24, i32* %16, align 4
  %25 = load i32, i32* %16, align 4
  %26 = load i32, i32* %14, align 4
  %27 = xor i32 %26, -1
  %28 = and i32 %25, %27
  %29 = zext i32 %28 to i64
  %30 = load i32, i32* %13, align 4
  %31 = zext i32 %30 to i64
  %32 = load i64, i64* %12, align 8
  %33 = and i64 %31, %32
  %34 = or i64 %29, %33
  %35 = trunc i64 %34 to i32
  store i32 %35, i32* %15, align 4
  br label %36

36:                                               ; preds = %22
  %37 = load i32*, i32** %11, align 8
  %38 = load i32, i32* %16, align 4
  %39 = load i32, i32* %15, align 4
  %40 = cmpxchg weak i32* %37, i32 %38, i32 %39 seq_cst seq_cst, align 4
  %41 = extractvalue { i32, i1 } %40, 0
  %42 = extractvalue { i32, i1 } %40, 1
  br i1 %42, label %44, label %43

43:                                               ; preds = %36
  store i32 %41, i32* %16, align 4
  br label %44

44:                                               ; preds = %43, %36
  %45 = zext i1 %42 to i8
  store i8 %45, i8* %17, align 1
  %46 = load i8, i8* %17, align 1
  %47 = trunc i8 %46 to i1
  %48 = xor i1 %47, true
  br i1 %48, label %22, label %49, !llvm.loop !59

49:                                               ; preds = %44
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @set_mask_b64(i64* %0, i64 %1, i64 %2) #2 {
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i64* addrspace(5)* %4 to i64**
  %9 = addrspacecast i64 addrspace(5)* %5 to i64*
  %10 = addrspacecast i64 addrspace(5)* %6 to i64*
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i64* %0, i64** %8, align 8
  store i64 %1, i64* %9, align 8
  store i64 %2, i64* %10, align 8
  %12 = load i64, i64* %9, align 8
  store i64 %12, i64* %11, align 8
  %13 = load i64*, i64** %8, align 8
  %14 = load i64, i64* %13, align 8
  %15 = load i64, i64* %11, align 8
  %16 = xor i64 %15, -1
  %17 = and i64 %14, %16
  %18 = load i64, i64* %10, align 8
  %19 = load i64, i64* %9, align 8
  %20 = and i64 %18, %19
  %21 = or i64 %17, %20
  %22 = load i64*, i64** %8, align 8
  store i64 %21, i64* %22, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define hidden void @atomic_set_mask_b64(i64* %0, i64 %1, i64 %2) #2 {
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64, align 8, addrspace(5)
  %6 = alloca i64, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i64, align 8, addrspace(5)
  %9 = alloca i64, align 8, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = addrspacecast i64* addrspace(5)* %4 to i64**
  %12 = addrspacecast i64 addrspace(5)* %5 to i64*
  %13 = addrspacecast i64 addrspace(5)* %6 to i64*
  %14 = addrspacecast i64 addrspace(5)* %7 to i64*
  %15 = addrspacecast i64 addrspace(5)* %8 to i64*
  %16 = addrspacecast i64 addrspace(5)* %9 to i64*
  %17 = addrspacecast i8 addrspace(5)* %10 to i8*
  store i64* %0, i64** %11, align 8
  store i64 %1, i64* %12, align 8
  store i64 %2, i64* %13, align 8
  %18 = load i64, i64* %12, align 8
  store i64 %18, i64* %14, align 8
  store i64 0, i64* %15, align 8
  %19 = load i64*, i64** %11, align 8
  %20 = load i64, i64* %19, align 8
  store i64 %20, i64* %16, align 8
  br label %21

21:                                               ; preds = %40, %3
  %22 = load i64*, i64** %11, align 8
  %23 = load i64, i64* %22, align 8
  store i64 %23, i64* %16, align 8
  %24 = load i64, i64* %16, align 8
  %25 = load i64, i64* %14, align 8
  %26 = xor i64 %25, -1
  %27 = and i64 %24, %26
  %28 = load i64, i64* %13, align 8
  %29 = load i64, i64* %12, align 8
  %30 = and i64 %28, %29
  %31 = or i64 %27, %30
  store i64 %31, i64* %15, align 8
  br label %32

32:                                               ; preds = %21
  %33 = load i64*, i64** %11, align 8
  %34 = load i64, i64* %16, align 8
  %35 = load i64, i64* %15, align 8
  %36 = cmpxchg weak i64* %33, i64 %34, i64 %35 seq_cst seq_cst, align 8
  %37 = extractvalue { i64, i1 } %36, 0
  %38 = extractvalue { i64, i1 } %36, 1
  br i1 %38, label %40, label %39

39:                                               ; preds = %32
  store i64 %37, i64* %16, align 8
  br label %40

40:                                               ; preds = %39, %32
  %41 = zext i1 %38 to i8
  store i8 %41, i8* %17, align 1
  %42 = load i8, i8* %17, align 1
  %43 = trunc i8 %42 to i1
  %44 = xor i1 %43, true
  br i1 %44, label %21, label %45, !llvm.loop !60

45:                                               ; preds = %40
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImlET_T0_(i64 %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca %union.anon, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast %union.anon addrspace(5)* %4 to %union.anon*
  store i64 %0, i64* %6, align 8
  %8 = load i64, i64* %6, align 8
  %9 = bitcast %union.anon* %7 to i64*
  store i64 %8, i64* %9, align 8
  %10 = bitcast %union.anon* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImcET_T0_(i8 signext %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i8, align 1, addrspace(5)
  %4 = alloca %union.anon.5, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast i8 addrspace(5)* %3 to i8*
  %7 = addrspacecast %union.anon.5 addrspace(5)* %4 to %union.anon.5*
  store i8 %0, i8* %6, align 1
  %8 = load i8, i8* %6, align 1
  %9 = bitcast %union.anon.5* %7 to i8*
  store i8 %8, i8* %9, align 8
  %10 = bitcast %union.anon.5* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImmET_T0_(i64 %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca %union.anon.6, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast %union.anon.6 addrspace(5)* %4 to %union.anon.6*
  store i64 %0, i64* %6, align 8
  %8 = load i64, i64* %6, align 8
  %9 = bitcast %union.anon.6* %7 to i64*
  store i64 %8, i64* %9, align 8
  %10 = bitcast %union.anon.6* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImiET_T0_(i32 %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %union.anon.7, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %union.anon.7 addrspace(5)* %4 to %union.anon.7*
  store i32 %0, i32* %6, align 4
  %8 = load i32, i32* %6, align 4
  %9 = bitcast %union.anon.7* %7 to i32*
  store i32 %8, i32* %9, align 8
  %10 = bitcast %union.anon.7* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImP11NodeManagerET_T0_(%struct.NodeManager* %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %4 = alloca %union.anon.8, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast %struct.NodeManager* addrspace(5)* %3 to %struct.NodeManager**
  %7 = addrspacecast %union.anon.8 addrspace(5)* %4 to %union.anon.8*
  store %struct.NodeManager* %0, %struct.NodeManager** %6, align 8
  %8 = load %struct.NodeManager*, %struct.NodeManager** %6, align 8
  %9 = bitcast %union.anon.8* %7 to %struct.NodeManager**
  store %struct.NodeManager* %8, %struct.NodeManager** %9, align 8
  %10 = bitcast %union.anon.8* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImP11ListManagerET_T0_(%struct.ListManager* %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.ListManager*, align 8, addrspace(5)
  %4 = alloca %union.anon.9, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast %struct.ListManager* addrspace(5)* %3 to %struct.ListManager**
  %7 = addrspacecast %union.anon.9 addrspace(5)* %4 to %union.anon.9*
  store %struct.ListManager* %0, %struct.ListManager** %6, align 8
  %8 = load %struct.ListManager*, %struct.ListManager** %6, align 8
  %9 = bitcast %union.anon.9* %7 to %struct.ListManager**
  store %struct.ListManager* %8, %struct.ListManager** %9, align 8
  %10 = bitcast %union.anon.9* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EvS2_S5_RKT0_(i8* %0, %class.anon* nonnull align 8 dereferenceable(32) %1, %class.anon.10* nonnull align 1 dereferenceable(1) %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %class.anon*, align 8, addrspace(5)
  %6 = alloca %class.anon.10*, align 8, addrspace(5)
  %7 = alloca %class.lock_guard, align 1, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast %class.anon* addrspace(5)* %5 to %class.anon**
  %10 = addrspacecast %class.anon.10* addrspace(5)* %6 to %class.anon.10**
  %11 = addrspacecast %class.lock_guard addrspace(5)* %7 to %class.lock_guard*
  store i8* %0, i8** %8, align 8
  store %class.anon* %1, %class.anon** %9, align 8
  store %class.anon.10* %2, %class.anon.10** %10, align 8
  %12 = load i8*, i8** %8, align 8
  %13 = load %class.anon*, %class.anon** %9, align 8
  %14 = load %class.anon.10*, %class.anon.10** %10, align 8
  call void @_ZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC2EPhRKS0_RKS6_(%class.lock_guard* nonnull align 1 dereferenceable(1) %11, i8* %12, %class.anon* nonnull align 8 dereferenceable(32) %13, %class.anon.10* nonnull align 1 dereferenceable(1) %14) #24
  ret void
}

; Function Attrs: convergent noinline nounwind optnone
define internal void @_ZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC2EPhRKS0_RKS6_(%class.lock_guard* nonnull align 1 dereferenceable(1) %0, i8* %1, %class.anon* nonnull align 8 dereferenceable(32) %2, %class.anon.10* nonnull align 1 dereferenceable(1) %3) unnamed_addr #5 align 2 {
  %5 = alloca %class.lock_guard*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca %class.anon*, align 8, addrspace(5)
  %8 = alloca %class.anon.10*, align 8, addrspace(5)
  %9 = alloca %class.anon.11, align 8, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = alloca i8, align 1, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca i32, align 4, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = addrspacecast %class.lock_guard* addrspace(5)* %5 to %class.lock_guard**
  %17 = addrspacecast i8* addrspace(5)* %6 to i8**
  %18 = addrspacecast %class.anon* addrspace(5)* %7 to %class.anon**
  %19 = addrspacecast %class.anon.10* addrspace(5)* %8 to %class.anon.10**
  %20 = addrspacecast %class.anon.11 addrspace(5)* %9 to %class.anon.11*
  %21 = addrspacecast i8 addrspace(5)* %10 to i8*
  %22 = addrspacecast i8 addrspace(5)* %11 to i8*
  %23 = addrspacecast i32 addrspace(5)* %12 to i32*
  %24 = addrspacecast i32 addrspace(5)* %13 to i32*
  %25 = addrspacecast i32 addrspace(5)* %14 to i32*
  %26 = addrspacecast i32 addrspace(5)* %15 to i32*
  store %class.lock_guard* %0, %class.lock_guard** %16, align 8
  store i8* %1, i8** %17, align 8
  store %class.anon* %2, %class.anon** %18, align 8
  store %class.anon.10* %3, %class.anon.10** %19, align 8
  %27 = load %class.lock_guard*, %class.lock_guard** %16, align 8
  %28 = getelementptr inbounds %class.anon.11, %class.anon.11* %20, i32 0, i32 0
  %29 = load %class.anon.10*, %class.anon.10** %19, align 8
  store %class.anon.10* %29, %class.anon.10** %28, align 8
  %30 = getelementptr inbounds %class.anon.11, %class.anon.11* %20, i32 0, i32 1
  store i8** %17, i8*** %30, align 8
  %31 = getelementptr inbounds %class.anon.11, %class.anon.11* %20, i32 0, i32 2
  %32 = load %class.anon*, %class.anon** %18, align 8
  store %class.anon* %32, %class.anon** %31, align 8
  %33 = call i32 @cuda_compute_capability() #24
  %34 = icmp slt i32 %33, 70
  br i1 %34, label %35, label %88

35:                                               ; preds = %4
  store i8 0, i8* %21, align 1
  br label %36

36:                                               ; preds = %48, %35
  %37 = load i8, i8* %21, align 1
  %38 = trunc i8 %37 to i1
  %39 = xor i1 %38, true
  br i1 %39, label %40, label %49

40:                                               ; preds = %36
  %41 = load i8*, i8** %17, align 8
  %42 = bitcast i8* %41 to i32*
  %43 = call i32 @atomic_exchange_i32(i32* %42, i32 1) #24
  %44 = icmp eq i32 %43, 1
  br i1 %44, label %45, label %48

45:                                               ; preds = %40
  %46 = load %class.anon*, %class.anon** %18, align 8
  call void @_ZZ20taichi_assert_formatENKUlvE_clEv(%class.anon* nonnull align 8 dereferenceable(32) %46) #24
  store i8 1, i8* %21, align 1
  %47 = load i8*, i8** %17, align 8
  call void @mutex_unlock_i32(i8* %47) #24
  br label %48

48:                                               ; preds = %45, %40
  br label %36, !llvm.loop !61

49:                                               ; preds = %36
  store i8 0, i8* %22, align 1
  %50 = load i8, i8* %22, align 1
  %51 = trunc i8 %50 to i1
  br i1 %51, label %52, label %72

52:                                               ; preds = %49
  %53 = call i32 @cuda_active_mask() #24
  store i32 %53, i32* %23, align 4
  %54 = load i32, i32* %23, align 4
  store i32 %54, i32* %24, align 4
  br label %55

55:                                               ; preds = %65, %52
  %56 = load i32, i32* %24, align 4
  %57 = icmp ne i32 %56, 0
  br i1 %57, label %58, label %71

58:                                               ; preds = %55
  %59 = load i32, i32* %24, align 4
  %60 = call i32 @cttz_i32(i32 %59) #24
  store i32 %60, i32* %25, align 4
  %61 = call i32 @warp_idx() #24
  %62 = load i32, i32* %25, align 4
  %63 = icmp eq i32 %61, %62
  br i1 %63, label %64, label %65

64:                                               ; preds = %58
  call void @_ZZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC1EPhRKS0_RKS6_ENKUlvE_clEv(%class.anon.11* nonnull align 8 dereferenceable(24) %20) #24
  br label %65

65:                                               ; preds = %64, %58
  %66 = load i32, i32* %25, align 4
  %67 = shl i32 1, %66
  %68 = xor i32 %67, -1
  %69 = load i32, i32* %24, align 4
  %70 = and i32 %69, %68
  store i32 %70, i32* %24, align 4
  br label %55, !llvm.loop !62

71:                                               ; preds = %55
  br label %87

72:                                               ; preds = %49
  store i32 0, i32* %26, align 4
  br label %73

73:                                               ; preds = %83, %72
  %74 = load i32, i32* %26, align 4
  %75 = call i32 @warp_size() #24
  %76 = icmp slt i32 %74, %75
  br i1 %76, label %77, label %86

77:                                               ; preds = %73
  %78 = call i32 @warp_idx() #24
  %79 = load i32, i32* %26, align 4
  %80 = icmp eq i32 %78, %79
  br i1 %80, label %81, label %82

81:                                               ; preds = %77
  call void @_ZZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC1EPhRKS0_RKS6_ENKUlvE_clEv(%class.anon.11* nonnull align 8 dereferenceable(24) %20) #24
  br label %82

82:                                               ; preds = %81, %77
  br label %83

83:                                               ; preds = %82
  %84 = load i32, i32* %26, align 4
  %85 = add nsw i32 %84, 1
  store i32 %85, i32* %26, align 4
  br label %73, !llvm.loop !63

86:                                               ; preds = %73
  br label %87

87:                                               ; preds = %86, %71
  br label %89

88:                                               ; preds = %4
  call void @_ZZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC1EPhRKS0_RKS6_ENKUlvE_clEv(%class.anon.11* nonnull align 8 dereferenceable(24) %20) #24
  br label %89

89:                                               ; preds = %88, %87
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZ20taichi_assert_formatENKUlvE_clEv(%class.anon* nonnull align 8 dereferenceable(32) %0) #2 align 2 {
  %2 = alloca %class.anon*, align 8, addrspace(5)
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i64, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast %class.anon* addrspace(5)* %2 to %class.anon**
  %7 = addrspacecast i64 addrspace(5)* %3 to i64*
  %8 = addrspacecast i64 addrspace(5)* %4 to i64*
  %9 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %class.anon* %0, %class.anon** %6, align 8
  %10 = load %class.anon*, %class.anon** %6, align 8
  %11 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 0
  %12 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %11, align 8
  %13 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %14 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %13, i32 0, i32 25
  %15 = load i64, i64* %14, align 8
  %16 = icmp ne i64 %15, 0
  br i1 %16, label %68, label %17

17:                                               ; preds = %1
  %18 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 0
  %19 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %18, align 8
  %20 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %19, align 8
  %21 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %20, i32 0, i32 25
  store i64 1, i64* %21, align 8
  %22 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 0
  %23 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %22, align 8
  %24 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %23, align 8
  %25 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %24, i32 0, i32 22
  %26 = getelementptr inbounds [2048 x i8], [2048 x i8]* %25, i64 0, i64 0
  %27 = call i8* @_ZL6memsetPvim(i8* %26, i32 0, i64 2048) #24
  %28 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 0
  %29 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %28, align 8
  %30 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %29, align 8
  %31 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %30, i32 0, i32 22
  %32 = getelementptr inbounds [2048 x i8], [2048 x i8]* %31, i64 0, i64 0
  %33 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 1
  %34 = load i8**, i8*** %33, align 8
  %35 = load i8*, i8** %34, align 8
  %36 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 1
  %37 = load i8**, i8*** %36, align 8
  %38 = load i8*, i8** %37, align 8
  %39 = call i64 @taichi_strlen(i8* %38) #24
  store i64 %39, i64* %7, align 8
  store i64 2047, i64* %8, align 8
  %40 = call nonnull align 8 dereferenceable(8) i64* @_ZSt3minImEUa9enable_ifILb1EERKT_S2_S2_(i64* nonnull align 8 dereferenceable(8) %7, i64* nonnull align 8 dereferenceable(8) %8) #24
  %41 = load i64, i64* %40, align 8
  %42 = call i8* @_ZL6memcpyPvPKvm(i8* %32, i8* %35, i64 %41) #24
  store i32 0, i32* %9, align 4
  br label %43

43:                                               ; preds = %64, %17
  %44 = load i32, i32* %9, align 4
  %45 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 2
  %46 = load i32*, i32** %45, align 8
  %47 = load i32, i32* %46, align 4
  %48 = icmp slt i32 %44, %47
  br i1 %48, label %49, label %67

49:                                               ; preds = %43
  %50 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 3
  %51 = load i64**, i64*** %50, align 8
  %52 = load i64*, i64** %51, align 8
  %53 = load i32, i32* %9, align 4
  %54 = sext i32 %53 to i64
  %55 = getelementptr inbounds i64, i64* %52, i64 %54
  %56 = load i64, i64* %55, align 8
  %57 = getelementptr inbounds %class.anon, %class.anon* %10, i32 0, i32 0
  %58 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %57, align 8
  %59 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %58, align 8
  %60 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %59, i32 0, i32 23
  %61 = load i32, i32* %9, align 4
  %62 = sext i32 %61 to i64
  %63 = getelementptr inbounds [32 x i64], [32 x i64]* %60, i64 0, i64 %62
  store i64 %56, i64* %63, align 8
  br label %64

64:                                               ; preds = %49
  %65 = load i32, i32* %9, align 4
  %66 = add nsw i32 %65, 1
  store i32 %66, i32* %9, align 4
  br label %43, !llvm.loop !64

67:                                               ; preds = %43
  br label %68

68:                                               ; preds = %67, %1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZN10lock_guardIZ20taichi_assert_formatEUlvE_Z11locked_taskIS0_EvPvRKT_EUlvE_EC1EPhRKS0_RKS6_ENKUlvE_clEv(%class.anon.11* nonnull align 8 dereferenceable(24) %0) #2 align 2 {
  %2 = alloca %class.anon.11*, align 8, addrspace(5)
  %3 = addrspacecast %class.anon.11* addrspace(5)* %2 to %class.anon.11**
  store %class.anon.11* %0, %class.anon.11** %3, align 8
  %4 = load %class.anon.11*, %class.anon.11** %3, align 8
  %5 = getelementptr inbounds %class.anon.11, %class.anon.11* %4, i32 0, i32 0
  %6 = load %class.anon.10*, %class.anon.10** %5, align 8
  %7 = call zeroext i1 @_ZZ11locked_taskIZ20taichi_assert_formatEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.10* nonnull align 1 dereferenceable(1) %6) #24
  br i1 %7, label %8, label %22

8:                                                ; preds = %1
  %9 = getelementptr inbounds %class.anon.11, %class.anon.11* %4, i32 0, i32 1
  %10 = load i8**, i8*** %9, align 8
  %11 = load i8*, i8** %10, align 8
  call void @mutex_lock_i32(i8* %11) #24
  call void @grid_memfence() #24
  %12 = getelementptr inbounds %class.anon.11, %class.anon.11* %4, i32 0, i32 0
  %13 = load %class.anon.10*, %class.anon.10** %12, align 8
  %14 = call zeroext i1 @_ZZ11locked_taskIZ20taichi_assert_formatEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.10* nonnull align 1 dereferenceable(1) %13) #24
  br i1 %14, label %15, label %18

15:                                               ; preds = %8
  %16 = getelementptr inbounds %class.anon.11, %class.anon.11* %4, i32 0, i32 2
  %17 = load %class.anon*, %class.anon** %16, align 8
  call void @_ZZ20taichi_assert_formatENKUlvE_clEv(%class.anon* nonnull align 8 dereferenceable(32) %17) #24
  br label %18

18:                                               ; preds = %15, %8
  call void @grid_memfence() #24
  %19 = getelementptr inbounds %class.anon.11, %class.anon.11* %4, i32 0, i32 1
  %20 = load i8**, i8*** %19, align 8
  %21 = load i8*, i8** %20, align 8
  call void @mutex_unlock_i32(i8* %21) #24
  br label %22

22:                                               ; preds = %18, %1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal i8* @_ZL6memsetPvim(i8* %0, i32 %1, i64 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8, align 1, addrspace(5)
  %9 = addrspacecast i8* addrspace(5)* %4 to i8**
  %10 = addrspacecast i8* addrspace(5)* %5 to i8**
  %11 = addrspacecast i32 addrspace(5)* %6 to i32*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  %13 = addrspacecast i8 addrspace(5)* %8 to i8*
  store i8* %0, i8** %10, align 8
  store i32 %1, i32* %11, align 4
  store i64 %2, i64* %12, align 8
  %14 = load i32, i32* %11, align 4
  %15 = trunc i32 %14 to i8
  store i8 %15, i8* %13, align 1
  %16 = load i8*, i8** %10, align 8
  %17 = load i8, i8* %13, align 1
  %18 = load i64, i64* %12, align 8
  %19 = call i8* @_ZL15__hip_hc_memsetPvhm(i8* %16, i8 zeroext %17, i64 %18) #24
  ret i8* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal i8* @_ZL6memcpyPvPKvm(i8* %0, i8* %1, i64 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast i8* addrspace(5)* %5 to i8**
  %10 = addrspacecast i8* addrspace(5)* %6 to i8**
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  store i8* %0, i8** %9, align 8
  store i8* %1, i8** %10, align 8
  store i64 %2, i64* %11, align 8
  %12 = load i8*, i8** %9, align 8
  %13 = load i8*, i8** %10, align 8
  %14 = load i64, i64* %11, align 8
  %15 = call i8* @_ZL15__hip_hc_memcpyPvPKvm(i8* %12, i8* %13, i64 %14) #24
  ret i8* %15
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 8 dereferenceable(8) i64* @_ZSt3minImEUa9enable_ifILb1EERKT_S2_S2_(i64* nonnull align 8 dereferenceable(8) %0, i64* nonnull align 8 dereferenceable(8) %1) #2 comdat {
  %3 = alloca i64*, align 8, addrspace(5)
  %4 = alloca i64*, align 8, addrspace(5)
  %5 = alloca i64*, align 8, addrspace(5)
  %6 = addrspacecast i64* addrspace(5)* %3 to i64**
  %7 = addrspacecast i64* addrspace(5)* %4 to i64**
  %8 = addrspacecast i64* addrspace(5)* %5 to i64**
  store i64* %0, i64** %7, align 8
  store i64* %1, i64** %8, align 8
  %9 = load i64*, i64** %7, align 8
  %10 = load i64, i64* %9, align 8
  %11 = load i64*, i64** %8, align 8
  %12 = load i64, i64* %11, align 8
  %13 = icmp ult i64 %10, %12
  br i1 %13, label %14, label %16

14:                                               ; preds = %2
  %15 = load i64*, i64** %7, align 8
  br label %18

16:                                               ; preds = %2
  %17 = load i64*, i64** %8, align 8
  br label %18

18:                                               ; preds = %16, %14
  %19 = phi i64* [ %15, %14 ], [ %17, %16 ]
  ret i64* %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal i8* @_ZL15__hip_hc_memsetPvhm(i8* %0, i8 zeroext %1, i64 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8, align 1, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8*, align 8, addrspace(5)
  %9 = addrspacecast i8* addrspace(5)* %4 to i8**
  %10 = addrspacecast i8* addrspace(5)* %5 to i8**
  %11 = addrspacecast i8 addrspace(5)* %6 to i8*
  %12 = addrspacecast i64 addrspace(5)* %7 to i64*
  %13 = addrspacecast i8* addrspace(5)* %8 to i8**
  store i8* %0, i8** %10, align 8
  store i8 %1, i8* %11, align 1
  store i64 %2, i64* %12, align 8
  %14 = load i8*, i8** %10, align 8
  store i8* %14, i8** %13, align 8
  br label %15

15:                                               ; preds = %18, %3
  %16 = load i64, i64* %12, align 8
  %17 = icmp uge i64 %16, 4
  br i1 %17, label %18, label %35

18:                                               ; preds = %15
  %19 = load i8, i8* %11, align 1
  %20 = load i8*, i8** %13, align 8
  %21 = getelementptr inbounds i8, i8* %20, i64 0
  store i8 %19, i8* %21, align 1
  %22 = load i8, i8* %11, align 1
  %23 = load i8*, i8** %13, align 8
  %24 = getelementptr inbounds i8, i8* %23, i64 1
  store i8 %22, i8* %24, align 1
  %25 = load i8, i8* %11, align 1
  %26 = load i8*, i8** %13, align 8
  %27 = getelementptr inbounds i8, i8* %26, i64 2
  store i8 %25, i8* %27, align 1
  %28 = load i8, i8* %11, align 1
  %29 = load i8*, i8** %13, align 8
  %30 = getelementptr inbounds i8, i8* %29, i64 3
  store i8 %28, i8* %30, align 1
  %31 = load i64, i64* %12, align 8
  %32 = sub i64 %31, 4
  store i64 %32, i64* %12, align 8
  %33 = load i8*, i8** %13, align 8
  %34 = getelementptr inbounds i8, i8* %33, i64 4
  store i8* %34, i8** %13, align 8
  br label %15, !llvm.loop !65

35:                                               ; preds = %15
  %36 = load i64, i64* %12, align 8
  switch i64 %36, label %49 [
    i64 3, label %37
    i64 2, label %41
    i64 1, label %45
  ]

37:                                               ; preds = %35
  %38 = load i8, i8* %11, align 1
  %39 = load i8*, i8** %13, align 8
  %40 = getelementptr inbounds i8, i8* %39, i64 2
  store i8 %38, i8* %40, align 1
  br label %41

41:                                               ; preds = %35, %37
  %42 = load i8, i8* %11, align 1
  %43 = load i8*, i8** %13, align 8
  %44 = getelementptr inbounds i8, i8* %43, i64 1
  store i8 %42, i8* %44, align 1
  br label %45

45:                                               ; preds = %35, %41
  %46 = load i8, i8* %11, align 1
  %47 = load i8*, i8** %13, align 8
  %48 = getelementptr inbounds i8, i8* %47, i64 0
  store i8 %46, i8* %48, align 1
  br label %49

49:                                               ; preds = %45, %35
  %50 = load i8*, i8** %10, align 8
  ret i8* %50
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal i8* @_ZL15__hip_hc_memcpyPvPKvm(i8* %0, i8* %1, i64 %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca i8*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i8*, align 8, addrspace(5)
  %9 = alloca i8*, align 8, addrspace(5)
  %10 = addrspacecast i8* addrspace(5)* %4 to i8**
  %11 = addrspacecast i8* addrspace(5)* %5 to i8**
  %12 = addrspacecast i8* addrspace(5)* %6 to i8**
  %13 = addrspacecast i64 addrspace(5)* %7 to i64*
  %14 = addrspacecast i8* addrspace(5)* %8 to i8**
  %15 = addrspacecast i8* addrspace(5)* %9 to i8**
  store i8* %0, i8** %11, align 8
  store i8* %1, i8** %12, align 8
  store i64 %2, i64* %13, align 8
  %16 = load i8*, i8** %11, align 8
  store i8* %16, i8** %14, align 8
  %17 = load i8*, i8** %12, align 8
  store i8* %17, i8** %15, align 8
  br label %18

18:                                               ; preds = %21, %3
  %19 = load i64, i64* %13, align 8
  %20 = icmp uge i64 %19, 4
  br i1 %20, label %21, label %48

21:                                               ; preds = %18
  %22 = load i8*, i8** %15, align 8
  %23 = getelementptr inbounds i8, i8* %22, i64 0
  %24 = load i8, i8* %23, align 1
  %25 = load i8*, i8** %14, align 8
  %26 = getelementptr inbounds i8, i8* %25, i64 0
  store i8 %24, i8* %26, align 1
  %27 = load i8*, i8** %15, align 8
  %28 = getelementptr inbounds i8, i8* %27, i64 1
  %29 = load i8, i8* %28, align 1
  %30 = load i8*, i8** %14, align 8
  %31 = getelementptr inbounds i8, i8* %30, i64 1
  store i8 %29, i8* %31, align 1
  %32 = load i8*, i8** %15, align 8
  %33 = getelementptr inbounds i8, i8* %32, i64 2
  %34 = load i8, i8* %33, align 1
  %35 = load i8*, i8** %14, align 8
  %36 = getelementptr inbounds i8, i8* %35, i64 2
  store i8 %34, i8* %36, align 1
  %37 = load i8*, i8** %15, align 8
  %38 = getelementptr inbounds i8, i8* %37, i64 3
  %39 = load i8, i8* %38, align 1
  %40 = load i8*, i8** %14, align 8
  %41 = getelementptr inbounds i8, i8* %40, i64 3
  store i8 %39, i8* %41, align 1
  %42 = load i64, i64* %13, align 8
  %43 = sub i64 %42, 4
  store i64 %43, i64* %13, align 8
  %44 = load i8*, i8** %15, align 8
  %45 = getelementptr inbounds i8, i8* %44, i64 4
  store i8* %45, i8** %15, align 8
  %46 = load i8*, i8** %14, align 8
  %47 = getelementptr inbounds i8, i8* %46, i64 4
  store i8* %47, i8** %14, align 8
  br label %18, !llvm.loop !66

48:                                               ; preds = %18
  %49 = load i64, i64* %13, align 8
  switch i64 %49, label %68 [
    i64 3, label %50
    i64 2, label %56
    i64 1, label %62
  ]

50:                                               ; preds = %48
  %51 = load i8*, i8** %15, align 8
  %52 = getelementptr inbounds i8, i8* %51, i64 2
  %53 = load i8, i8* %52, align 1
  %54 = load i8*, i8** %14, align 8
  %55 = getelementptr inbounds i8, i8* %54, i64 2
  store i8 %53, i8* %55, align 1
  br label %56

56:                                               ; preds = %48, %50
  %57 = load i8*, i8** %15, align 8
  %58 = getelementptr inbounds i8, i8* %57, i64 1
  %59 = load i8, i8* %58, align 1
  %60 = load i8*, i8** %14, align 8
  %61 = getelementptr inbounds i8, i8* %60, i64 1
  store i8 %59, i8* %61, align 1
  br label %62

62:                                               ; preds = %48, %56
  %63 = load i8*, i8** %15, align 8
  %64 = getelementptr inbounds i8, i8* %63, i64 0
  %65 = load i8, i8* %64, align 1
  %66 = load i8*, i8** %14, align 8
  %67 = getelementptr inbounds i8, i8* %66, i64 0
  store i8 %65, i8* %67, align 1
  br label %68

68:                                               ; preds = %62, %48
  %69 = load i8*, i8** %11, align 8
  ret i8* %69
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal zeroext i1 @_ZZ11locked_taskIZ20taichi_assert_formatEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.10* nonnull align 1 dereferenceable(1) %0) #2 align 2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca %class.anon.10*, align 8, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast %class.anon.10* addrspace(5)* %3 to %class.anon.10**
  store %class.anon.10* %0, %class.anon.10** %5, align 8
  %6 = load %class.anon.10*, %class.anon.10** %5, align 8
  ret i1 true
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EvS3_S6_RKT0_(i8* %0, %class.anon.0* nonnull align 8 dereferenceable(40) %1, %class.anon.12* nonnull align 1 dereferenceable(1) %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %class.anon.0*, align 8, addrspace(5)
  %6 = alloca %class.anon.12*, align 8, addrspace(5)
  %7 = alloca %class.lock_guard.14, align 1, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast %class.anon.0* addrspace(5)* %5 to %class.anon.0**
  %10 = addrspacecast %class.anon.12* addrspace(5)* %6 to %class.anon.12**
  %11 = addrspacecast %class.lock_guard.14 addrspace(5)* %7 to %class.lock_guard.14*
  store i8* %0, i8** %8, align 8
  store %class.anon.0* %1, %class.anon.0** %9, align 8
  store %class.anon.12* %2, %class.anon.12** %10, align 8
  %12 = load i8*, i8** %8, align 8
  %13 = load %class.anon.0*, %class.anon.0** %9, align 8
  %14 = load %class.anon.12*, %class.anon.12** %10, align 8
  call void @_ZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC2EPhRKS1_RKS7_(%class.lock_guard.14* nonnull align 1 dereferenceable(1) %11, i8* %12, %class.anon.0* nonnull align 8 dereferenceable(40) %13, %class.anon.12* nonnull align 1 dereferenceable(1) %14) #24
  ret void
}

; Function Attrs: convergent noinline nounwind optnone
define internal void @_ZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC2EPhRKS1_RKS7_(%class.lock_guard.14* nonnull align 1 dereferenceable(1) %0, i8* %1, %class.anon.0* nonnull align 8 dereferenceable(40) %2, %class.anon.12* nonnull align 1 dereferenceable(1) %3) unnamed_addr #5 align 2 {
  %5 = alloca %class.lock_guard.14*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca %class.anon.0*, align 8, addrspace(5)
  %8 = alloca %class.anon.12*, align 8, addrspace(5)
  %9 = alloca %class.anon.16, align 8, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = alloca i8, align 1, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca i32, align 4, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = addrspacecast %class.lock_guard.14* addrspace(5)* %5 to %class.lock_guard.14**
  %17 = addrspacecast i8* addrspace(5)* %6 to i8**
  %18 = addrspacecast %class.anon.0* addrspace(5)* %7 to %class.anon.0**
  %19 = addrspacecast %class.anon.12* addrspace(5)* %8 to %class.anon.12**
  %20 = addrspacecast %class.anon.16 addrspace(5)* %9 to %class.anon.16*
  %21 = addrspacecast i8 addrspace(5)* %10 to i8*
  %22 = addrspacecast i8 addrspace(5)* %11 to i8*
  %23 = addrspacecast i32 addrspace(5)* %12 to i32*
  %24 = addrspacecast i32 addrspace(5)* %13 to i32*
  %25 = addrspacecast i32 addrspace(5)* %14 to i32*
  %26 = addrspacecast i32 addrspace(5)* %15 to i32*
  store %class.lock_guard.14* %0, %class.lock_guard.14** %16, align 8
  store i8* %1, i8** %17, align 8
  store %class.anon.0* %2, %class.anon.0** %18, align 8
  store %class.anon.12* %3, %class.anon.12** %19, align 8
  %27 = load %class.lock_guard.14*, %class.lock_guard.14** %16, align 8
  %28 = getelementptr inbounds %class.anon.16, %class.anon.16* %20, i32 0, i32 0
  %29 = load %class.anon.12*, %class.anon.12** %19, align 8
  store %class.anon.12* %29, %class.anon.12** %28, align 8
  %30 = getelementptr inbounds %class.anon.16, %class.anon.16* %20, i32 0, i32 1
  store i8** %17, i8*** %30, align 8
  %31 = getelementptr inbounds %class.anon.16, %class.anon.16* %20, i32 0, i32 2
  %32 = load %class.anon.0*, %class.anon.0** %18, align 8
  store %class.anon.0* %32, %class.anon.0** %31, align 8
  %33 = call i32 @cuda_compute_capability() #24
  %34 = icmp slt i32 %33, 70
  br i1 %34, label %35, label %88

35:                                               ; preds = %4
  store i8 0, i8* %21, align 1
  br label %36

36:                                               ; preds = %48, %35
  %37 = load i8, i8* %21, align 1
  %38 = trunc i8 %37 to i1
  %39 = xor i1 %38, true
  br i1 %39, label %40, label %49

40:                                               ; preds = %36
  %41 = load i8*, i8** %17, align 8
  %42 = bitcast i8* %41 to i32*
  %43 = call i32 @atomic_exchange_i32(i32* %42, i32 1) #24
  %44 = icmp eq i32 %43, 1
  br i1 %44, label %45, label %48

45:                                               ; preds = %40
  %46 = load %class.anon.0*, %class.anon.0** %18, align 8
  call void @_ZZN11LLVMRuntime20allocate_from_bufferEmmENKUlvE_clEv(%class.anon.0* nonnull align 8 dereferenceable(40) %46) #24
  store i8 1, i8* %21, align 1
  %47 = load i8*, i8** %17, align 8
  call void @mutex_unlock_i32(i8* %47) #24
  br label %48

48:                                               ; preds = %45, %40
  br label %36, !llvm.loop !67

49:                                               ; preds = %36
  store i8 0, i8* %22, align 1
  %50 = load i8, i8* %22, align 1
  %51 = trunc i8 %50 to i1
  br i1 %51, label %52, label %72

52:                                               ; preds = %49
  %53 = call i32 @cuda_active_mask() #24
  store i32 %53, i32* %23, align 4
  %54 = load i32, i32* %23, align 4
  store i32 %54, i32* %24, align 4
  br label %55

55:                                               ; preds = %65, %52
  %56 = load i32, i32* %24, align 4
  %57 = icmp ne i32 %56, 0
  br i1 %57, label %58, label %71

58:                                               ; preds = %55
  %59 = load i32, i32* %24, align 4
  %60 = call i32 @cttz_i32(i32 %59) #24
  store i32 %60, i32* %25, align 4
  %61 = call i32 @warp_idx() #24
  %62 = load i32, i32* %25, align 4
  %63 = icmp eq i32 %61, %62
  br i1 %63, label %64, label %65

64:                                               ; preds = %58
  call void @_ZZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.16* nonnull align 8 dereferenceable(24) %20) #24
  br label %65

65:                                               ; preds = %64, %58
  %66 = load i32, i32* %25, align 4
  %67 = shl i32 1, %66
  %68 = xor i32 %67, -1
  %69 = load i32, i32* %24, align 4
  %70 = and i32 %69, %68
  store i32 %70, i32* %24, align 4
  br label %55, !llvm.loop !68

71:                                               ; preds = %55
  br label %87

72:                                               ; preds = %49
  store i32 0, i32* %26, align 4
  br label %73

73:                                               ; preds = %83, %72
  %74 = load i32, i32* %26, align 4
  %75 = call i32 @warp_size() #24
  %76 = icmp slt i32 %74, %75
  br i1 %76, label %77, label %86

77:                                               ; preds = %73
  %78 = call i32 @warp_idx() #24
  %79 = load i32, i32* %26, align 4
  %80 = icmp eq i32 %78, %79
  br i1 %80, label %81, label %82

81:                                               ; preds = %77
  call void @_ZZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.16* nonnull align 8 dereferenceable(24) %20) #24
  br label %82

82:                                               ; preds = %81, %77
  br label %83

83:                                               ; preds = %82
  %84 = load i32, i32* %26, align 4
  %85 = add nsw i32 %84, 1
  store i32 %85, i32* %26, align 4
  br label %73, !llvm.loop !69

86:                                               ; preds = %73
  br label %87

87:                                               ; preds = %86, %71
  br label %89

88:                                               ; preds = %4
  call void @_ZZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.16* nonnull align 8 dereferenceable(24) %20) #24
  br label %89

89:                                               ; preds = %88, %87
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZN11LLVMRuntime20allocate_from_bufferEmmENKUlvE_clEv(%class.anon.0* nonnull align 8 dereferenceable(40) %0) #2 align 2 {
  %2 = alloca %class.anon.0*, align 8, addrspace(5)
  %3 = alloca i64, align 8, addrspace(5)
  %4 = addrspacecast %class.anon.0* addrspace(5)* %2 to %class.anon.0**
  %5 = addrspacecast i64 addrspace(5)* %3 to i64*
  store %class.anon.0* %0, %class.anon.0** %4, align 8
  %6 = load %class.anon.0*, %class.anon.0** %4, align 8
  %7 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 1
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %7, align 8
  %9 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 0
  %10 = load i64*, i64** %9, align 8
  %11 = load i64, i64* %10, align 8
  %12 = sub i64 %11, 1
  %13 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 2
  %14 = load i8*, i8** %13, align 8
  %15 = ptrtoint i8* %14 to i64
  %16 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 0
  %17 = load i64*, i64** %16, align 8
  %18 = load i64, i64* %17, align 8
  %19 = add i64 %15, %18
  %20 = sub i64 %19, 1
  %21 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 0
  %22 = load i64*, i64** %21, align 8
  %23 = load i64, i64* %22, align 8
  %24 = urem i64 %20, %23
  %25 = sub i64 %12, %24
  store i64 %25, i64* %5, align 8
  %26 = load i64, i64* %5, align 8
  %27 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 2
  %28 = load i64*, i64** %27, align 8
  %29 = load i64, i64* %28, align 8
  %30 = add i64 %29, %26
  store i64 %30, i64* %28, align 8
  %31 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 2
  %32 = load i8*, i8** %31, align 8
  %33 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 2
  %34 = load i64*, i64** %33, align 8
  %35 = load i64, i64* %34, align 8
  %36 = getelementptr inbounds i8, i8* %32, i64 %35
  %37 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 3
  %38 = load i8*, i8** %37, align 8
  %39 = icmp ule i8* %36, %38
  br i1 %39, label %40, label %55

40:                                               ; preds = %1
  %41 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 2
  %42 = load i8*, i8** %41, align 8
  %43 = load i64, i64* %5, align 8
  %44 = getelementptr inbounds i8, i8* %42, i64 %43
  %45 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 3
  %46 = load i8**, i8*** %45, align 8
  store i8* %44, i8** %46, align 8
  %47 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 2
  %48 = load i64*, i64** %47, align 8
  %49 = load i64, i64* %48, align 8
  %50 = getelementptr inbounds %struct.LLVMRuntime, %struct.LLVMRuntime* %8, i32 0, i32 2
  %51 = load i8*, i8** %50, align 8
  %52 = getelementptr inbounds i8, i8* %51, i64 %49
  store i8* %52, i8** %50, align 8
  %53 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 4
  %54 = load i8*, i8** %53, align 8
  store i8 1, i8* %54, align 1
  br label %58

55:                                               ; preds = %1
  %56 = getelementptr inbounds %class.anon.0, %class.anon.0* %6, i32 0, i32 4
  %57 = load i8*, i8** %56, align 8
  store i8 0, i8* %57, align 1
  br label %58

58:                                               ; preds = %55, %40
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZN10lock_guardIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.16* nonnull align 8 dereferenceable(24) %0) #2 align 2 {
  %2 = alloca %class.anon.16*, align 8, addrspace(5)
  %3 = addrspacecast %class.anon.16* addrspace(5)* %2 to %class.anon.16**
  store %class.anon.16* %0, %class.anon.16** %3, align 8
  %4 = load %class.anon.16*, %class.anon.16** %3, align 8
  %5 = getelementptr inbounds %class.anon.16, %class.anon.16* %4, i32 0, i32 0
  %6 = load %class.anon.12*, %class.anon.12** %5, align 8
  %7 = call zeroext i1 @_ZZ11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.12* nonnull align 1 dereferenceable(1) %6) #24
  br i1 %7, label %8, label %22

8:                                                ; preds = %1
  %9 = getelementptr inbounds %class.anon.16, %class.anon.16* %4, i32 0, i32 1
  %10 = load i8**, i8*** %9, align 8
  %11 = load i8*, i8** %10, align 8
  call void @mutex_lock_i32(i8* %11) #24
  call void @grid_memfence() #24
  %12 = getelementptr inbounds %class.anon.16, %class.anon.16* %4, i32 0, i32 0
  %13 = load %class.anon.12*, %class.anon.12** %12, align 8
  %14 = call zeroext i1 @_ZZ11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.12* nonnull align 1 dereferenceable(1) %13) #24
  br i1 %14, label %15, label %18

15:                                               ; preds = %8
  %16 = getelementptr inbounds %class.anon.16, %class.anon.16* %4, i32 0, i32 2
  %17 = load %class.anon.0*, %class.anon.0** %16, align 8
  call void @_ZZN11LLVMRuntime20allocate_from_bufferEmmENKUlvE_clEv(%class.anon.0* nonnull align 8 dereferenceable(40) %17) #24
  br label %18

18:                                               ; preds = %15, %8
  call void @grid_memfence() #24
  %19 = getelementptr inbounds %class.anon.16, %class.anon.16* %4, i32 0, i32 1
  %20 = load i8**, i8*** %19, align 8
  %21 = load i8*, i8** %20, align 8
  call void @mutex_unlock_i32(i8* %21) #24
  br label %22

22:                                               ; preds = %18, %1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal zeroext i1 @_ZZ11locked_taskIZN11LLVMRuntime20allocate_from_bufferEmmEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.12* nonnull align 1 dereferenceable(1) %0) #2 align 2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca %class.anon.12*, align 8, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast %class.anon.12* addrspace(5)* %3 to %class.anon.12**
  store %class.anon.12* %0, %class.anon.12** %5, align 8
  %6 = load %class.anon.12*, %class.anon.12** %5, align 8
  ret i1 true
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImPhET_T0_(i8* %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = alloca %union.anon.17, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast i8* addrspace(5)* %3 to i8**
  %7 = addrspacecast %union.anon.17 addrspace(5)* %4 to %union.anon.17*
  store i8* %0, i8** %6, align 8
  %8 = load i8*, i8** %6, align 8
  %9 = bitcast %union.anon.17* %7 to i8**
  store i8* %8, i8** %9, align 8
  %10 = bitcast %union.anon.17* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImP15MemRequestQueueET_T0_(%struct.MemRequestQueue* %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.MemRequestQueue*, align 8, addrspace(5)
  %4 = alloca %union.anon.18, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast %struct.MemRequestQueue* addrspace(5)* %3 to %struct.MemRequestQueue**
  %7 = addrspacecast %union.anon.18 addrspace(5)* %4 to %union.anon.18*
  store %struct.MemRequestQueue* %0, %struct.MemRequestQueue** %6, align 8
  %8 = load %struct.MemRequestQueue*, %struct.MemRequestQueue** %6, align 8
  %9 = bitcast %union.anon.18* %7 to %struct.MemRequestQueue**
  store %struct.MemRequestQueue* %8, %struct.MemRequestQueue** %9, align 8
  %10 = bitcast %union.anon.18* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i64 @_Z38taichi_union_cast_with_different_sizesImP11LLVMRuntimeET_T0_(%struct.LLVMRuntime* %0) #2 comdat {
  %2 = alloca i64, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %4 = alloca %union.anon.19, align 8, addrspace(5)
  %5 = addrspacecast i64 addrspace(5)* %2 to i64*
  %6 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %3 to %struct.LLVMRuntime**
  %7 = addrspacecast %union.anon.19 addrspace(5)* %4 to %union.anon.19*
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %6, align 8
  %8 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %6, align 8
  %9 = bitcast %union.anon.19* %7 to %struct.LLVMRuntime**
  store %struct.LLVMRuntime* %8, %struct.LLVMRuntime** %9, align 8
  %10 = bitcast %union.anon.19* %7 to i64*
  %11 = load i64, i64* %10, align 8
  ret i64 %11
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden nonnull align 8 dereferenceable(8) %struct.LLVMRuntime** @_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE(%struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %0) #2 comdat {
  %2 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %3 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %4 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %2 to %struct.LLVMRuntime***
  %5 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %3 to %struct.LLVMRuntime***
  store %struct.LLVMRuntime** %0, %struct.LLVMRuntime*** %5, align 8
  %6 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %5, align 8
  ret %struct.LLVMRuntime** %6
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr nonnull align 8 dereferenceable(8) i64* @_ZSt7forwardImEOT_RNSt16remove_referenceIS0_E4typeE(i64* nonnull align 8 dereferenceable(8) %0) #2 comdat {
  %2 = alloca i64*, align 8, addrspace(5)
  %3 = alloca i64*, align 8, addrspace(5)
  %4 = addrspacecast i64* addrspace(5)* %2 to i64**
  %5 = addrspacecast i64* addrspace(5)* %3 to i64**
  store i64* %0, i64** %5, align 8
  %6 = load i64*, i64** %5, align 8
  ret i64* %6
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIiEOT_RNSt16remove_referenceIS0_E4typeE(i32* nonnull align 4 dereferenceable(4) %0) #2 comdat {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca i32*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast i32* addrspace(5)* %3 to i32**
  store i32* %0, i32** %5, align 8
  %6 = load i32*, i32** %5, align 8
  ret i32* %6
}

; Function Attrs: convergent noinline nounwind optnone
define linkonce_odr hidden void @_ZN11ListManagerC2EP11LLVMRuntimemm(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %0, %struct.LLVMRuntime* %1, i64 %2, i64 %3) unnamed_addr #5 comdat align 2 {
  %5 = alloca %struct.ListManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca i64, align 8, addrspace(5)
  %8 = alloca i64, align 8, addrspace(5)
  %9 = addrspacecast %struct.ListManager* addrspace(5)* %5 to %struct.ListManager**
  %10 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %11 = addrspacecast i64 addrspace(5)* %7 to i64*
  %12 = addrspacecast i64 addrspace(5)* %8 to i64*
  store %struct.ListManager* %0, %struct.ListManager** %9, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %10, align 8
  store i64 %2, i64* %11, align 8
  store i64 %3, i64* %12, align 8
  %13 = load %struct.ListManager*, %struct.ListManager** %9, align 8
  %14 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 1
  %15 = load i64, i64* %11, align 8
  store i64 %15, i64* %14, align 8
  %16 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 2
  %17 = load i64, i64* %12, align 8
  store i64 %17, i64* %16, align 8
  %18 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 6
  %19 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %10, align 8
  store %struct.LLVMRuntime* %19, %struct.LLVMRuntime** %18, align 8
  %20 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %10, align 8
  %21 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 2
  %22 = load i64, i64* %21, align 8
  %23 = trunc i64 %22 to i32
  %24 = call zeroext i1 @_Z15is_power_of_twoj(i32 %23) #24
  %25 = zext i1 %24 to i32
  call void @taichi_assert_runtime(%struct.LLVMRuntime* %20, i32 %25, i8* getelementptr inbounds ([40 x i8], [40 x i8]* addrspacecast ([40 x i8] addrspace(4)* @.str.6 to [40 x i8]*), i64 0, i64 0)) #24
  %26 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 4
  store i32 0, i32* %26, align 4
  %27 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 5
  store i32 0, i32* %27, align 8
  %28 = load i64, i64* %12, align 8
  %29 = call i32 @_ZN6taichi7log2intImEEjT_(i64 %28) #24
  %30 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %13, i32 0, i32 3
  store i32 %29, i32* %30, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZN6taichi7log2intImEEjT_(i64 %0) #2 comdat {
  %2 = alloca i32, align 4, addrspace(5)
  %3 = alloca i64, align 8, addrspace(5)
  %4 = alloca i32, align 4, addrspace(5)
  %5 = addrspacecast i32 addrspace(5)* %2 to i32*
  %6 = addrspacecast i64 addrspace(5)* %3 to i64*
  %7 = addrspacecast i32 addrspace(5)* %4 to i32*
  store i64 %0, i64* %6, align 8
  store i32 0, i32* %7, align 4
  %8 = load i64, i64* %6, align 8
  %9 = lshr i64 %8, 1
  store i64 %9, i64* %6, align 8
  br label %10

10:                                               ; preds = %13, %1
  %11 = load i64, i64* %6, align 8
  %12 = icmp ne i64 %11, 0
  br i1 %12, label %13, label %18

13:                                               ; preds = %10
  %14 = load i64, i64* %6, align 8
  %15 = lshr i64 %14, 1
  store i64 %15, i64* %6, align 8
  %16 = load i32, i32* %7, align 4
  %17 = add i32 %16, 1
  store i32 %17, i32* %7, align 4
  br label %10, !llvm.loop !70

18:                                               ; preds = %10
  %19 = load i32, i32* %7, align 4
  ret i32 %19
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr nonnull align 8 dereferenceable(8) i64* @_ZSt7forwardIRmEOT_RNSt16remove_referenceIS1_E4typeE(i64* nonnull align 8 dereferenceable(8) %0) #2 comdat {
  %2 = alloca i64*, align 8, addrspace(5)
  %3 = alloca i64*, align 8, addrspace(5)
  %4 = addrspacecast i64* addrspace(5)* %2 to i64**
  %5 = addrspacecast i64* addrspace(5)* %3 to i64**
  store i64* %0, i64** %5, align 8
  %6 = load i64*, i64** %5, align 8
  ret i64* %6
}

; Function Attrs: convergent noinline nounwind optnone
define linkonce_odr hidden void @_ZN11NodeManagerC2EP11LLVMRuntimeii(%struct.NodeManager* nonnull align 8 dereferenceable(52) %0, %struct.LLVMRuntime* %1, i32 %2, i32 %3) unnamed_addr #5 comdat align 2 {
  %5 = alloca %struct.NodeManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i64, align 8, addrspace(5)
  %10 = alloca i64, align 8, addrspace(5)
  %11 = addrspacecast %struct.NodeManager* addrspace(5)* %5 to %struct.NodeManager**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast i32 addrspace(5)* %7 to i32*
  %14 = addrspacecast i32 addrspace(5)* %8 to i32*
  %15 = addrspacecast i64 addrspace(5)* %9 to i64*
  %16 = addrspacecast i64 addrspace(5)* %10 to i64*
  store %struct.NodeManager* %0, %struct.NodeManager** %11, align 8
  store %struct.LLVMRuntime* %1, %struct.LLVMRuntime** %12, align 8
  store i32 %2, i32* %13, align 4
  store i32 %3, i32* %14, align 4
  %17 = load %struct.NodeManager*, %struct.NodeManager** %11, align 8
  %18 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 0
  %19 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  store %struct.LLVMRuntime* %19, %struct.LLVMRuntime** %18, align 8
  %20 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 2
  %21 = load i32, i32* %13, align 4
  store i32 %21, i32* %20, align 4
  %22 = load i32, i32* %14, align 4
  %23 = icmp eq i32 %22, -1
  br i1 %23, label %24, label %25

24:                                               ; preds = %4
  store i32 131072, i32* %14, align 4
  br label %25

25:                                               ; preds = %24, %4
  br label %26

26:                                               ; preds = %38, %25
  %27 = load i32, i32* %14, align 4
  %28 = icmp sgt i32 %27, 1
  br i1 %28, label %29, label %36

29:                                               ; preds = %26
  %30 = load i32, i32* %14, align 4
  %31 = sext i32 %30 to i64
  %32 = load i32, i32* %13, align 4
  %33 = sext i32 %32 to i64
  %34 = mul i64 %31, %33
  %35 = icmp ugt i64 %34, 134217728
  br label %36

36:                                               ; preds = %29, %26
  %37 = phi i1 [ false, %26 ], [ %35, %29 ]
  br i1 %37, label %38, label %41

38:                                               ; preds = %36
  %39 = load i32, i32* %14, align 4
  %40 = sdiv i32 %39, 2
  store i32 %40, i32* %14, align 4
  br label %26, !llvm.loop !71

41:                                               ; preds = %36
  %42 = load i32, i32* %14, align 4
  %43 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 3
  store i32 %42, i32* %43, align 8
  %44 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 4
  store i32 0, i32* %44, align 4
  %45 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  store i64 4, i64* %15, align 8
  %46 = call %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_mRiEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %45, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %12, i64* nonnull align 8 dereferenceable(8) %15, i32* nonnull align 4 dereferenceable(4) %14) #24
  %47 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 5
  store %struct.ListManager* %46, %struct.ListManager** %47, align 8
  %48 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  store i64 4, i64* %16, align 8
  %49 = call %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_mRiEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %48, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %12, i64* nonnull align 8 dereferenceable(8) %16, i32* nonnull align 4 dereferenceable(4) %14) #24
  %50 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 6
  store %struct.ListManager* %49, %struct.ListManager** %50, align 8
  %51 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %52 = call %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_RiS4_EEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %51, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %12, i32* nonnull align 4 dereferenceable(4) %13, i32* nonnull align 4 dereferenceable(4) %14) #24
  %53 = getelementptr inbounds %struct.NodeManager, %struct.NodeManager* %17, i32 0, i32 7
  store %struct.ListManager* %52, %struct.ListManager** %53, align 8
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_mRiEEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %1, i64* nonnull align 8 dereferenceable(8) %2, i32* nonnull align 4 dereferenceable(4) %3) #2 comdat align 2 {
  %5 = alloca %struct.ListManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %8 = alloca i64*, align 8, addrspace(5)
  %9 = alloca i32*, align 8, addrspace(5)
  %10 = alloca %struct.ListManager*, align 8, addrspace(5)
  %11 = addrspacecast %struct.ListManager* addrspace(5)* %5 to %struct.ListManager**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %7 to %struct.LLVMRuntime***
  %14 = addrspacecast i64* addrspace(5)* %8 to i64**
  %15 = addrspacecast i32* addrspace(5)* %9 to i32**
  %16 = addrspacecast %struct.ListManager* addrspace(5)* %10 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store %struct.LLVMRuntime** %1, %struct.LLVMRuntime*** %13, align 8
  store i64* %2, i64** %14, align 8
  store i32* %3, i32** %15, align 8
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %18 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %17, i64 1048616, i64 4096) #24
  %19 = bitcast i8* %18 to %struct.ListManager*
  store %struct.ListManager* %19, %struct.ListManager** %16, align 8
  %20 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  %21 = bitcast %struct.ListManager* %20 to i8*
  %22 = bitcast i8* %21 to %struct.ListManager*
  %23 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %13, align 8
  %24 = call nonnull align 8 dereferenceable(8) %struct.LLVMRuntime** @_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE(%struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %23) #24
  %25 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  %26 = load i64*, i64** %14, align 8
  %27 = call nonnull align 8 dereferenceable(8) i64* @_ZSt7forwardImEOT_RNSt16remove_referenceIS0_E4typeE(i64* nonnull align 8 dereferenceable(8) %26) #24
  %28 = load i64, i64* %27, align 8
  %29 = load i32*, i32** %15, align 8
  %30 = call nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIRiEOT_RNSt16remove_referenceIS1_E4typeE(i32* nonnull align 4 dereferenceable(4) %29) #24
  %31 = load i32, i32* %30, align 4
  %32 = sext i32 %31 to i64
  call void @_ZN11ListManagerC2EP11LLVMRuntimemm(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %22, %struct.LLVMRuntime* %25, i64 %28, i64 %32) #24
  %33 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  ret %struct.ListManager* %33
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden %struct.ListManager* @_ZN11LLVMRuntime6createI11ListManagerJRPS_RiS4_EEEPT_DpOT0_(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %0, %struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %1, i32* nonnull align 4 dereferenceable(4) %2, i32* nonnull align 4 dereferenceable(4) %3) #2 comdat align 2 {
  %5 = alloca %struct.ListManager*, align 8, addrspace(5)
  %6 = alloca %struct.LLVMRuntime*, align 8, addrspace(5)
  %7 = alloca %struct.LLVMRuntime**, align 8, addrspace(5)
  %8 = alloca i32*, align 8, addrspace(5)
  %9 = alloca i32*, align 8, addrspace(5)
  %10 = alloca %struct.ListManager*, align 8, addrspace(5)
  %11 = addrspacecast %struct.ListManager* addrspace(5)* %5 to %struct.ListManager**
  %12 = addrspacecast %struct.LLVMRuntime* addrspace(5)* %6 to %struct.LLVMRuntime**
  %13 = addrspacecast %struct.LLVMRuntime** addrspace(5)* %7 to %struct.LLVMRuntime***
  %14 = addrspacecast i32* addrspace(5)* %8 to i32**
  %15 = addrspacecast i32* addrspace(5)* %9 to i32**
  %16 = addrspacecast %struct.ListManager* addrspace(5)* %10 to %struct.ListManager**
  store %struct.LLVMRuntime* %0, %struct.LLVMRuntime** %12, align 8
  store %struct.LLVMRuntime** %1, %struct.LLVMRuntime*** %13, align 8
  store i32* %2, i32** %14, align 8
  store i32* %3, i32** %15, align 8
  %17 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %12, align 8
  %18 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %17, i64 1048616, i64 4096) #24
  %19 = bitcast i8* %18 to %struct.ListManager*
  store %struct.ListManager* %19, %struct.ListManager** %16, align 8
  %20 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  %21 = bitcast %struct.ListManager* %20 to i8*
  %22 = bitcast i8* %21 to %struct.ListManager*
  %23 = load %struct.LLVMRuntime**, %struct.LLVMRuntime*** %13, align 8
  %24 = call nonnull align 8 dereferenceable(8) %struct.LLVMRuntime** @_ZSt7forwardIRP11LLVMRuntimeEOT_RNSt16remove_referenceIS3_E4typeE(%struct.LLVMRuntime** nonnull align 8 dereferenceable(8) %23) #24
  %25 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %24, align 8
  %26 = load i32*, i32** %14, align 8
  %27 = call nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIRiEOT_RNSt16remove_referenceIS1_E4typeE(i32* nonnull align 4 dereferenceable(4) %26) #24
  %28 = load i32, i32* %27, align 4
  %29 = sext i32 %28 to i64
  %30 = load i32*, i32** %15, align 8
  %31 = call nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIRiEOT_RNSt16remove_referenceIS1_E4typeE(i32* nonnull align 4 dereferenceable(4) %30) #24
  %32 = load i32, i32* %31, align 4
  %33 = sext i32 %32 to i64
  call void @_ZN11ListManagerC2EP11LLVMRuntimemm(%struct.ListManager* nonnull align 8 dereferenceable(1048616) %22, %struct.LLVMRuntime* %25, i64 %29, i64 %33) #24
  %34 = load %struct.ListManager*, %struct.ListManager** %16, align 8
  ret %struct.ListManager* %34
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr nonnull align 4 dereferenceable(4) i32* @_ZSt7forwardIRiEOT_RNSt16remove_referenceIS1_E4typeE(i32* nonnull align 4 dereferenceable(4) %0) #2 comdat {
  %2 = alloca i32*, align 8, addrspace(5)
  %3 = alloca i32*, align 8, addrspace(5)
  %4 = addrspacecast i32* addrspace(5)* %2 to i32**
  %5 = addrspacecast i32* addrspace(5)* %3 to i32**
  store i32* %0, i32** %5, align 8
  %6 = load i32*, i32** %5, align 8
  ret i32* %6
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK14__HIP_BlockDimclEj(%struct.__HIP_BlockDim* nonnull align 1 dereferenceable(1) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.__HIP_BlockDim*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.__HIP_BlockDim* addrspace(5)* %4 to %struct.__HIP_BlockDim**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.__HIP_BlockDim* %0, %struct.__HIP_BlockDim** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.__HIP_BlockDim*, %struct.__HIP_BlockDim** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i64 @__ockl_get_local_size(i32 %10) #17
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK14__HIP_BlockIdxclEj(%struct.__HIP_BlockIdx* nonnull align 1 dereferenceable(1) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.__HIP_BlockIdx*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.__HIP_BlockIdx* addrspace(5)* %4 to %struct.__HIP_BlockIdx**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.__HIP_BlockIdx* %0, %struct.__HIP_BlockIdx** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.__HIP_BlockIdx*, %struct.__HIP_BlockIdx** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i64 @__ockl_get_group_id(i32 %10) #17
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK15__HIP_ThreadIdxclEj(%struct.__HIP_ThreadIdx* nonnull align 1 dereferenceable(1) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.__HIP_ThreadIdx*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.__HIP_ThreadIdx* addrspace(5)* %4 to %struct.__HIP_ThreadIdx**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.__HIP_ThreadIdx* %0, %struct.__HIP_ThreadIdx** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.__HIP_ThreadIdx*, %struct.__HIP_ThreadIdx** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i64 @__ockl_get_local_id(i32 %10) #17
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define linkonce_odr hidden i32 @_ZNK13__HIP_GridDimclEj(%struct.__HIP_GridDim* nonnull align 1 dereferenceable(1) %0, i32 %1) #2 comdat align 2 {
  %3 = alloca i32, align 4, addrspace(5)
  %4 = alloca %struct.__HIP_GridDim*, align 8, addrspace(5)
  %5 = alloca i32, align 4, addrspace(5)
  %6 = addrspacecast i32 addrspace(5)* %3 to i32*
  %7 = addrspacecast %struct.__HIP_GridDim* addrspace(5)* %4 to %struct.__HIP_GridDim**
  %8 = addrspacecast i32 addrspace(5)* %5 to i32*
  store %struct.__HIP_GridDim* %0, %struct.__HIP_GridDim** %7, align 8
  store i32 %1, i32* %8, align 4
  %9 = load %struct.__HIP_GridDim*, %struct.__HIP_GridDim** %7, align 8
  %10 = load i32, i32* %8, align 4
  %11 = call i64 @__ockl_get_num_groups(i32 %10) #17
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_Z11locked_taskIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EvS3_S6_RKT0_(i8* %0, %class.anon.4* nonnull align 8 dereferenceable(16) %1, %class.anon.20* nonnull align 1 dereferenceable(1) %2) #2 {
  %4 = alloca i8*, align 8, addrspace(5)
  %5 = alloca %class.anon.4*, align 8, addrspace(5)
  %6 = alloca %class.anon.20*, align 8, addrspace(5)
  %7 = alloca %class.lock_guard.22, align 1, addrspace(5)
  %8 = addrspacecast i8* addrspace(5)* %4 to i8**
  %9 = addrspacecast %class.anon.4* addrspace(5)* %5 to %class.anon.4**
  %10 = addrspacecast %class.anon.20* addrspace(5)* %6 to %class.anon.20**
  %11 = addrspacecast %class.lock_guard.22 addrspace(5)* %7 to %class.lock_guard.22*
  store i8* %0, i8** %8, align 8
  store %class.anon.4* %1, %class.anon.4** %9, align 8
  store %class.anon.20* %2, %class.anon.20** %10, align 8
  %12 = load i8*, i8** %8, align 8
  %13 = load %class.anon.4*, %class.anon.4** %9, align 8
  %14 = load %class.anon.20*, %class.anon.20** %10, align 8
  call void @_ZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC2EPhRKS1_RKS7_(%class.lock_guard.22* nonnull align 1 dereferenceable(1) %11, i8* %12, %class.anon.4* nonnull align 8 dereferenceable(16) %13, %class.anon.20* nonnull align 1 dereferenceable(1) %14) #24
  ret void
}

; Function Attrs: convergent noinline nounwind optnone
define internal void @_ZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC2EPhRKS1_RKS7_(%class.lock_guard.22* nonnull align 1 dereferenceable(1) %0, i8* %1, %class.anon.4* nonnull align 8 dereferenceable(16) %2, %class.anon.20* nonnull align 1 dereferenceable(1) %3) unnamed_addr #5 align 2 {
  %5 = alloca %class.lock_guard.22*, align 8, addrspace(5)
  %6 = alloca i8*, align 8, addrspace(5)
  %7 = alloca %class.anon.4*, align 8, addrspace(5)
  %8 = alloca %class.anon.20*, align 8, addrspace(5)
  %9 = alloca %class.anon.24, align 8, addrspace(5)
  %10 = alloca i8, align 1, addrspace(5)
  %11 = alloca i8, align 1, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i32, align 4, addrspace(5)
  %14 = alloca i32, align 4, addrspace(5)
  %15 = alloca i32, align 4, addrspace(5)
  %16 = addrspacecast %class.lock_guard.22* addrspace(5)* %5 to %class.lock_guard.22**
  %17 = addrspacecast i8* addrspace(5)* %6 to i8**
  %18 = addrspacecast %class.anon.4* addrspace(5)* %7 to %class.anon.4**
  %19 = addrspacecast %class.anon.20* addrspace(5)* %8 to %class.anon.20**
  %20 = addrspacecast %class.anon.24 addrspace(5)* %9 to %class.anon.24*
  %21 = addrspacecast i8 addrspace(5)* %10 to i8*
  %22 = addrspacecast i8 addrspace(5)* %11 to i8*
  %23 = addrspacecast i32 addrspace(5)* %12 to i32*
  %24 = addrspacecast i32 addrspace(5)* %13 to i32*
  %25 = addrspacecast i32 addrspace(5)* %14 to i32*
  %26 = addrspacecast i32 addrspace(5)* %15 to i32*
  store %class.lock_guard.22* %0, %class.lock_guard.22** %16, align 8
  store i8* %1, i8** %17, align 8
  store %class.anon.4* %2, %class.anon.4** %18, align 8
  store %class.anon.20* %3, %class.anon.20** %19, align 8
  %27 = load %class.lock_guard.22*, %class.lock_guard.22** %16, align 8
  %28 = getelementptr inbounds %class.anon.24, %class.anon.24* %20, i32 0, i32 0
  %29 = load %class.anon.20*, %class.anon.20** %19, align 8
  store %class.anon.20* %29, %class.anon.20** %28, align 8
  %30 = getelementptr inbounds %class.anon.24, %class.anon.24* %20, i32 0, i32 1
  store i8** %17, i8*** %30, align 8
  %31 = getelementptr inbounds %class.anon.24, %class.anon.24* %20, i32 0, i32 2
  %32 = load %class.anon.4*, %class.anon.4** %18, align 8
  store %class.anon.4* %32, %class.anon.4** %31, align 8
  %33 = call i32 @cuda_compute_capability() #24
  %34 = icmp slt i32 %33, 70
  br i1 %34, label %35, label %88

35:                                               ; preds = %4
  store i8 0, i8* %21, align 1
  br label %36

36:                                               ; preds = %48, %35
  %37 = load i8, i8* %21, align 1
  %38 = trunc i8 %37 to i1
  %39 = xor i1 %38, true
  br i1 %39, label %40, label %49

40:                                               ; preds = %36
  %41 = load i8*, i8** %17, align 8
  %42 = bitcast i8* %41 to i32*
  %43 = call i32 @atomic_exchange_i32(i32* %42, i32 1) #24
  %44 = icmp eq i32 %43, 1
  br i1 %44, label %45, label %48

45:                                               ; preds = %40
  %46 = load %class.anon.4*, %class.anon.4** %18, align 8
  call void @_ZZN11ListManager11touch_chunkEiENKUlvE_clEv(%class.anon.4* nonnull align 8 dereferenceable(16) %46) #24
  store i8 1, i8* %21, align 1
  %47 = load i8*, i8** %17, align 8
  call void @mutex_unlock_i32(i8* %47) #24
  br label %48

48:                                               ; preds = %45, %40
  br label %36, !llvm.loop !72

49:                                               ; preds = %36
  store i8 0, i8* %22, align 1
  %50 = load i8, i8* %22, align 1
  %51 = trunc i8 %50 to i1
  br i1 %51, label %52, label %72

52:                                               ; preds = %49
  %53 = call i32 @cuda_active_mask() #24
  store i32 %53, i32* %23, align 4
  %54 = load i32, i32* %23, align 4
  store i32 %54, i32* %24, align 4
  br label %55

55:                                               ; preds = %65, %52
  %56 = load i32, i32* %24, align 4
  %57 = icmp ne i32 %56, 0
  br i1 %57, label %58, label %71

58:                                               ; preds = %55
  %59 = load i32, i32* %24, align 4
  %60 = call i32 @cttz_i32(i32 %59) #24
  store i32 %60, i32* %25, align 4
  %61 = call i32 @warp_idx() #24
  %62 = load i32, i32* %25, align 4
  %63 = icmp eq i32 %61, %62
  br i1 %63, label %64, label %65

64:                                               ; preds = %58
  call void @_ZZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.24* nonnull align 8 dereferenceable(24) %20) #24
  br label %65

65:                                               ; preds = %64, %58
  %66 = load i32, i32* %25, align 4
  %67 = shl i32 1, %66
  %68 = xor i32 %67, -1
  %69 = load i32, i32* %24, align 4
  %70 = and i32 %69, %68
  store i32 %70, i32* %24, align 4
  br label %55, !llvm.loop !73

71:                                               ; preds = %55
  br label %87

72:                                               ; preds = %49
  store i32 0, i32* %26, align 4
  br label %73

73:                                               ; preds = %83, %72
  %74 = load i32, i32* %26, align 4
  %75 = call i32 @warp_size() #24
  %76 = icmp slt i32 %74, %75
  br i1 %76, label %77, label %86

77:                                               ; preds = %73
  %78 = call i32 @warp_idx() #24
  %79 = load i32, i32* %26, align 4
  %80 = icmp eq i32 %78, %79
  br i1 %80, label %81, label %82

81:                                               ; preds = %77
  call void @_ZZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.24* nonnull align 8 dereferenceable(24) %20) #24
  br label %82

82:                                               ; preds = %81, %77
  br label %83

83:                                               ; preds = %82
  %84 = load i32, i32* %26, align 4
  %85 = add nsw i32 %84, 1
  store i32 %85, i32* %26, align 4
  br label %73, !llvm.loop !74

86:                                               ; preds = %73
  br label %87

87:                                               ; preds = %86, %71
  br label %89

88:                                               ; preds = %4
  call void @_ZZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.24* nonnull align 8 dereferenceable(24) %20) #24
  br label %89

89:                                               ; preds = %88, %87
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZN11ListManager11touch_chunkEiENKUlvE_clEv(%class.anon.4* nonnull align 8 dereferenceable(16) %0) #2 align 2 {
  %2 = alloca %class.anon.4*, align 8, addrspace(5)
  %3 = alloca i8*, align 8, addrspace(5)
  %4 = addrspacecast %class.anon.4* addrspace(5)* %2 to %class.anon.4**
  %5 = addrspacecast i8* addrspace(5)* %3 to i8**
  store %class.anon.4* %0, %class.anon.4** %4, align 8
  %6 = load %class.anon.4*, %class.anon.4** %4, align 8
  %7 = getelementptr inbounds %class.anon.4, %class.anon.4* %6, i32 0, i32 0
  %8 = load %struct.ListManager*, %struct.ListManager** %7, align 8
  %9 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 0
  %10 = getelementptr inbounds %class.anon.4, %class.anon.4* %6, i32 0, i32 1
  %11 = load i32*, i32** %10, align 8
  %12 = load i32, i32* %11, align 4
  %13 = sext i32 %12 to i64
  %14 = getelementptr inbounds [131072 x i8*], [131072 x i8*]* %9, i64 0, i64 %13
  %15 = load i8*, i8** %14, align 8
  %16 = icmp ne i8* %15, null
  br i1 %16, label %36, label %17

17:                                               ; preds = %1
  call void @grid_memfence() #24
  %18 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 6
  %19 = load %struct.LLVMRuntime*, %struct.LLVMRuntime** %18, align 8
  %20 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 2
  %21 = load i64, i64* %20, align 8
  %22 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 1
  %23 = load i64, i64* %22, align 8
  %24 = mul i64 %21, %23
  %25 = call i8* @_ZN11LLVMRuntime24request_allocate_alignedEmm(%struct.LLVMRuntime* nonnull align 8 dereferenceable(35256) %19, i64 %24, i64 4096) #24
  store i8* %25, i8** %5, align 8
  %26 = getelementptr inbounds %struct.ListManager, %struct.ListManager* %8, i32 0, i32 0
  %27 = getelementptr inbounds %class.anon.4, %class.anon.4* %6, i32 0, i32 1
  %28 = load i32*, i32** %27, align 8
  %29 = load i32, i32* %28, align 4
  %30 = sext i32 %29 to i64
  %31 = getelementptr inbounds [131072 x i8*], [131072 x i8*]* %26, i64 0, i64 %30
  %32 = bitcast i8** %31 to i64*
  %33 = load i8*, i8** %5, align 8
  %34 = ptrtoint i8* %33 to i64
  %35 = call i64 @atomic_exchange_u64(i64* %32, i64 %34) #24
  br label %36

36:                                               ; preds = %17, %1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal void @_ZZN10lock_guardIZN11ListManager11touch_chunkEiEUlvE_Z11locked_taskIS1_EvPvRKT_EUlvE_EC1EPhRKS1_RKS7_ENKUlvE_clEv(%class.anon.24* nonnull align 8 dereferenceable(24) %0) #2 align 2 {
  %2 = alloca %class.anon.24*, align 8, addrspace(5)
  %3 = addrspacecast %class.anon.24* addrspace(5)* %2 to %class.anon.24**
  store %class.anon.24* %0, %class.anon.24** %3, align 8
  %4 = load %class.anon.24*, %class.anon.24** %3, align 8
  %5 = getelementptr inbounds %class.anon.24, %class.anon.24* %4, i32 0, i32 0
  %6 = load %class.anon.20*, %class.anon.20** %5, align 8
  %7 = call zeroext i1 @_ZZ11locked_taskIZN11ListManager11touch_chunkEiEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.20* nonnull align 1 dereferenceable(1) %6) #24
  br i1 %7, label %8, label %22

8:                                                ; preds = %1
  %9 = getelementptr inbounds %class.anon.24, %class.anon.24* %4, i32 0, i32 1
  %10 = load i8**, i8*** %9, align 8
  %11 = load i8*, i8** %10, align 8
  call void @mutex_lock_i32(i8* %11) #24
  call void @grid_memfence() #24
  %12 = getelementptr inbounds %class.anon.24, %class.anon.24* %4, i32 0, i32 0
  %13 = load %class.anon.20*, %class.anon.20** %12, align 8
  %14 = call zeroext i1 @_ZZ11locked_taskIZN11ListManager11touch_chunkEiEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.20* nonnull align 1 dereferenceable(1) %13) #24
  br i1 %14, label %15, label %18

15:                                               ; preds = %8
  %16 = getelementptr inbounds %class.anon.24, %class.anon.24* %4, i32 0, i32 2
  %17 = load %class.anon.4*, %class.anon.4** %16, align 8
  call void @_ZZN11ListManager11touch_chunkEiENKUlvE_clEv(%class.anon.4* nonnull align 8 dereferenceable(16) %17) #24
  br label %18

18:                                               ; preds = %15, %8
  call void @grid_memfence() #24
  %19 = getelementptr inbounds %class.anon.24, %class.anon.24* %4, i32 0, i32 1
  %20 = load i8**, i8*** %19, align 8
  %21 = load i8*, i8** %20, align 8
  call void @mutex_unlock_i32(i8* %21) #24
  br label %22

22:                                               ; preds = %18, %1
  ret void
}

; Function Attrs: convergent mustprogress noinline nounwind optnone
define internal zeroext i1 @_ZZ11locked_taskIZN11ListManager11touch_chunkEiEUlvE_EvPvRKT_ENKUlvE_clEv(%class.anon.20* nonnull align 1 dereferenceable(1) %0) #2 align 2 {
  %2 = alloca i1, align 1, addrspace(5)
  %3 = alloca %class.anon.20*, align 8, addrspace(5)
  %4 = addrspacecast i1 addrspace(5)* %2 to i1*
  %5 = addrspacecast %class.anon.20* addrspace(5)* %3 to %class.anon.20**
  store %class.anon.20* %0, %class.anon.20** %5, align 8
  %6 = load %class.anon.20*, %class.anon.20** %5, align 8
  ret i1 true
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocml_acos_f64(double %0) #6 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = fcmp oge double %2, 5.000000e-01
  %4 = tail call double @llvm.fma.f64(double %2, double -5.000000e-01, double 5.000000e-01)
  %5 = fmul double %0, %0
  %6 = select i1 %3, double %4, double %5
  %7 = tail call double @llvm.fma.f64(double %6, double 0x3FA059859FEA6A70, double 0xBF90A5A378A05EAF)
  %8 = tail call double @llvm.fma.f64(double %6, double %7, double 0x3F94052137024D6A)
  %9 = tail call double @llvm.fma.f64(double %6, double %8, double 0x3F7AB3A098A70509)
  %10 = tail call double @llvm.fma.f64(double %6, double %9, double 0x3F88ED60A300C8D2)
  %11 = tail call double @llvm.fma.f64(double %6, double %10, double 0x3F8C6FA84B77012B)
  %12 = tail call double @llvm.fma.f64(double %6, double %11, double 0x3F91C6C111DCCB70)
  %13 = tail call double @llvm.fma.f64(double %6, double %12, double 0x3F96E89F0A0ADACF)
  %14 = tail call double @llvm.fma.f64(double %6, double %13, double 0x3F9F1C72C668963F)
  %15 = tail call double @llvm.fma.f64(double %6, double %14, double 0x3FA6DB6DB41CE4BD)
  %16 = tail call double @llvm.fma.f64(double %6, double %15, double 0x3FB333333336FD5B)
  %17 = tail call double @llvm.fma.f64(double %6, double %16, double 0x3FC5555555555380)
  %18 = fmul double %6, %17
  %19 = tail call double @llvm.fma.f64(double %0, double %18, double %0)
  %20 = fneg double %19
  %21 = tail call double @llvm.fma.f64(double 0x3FEDD9AD336A0500, double 0x3FFAF154EEB562D6, double %20)
  br i1 %3, label %22, label %69

22:                                               ; preds = %1
  %23 = tail call double @llvm.amdgcn.rsq.f64(double %6) #21
  %24 = fmul double %6, %23
  %25 = fmul double %23, 5.000000e-01
  %26 = fneg double %25
  %27 = tail call double @llvm.fma.f64(double %26, double %24, double 5.000000e-01) #21
  %28 = tail call double @llvm.fma.f64(double %25, double %27, double %25) #21
  %29 = tail call double @llvm.fma.f64(double %24, double %27, double %24) #21
  %30 = fneg double %29
  %31 = tail call double @llvm.fma.f64(double %30, double %29, double %6) #21
  %32 = tail call double @llvm.fma.f64(double %31, double %28, double %29) #21
  %33 = fcmp oeq double %6, 0.000000e+00
  %34 = select i1 %33, double %6, double %32
  %35 = fmul double %34, %34
  %36 = fneg double %35
  %37 = tail call double @llvm.fma.f64(double %34, double %34, double %36) #21
  %38 = fsub double %6, %35
  %39 = fsub double %6, %38
  %40 = fsub double %39, %35
  %41 = fsub double %40, %37
  %42 = fadd double %38, %41
  %43 = fmul double %34, 2.000000e+00
  %44 = tail call double @llvm.amdgcn.rcp.f64(double %43) #21
  %45 = fneg double %43
  %46 = tail call double @llvm.fma.f64(double %45, double %44, double 1.000000e+00) #21
  %47 = tail call double @llvm.fma.f64(double %46, double %44, double %44) #21
  %48 = tail call double @llvm.fma.f64(double %45, double %47, double 1.000000e+00) #21
  %49 = tail call double @llvm.fma.f64(double %48, double %47, double %47) #21
  %50 = fmul double %42, %49
  %51 = tail call double @llvm.fma.f64(double %45, double %50, double %42) #21
  %52 = tail call double @llvm.fma.f64(double %51, double %49, double %50) #21
  %53 = select i1 %33, double 0.000000e+00, double %52
  %54 = fadd double %34, %53
  %55 = fsub double %54, %34
  %56 = fsub double %53, %55
  %57 = tail call double @llvm.fma.f64(double %54, double %18, double %54)
  %58 = fmul double %57, -2.000000e+00
  %59 = tail call double @llvm.fma.f64(double 0x3FFDD9AD336A0500, double 0x3FFAF154EEB562D6, double %58)
  %60 = tail call double @llvm.fma.f64(double %54, double %18, double %56)
  %61 = fadd double %54, %60
  %62 = fmul double %61, 2.000000e+00
  %63 = fcmp olt double %0, 0.000000e+00
  %64 = select i1 %63, double %59, double %62
  %65 = fcmp oeq double %0, -1.000000e+00
  %66 = select i1 %65, double 0x400921FB54442D18, double %64
  %67 = fcmp oeq double %0, 1.000000e+00
  %68 = select i1 %67, double 0.000000e+00, double %66
  br label %69

69:                                               ; preds = %22, %1
  %70 = phi double [ %68, %22 ], [ %21, %1 ]
  ret double %70
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.fabs.f64(double) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.fma.f64(double, double, double) #7

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.rsq.f64(double) #8

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.rcp.f64(double) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_acos_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = tail call float @__ocml_fmuladd_f32(float -5.000000e-01, float %2, float 5.000000e-01) #17
  %4 = fmul float %0, %0
  %5 = fcmp ogt float %2, 5.000000e-01
  %6 = select i1 %5, float %3, float %4
  %7 = tail call float @__ocml_fmuladd_f32(float %6, float 0x3FA38434E0000000, float 0x3F8BF8BB40000000) #17
  %8 = tail call float @__ocml_fmuladd_f32(float %6, float %7, float 0x3FA0698780000000) #17
  %9 = tail call float @__ocml_fmuladd_f32(float %6, float %8, float 0x3FA6C83620000000) #17
  %10 = tail call float @__ocml_fmuladd_f32(float %6, float %9, float 0x3FB3337900000000) #17
  %11 = tail call float @__ocml_fmuladd_f32(float %6, float %10, float 0x3FC5555580000000) #17
  %12 = fmul float %6, %11
  %13 = tail call float @llvm.sqrt.f32(float %6)
  %14 = tail call float @__ocml_fmuladd_f32(float %13, float %12, float %13) #17
  %15 = fmul float %14, 2.000000e+00
  %16 = fneg float %15
  %17 = tail call float @__ocml_fmuladd_f32(float 0x3FFDDCB020000000, float 0x3FFAEE9D60000000, float %16) #17
  %18 = fcmp olt float %0, 0.000000e+00
  %19 = select i1 %18, float %17, float %15
  %20 = tail call float @__ocml_fmuladd_f32(float %0, float %12, float %0) #17
  %21 = fneg float %20
  %22 = tail call float @__ocml_fmuladd_f32(float 0x3FEDDCB020000000, float 0x3FFAEE9D60000000, float %21) #17
  %23 = select i1 %5, float %19, float %22
  ret float %23
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.fabs.f32(float) #7

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal float @__ocml_fmuladd_f32(float %0, float %1, float %2) local_unnamed_addr #10 {
  %4 = tail call float @llvm.fmuladd.f32(float %0, float %1, float %2)
  ret float %4
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.sqrt.f32(float) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.fmuladd.f32(float, float, float) #7

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocml_asin_f64(double %0) #6 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = fcmp oge double %2, 5.000000e-01
  %4 = tail call double @llvm.fma.f64(double %2, double -5.000000e-01, double 5.000000e-01)
  %5 = fmul double %0, %0
  %6 = select i1 %3, double %4, double %5
  %7 = tail call double @llvm.fma.f64(double %6, double 0x3FA059859FEA6A70, double 0xBF90A5A378A05EAF)
  %8 = tail call double @llvm.fma.f64(double %6, double %7, double 0x3F94052137024D6A)
  %9 = tail call double @llvm.fma.f64(double %6, double %8, double 0x3F7AB3A098A70509)
  %10 = tail call double @llvm.fma.f64(double %6, double %9, double 0x3F88ED60A300C8D2)
  %11 = tail call double @llvm.fma.f64(double %6, double %10, double 0x3F8C6FA84B77012B)
  %12 = tail call double @llvm.fma.f64(double %6, double %11, double 0x3F91C6C111DCCB70)
  %13 = tail call double @llvm.fma.f64(double %6, double %12, double 0x3F96E89F0A0ADACF)
  %14 = tail call double @llvm.fma.f64(double %6, double %13, double 0x3F9F1C72C668963F)
  %15 = tail call double @llvm.fma.f64(double %6, double %14, double 0x3FA6DB6DB41CE4BD)
  %16 = tail call double @llvm.fma.f64(double %6, double %15, double 0x3FB333333336FD5B)
  %17 = tail call double @llvm.fma.f64(double %6, double %16, double 0x3FC5555555555380)
  %18 = fmul double %6, %17
  %19 = tail call double @llvm.fma.f64(double %2, double %18, double %2)
  br i1 %3, label %20, label %79

20:                                               ; preds = %1
  %21 = tail call double @llvm.amdgcn.rsq.f64(double %6) #21
  %22 = fmul double %6, %21
  %23 = fmul double %21, 5.000000e-01
  %24 = fneg double %23
  %25 = tail call double @llvm.fma.f64(double %24, double %22, double 5.000000e-01) #21
  %26 = tail call double @llvm.fma.f64(double %23, double %25, double %23) #21
  %27 = tail call double @llvm.fma.f64(double %22, double %25, double %22) #21
  %28 = fneg double %27
  %29 = tail call double @llvm.fma.f64(double %28, double %27, double %6) #21
  %30 = tail call double @llvm.fma.f64(double %29, double %26, double %27) #21
  %31 = fcmp oeq double %6, 0.000000e+00
  %32 = select i1 %31, double %6, double %30
  %33 = fmul double %32, %32
  %34 = fneg double %33
  %35 = tail call double @llvm.fma.f64(double %32, double %32, double %34) #21
  %36 = fsub double %6, %33
  %37 = fsub double %6, %36
  %38 = fsub double %37, %33
  %39 = fsub double %38, %35
  %40 = fadd double %36, %39
  %41 = fmul double %32, 2.000000e+00
  %42 = tail call double @llvm.amdgcn.rcp.f64(double %41) #21
  %43 = fneg double %41
  %44 = tail call double @llvm.fma.f64(double %43, double %42, double 1.000000e+00) #21
  %45 = tail call double @llvm.fma.f64(double %44, double %42, double %42) #21
  %46 = tail call double @llvm.fma.f64(double %43, double %45, double 1.000000e+00) #21
  %47 = tail call double @llvm.fma.f64(double %46, double %45, double %45) #21
  %48 = fmul double %40, %47
  %49 = tail call double @llvm.fma.f64(double %43, double %48, double %40) #21
  %50 = tail call double @llvm.fma.f64(double %49, double %47, double %48) #21
  %51 = select i1 %31, double 0.000000e+00, double %50
  %52 = fadd double %32, %51
  %53 = fsub double %52, %32
  %54 = fsub double %51, %53
  %55 = fmul double %18, %52
  %56 = fneg double %55
  %57 = tail call double @llvm.fma.f64(double %52, double %18, double %56) #21
  %58 = tail call double @llvm.fma.f64(double %54, double %18, double %57) #21
  %59 = fadd double %55, %58
  %60 = fsub double %59, %55
  %61 = fsub double %58, %60
  %62 = fadd double %52, %59
  %63 = fsub double %62, %52
  %64 = fsub double %59, %63
  %65 = fadd double %54, %61
  %66 = fadd double %65, %64
  %67 = fadd double %62, %66
  %68 = fsub double %67, %62
  %69 = fsub double %66, %68
  %70 = fsub double 0x3FE921FB54442D18, %67
  %71 = fsub double 0x3FE921FB54442D18, %70
  %72 = fsub double %71, %67
  %73 = fadd double %72, 0x3C81A62633145C07
  %74 = fsub double %73, %69
  %75 = fadd double %70, %74
  %76 = fadd double %75, %75
  %77 = fcmp oeq double %2, 1.000000e+00
  %78 = select i1 %77, double 0x3FF921FB54442D18, double %76
  br label %79

79:                                               ; preds = %20, %1
  %80 = phi double [ %78, %20 ], [ %19, %1 ]
  %81 = tail call double @llvm.copysign.f64(double %80, double %0)
  ret double %81
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.copysign.f64(double, double) #7

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_asin_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = tail call float @__ocml_fmuladd_f32(float %2, float -5.000000e-01, float 5.000000e-01) #17
  %4 = fmul float %0, %0
  %5 = fcmp oge float %2, 5.000000e-01
  %6 = select i1 %5, float %3, float %4
  %7 = tail call float @__ocml_fmuladd_f32(float %6, float 0x3FA38434E0000000, float 0x3F8BF8BB40000000) #17
  %8 = tail call float @__ocml_fmuladd_f32(float %6, float %7, float 0x3FA0698780000000) #17
  %9 = tail call float @__ocml_fmuladd_f32(float %6, float %8, float 0x3FA6C83620000000) #17
  %10 = tail call float @__ocml_fmuladd_f32(float %6, float %9, float 0x3FB3337900000000) #17
  %11 = tail call float @__ocml_fmuladd_f32(float %6, float %10, float 0x3FC5555580000000) #17
  %12 = fmul float %6, %11
  %13 = tail call float @llvm.sqrt.f32(float %6)
  %14 = tail call float @__ocml_fmuladd_f32(float %13, float %12, float %13) #17
  %15 = fmul float %14, -2.000000e+00
  %16 = tail call float @__ocml_fmuladd_f32(float 0x3FEDDCB020000000, float 0x3FFAEE9D60000000, float %15) #17
  %17 = tail call float @__ocml_fmuladd_f32(float %2, float %12, float %2) #17
  %18 = fcmp olt float %2, 5.000000e-01
  %19 = select i1 %18, float %17, float %16
  %20 = tail call float @llvm.copysign.f32(float %19, float %0)
  ret float %20
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.copysign.f32(float, float) #7

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_atan2_f64(double %0, double %1) #9 {
  %3 = tail call double @llvm.fabs.f64(double %0)
  %4 = tail call double @llvm.fabs.f64(double %1)
  %5 = tail call double @llvm.maxnum.f64(double %4, double %3)
  %6 = tail call double @llvm.minnum.f64(double %4, double %3)
  %7 = fdiv double %6, %5
  %8 = tail call double @__ocmlpriv_atanred_f64(double %7) #17
  %9 = bitcast double %1 to <2 x i32>
  %10 = extractelement <2 x i32> %9, i64 1
  %11 = icmp slt i32 %10, 0
  %12 = fsub double 0x3FF921FB54442D18, %8
  %13 = fcmp olt double %4, %3
  %14 = select i1 %13, double %12, double %8
  %15 = fsub double 0x400921FB54442D18, %14
  %16 = select i1 %11, double %15, double %14
  %17 = select i1 %11, double 0x400921FB54442D18, double 0.000000e+00
  %18 = fcmp oeq double %0, 0.000000e+00
  %19 = select i1 %18, double %17, double %16
  %20 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %21 = icmp eq i8 %20, 0
  br i1 %21, label %22, label %31

22:                                               ; preds = %2
  %23 = select i1 %11, double 0x4002D97C7F3321D2, double 0x3FE921FB54442D18
  %24 = tail call double @llvm.copysign.f64(double %23, double %0)
  %25 = fcmp oeq double %4, 0x7FF0000000000000
  %26 = fcmp oeq double %3, 0x7FF0000000000000
  %27 = and i1 %26, %25
  %28 = select i1 %27, double %24, double %19
  %29 = fcmp uno double %1, %0
  %30 = select i1 %29, double 0x7FF8000000000000, double %28
  br label %31

31:                                               ; preds = %22, %2
  %32 = phi double [ %19, %2 ], [ %30, %22 ]
  %33 = tail call double @llvm.copysign.f64(double %32, double %0)
  ret double %33
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.maxnum.f64(double, double) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.minnum.f64(double, double) #7

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocmlpriv_atanred_f64(double %0) local_unnamed_addr #10 {
  %2 = fmul double %0, %0
  %3 = tail call double @llvm.fma.f64(double %2, double 0x3EEBA404B5E68A13, double 0xBF23E260BD3237F4)
  %4 = tail call double @llvm.fma.f64(double %2, double %3, double 0x3F4B2BB069EFB384)
  %5 = tail call double @llvm.fma.f64(double %2, double %4, double 0xBF67952DAF56DE9B)
  %6 = tail call double @llvm.fma.f64(double %2, double %5, double 0x3F7D6D43A595C56F)
  %7 = tail call double @llvm.fma.f64(double %2, double %6, double 0xBF8C6EA4A57D9582)
  %8 = tail call double @llvm.fma.f64(double %2, double %7, double 0x3F967E295F08B19F)
  %9 = tail call double @llvm.fma.f64(double %2, double %8, double 0xBF9E9AE6FC27006A)
  %10 = tail call double @llvm.fma.f64(double %2, double %9, double 0x3FA2C15B5711927A)
  %11 = tail call double @llvm.fma.f64(double %2, double %10, double 0xBFA59976E82D3FF0)
  %12 = tail call double @llvm.fma.f64(double %2, double %11, double 0x3FA82D5D6EF28734)
  %13 = tail call double @llvm.fma.f64(double %2, double %12, double 0xBFAAE5CE6A214619)
  %14 = tail call double @llvm.fma.f64(double %2, double %13, double 0x3FAE1BB48427B883)
  %15 = tail call double @llvm.fma.f64(double %2, double %14, double 0xBFB110E48B207F05)
  %16 = tail call double @llvm.fma.f64(double %2, double %15, double 0x3FB3B13657B87036)
  %17 = tail call double @llvm.fma.f64(double %2, double %16, double 0xBFB745D119378E4F)
  %18 = tail call double @llvm.fma.f64(double %2, double %17, double 0x3FBC71C717E1913C)
  %19 = tail call double @llvm.fma.f64(double %2, double %18, double 0xBFC2492492376B7D)
  %20 = tail call double @llvm.fma.f64(double %2, double %19, double 0x3FC99999999952CC)
  %21 = tail call double @llvm.fma.f64(double %2, double %20, double 0xBFD5555555555523)
  %22 = fmul double %2, %21
  %23 = tail call double @llvm.fma.f64(double %0, double %22, double %0)
  ret double %23
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_atan2_f32(float %0, float %1) #9 {
  %3 = tail call float @llvm.fabs.f32(float %1)
  %4 = tail call float @llvm.fabs.f32(float %0)
  %5 = tail call float @llvm.minnum.f32(float %3, float %4)
  %6 = tail call float @llvm.maxnum.f32(float %3, float %4)
  %7 = load i8, i8 addrspace(4)* @__oclc_daz_opt, align 1, !tbaa !75, !range !79
  %8 = icmp eq i8 %7, 0
  br i1 %8, label %16, label %9

9:                                                ; preds = %2
  %10 = fcmp ogt float %6, 0x45F0000000000000
  %11 = select i1 %10, float 0x3DF0000000000000, float 1.000000e+00
  %12 = fmul float %6, %11
  %13 = tail call float @llvm.amdgcn.rcp.f32(float %12)
  %14 = fmul float %5, %13
  %15 = fmul float %11, %14
  br label %18

16:                                               ; preds = %2
  %17 = fdiv float %5, %6, !fpmath !80
  br label %18

18:                                               ; preds = %16, %9
  %19 = phi float [ %15, %9 ], [ %17, %16 ]
  %20 = tail call float @__ocmlpriv_atanred_f32(float %19) #17
  %21 = fsub float 0x3FF921FB60000000, %20
  %22 = fcmp ogt float %4, %3
  %23 = select i1 %22, float %21, float %20
  %24 = fsub float 0x400921FB60000000, %23
  %25 = fcmp olt float %1, 0.000000e+00
  %26 = select i1 %25, float %24, float %23
  %27 = bitcast float %1 to i32
  %28 = icmp slt i32 %27, 0
  %29 = select i1 %28, float 0x400921FB60000000, float 0.000000e+00
  %30 = fcmp oeq float %0, 0.000000e+00
  %31 = select i1 %30, float %29, float %26
  %32 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %33 = icmp eq i8 %32, 0
  %34 = select i1 %25, float 0x4002D97C80000000, float 0x3FE921FB60000000
  %35 = fcmp oeq float %3, 0x7FF0000000000000
  %36 = fcmp oeq float %4, 0x7FF0000000000000
  %37 = and i1 %36, %35
  %38 = select i1 %37, float %34, float %31
  %39 = fcmp uno float %1, %0
  %40 = select i1 %39, float 0x7FF8000000000000, float %38
  %41 = select i1 %33, float %40, float %31
  %42 = tail call float @llvm.copysign.f32(float %41, float %0)
  ret float %42
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.minnum.f32(float, float) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.maxnum.f32(float, float) #7

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.amdgcn.rcp.f32(float) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocmlpriv_atanred_f32(float %0) local_unnamed_addr #9 {
  %2 = fmul float %0, %0
  %3 = tail call float @__ocml_fmuladd_f32(float %2, float 0x3F65A54B00000000, float 0xBF8F4B2180000000) #17
  %4 = tail call float @__ocml_fmuladd_f32(float %2, float %3, float 0x3FA53F67E0000000) #17
  %5 = tail call float @__ocml_fmuladd_f32(float %2, float %4, float 0xBFB2FA9AE0000000) #17
  %6 = tail call float @__ocml_fmuladd_f32(float %2, float %5, float 0x3FBB263640000000) #17
  %7 = tail call float @__ocml_fmuladd_f32(float %2, float %6, float 0xBFC22C1CC0000000) #17
  %8 = tail call float @__ocml_fmuladd_f32(float %2, float %7, float 0x3FC99717E0000000) #17
  %9 = tail call float @__ocml_fmuladd_f32(float %2, float %8, float 0xBFD5554C40000000) #17
  %10 = fmul float %2, %9
  %11 = tail call float @__ocml_fmuladd_f32(float %0, float %10, float %0) #17
  ret float %11
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocml_exp_f64(double %0) #10 {
  %2 = fmul double %0, 0x3FF71547652B82FE
  %3 = tail call double @llvm.rint.f64(double %2)
  %4 = fneg double %3
  %5 = tail call double @llvm.fma.f64(double %4, double 0x3FE62E42FEFA39EF, double %0)
  %6 = tail call double @llvm.fma.f64(double %4, double 0x3C7ABC9E3B39803F, double %5)
  %7 = tail call double @llvm.fma.f64(double %6, double 0x3E5ADE156A5DCB37, double 0x3E928AF3FCA7AB0C)
  %8 = tail call double @llvm.fma.f64(double %6, double %7, double 0x3EC71DEE623FDE64)
  %9 = tail call double @llvm.fma.f64(double %6, double %8, double 0x3EFA01997C89E6B0)
  %10 = tail call double @llvm.fma.f64(double %6, double %9, double 0x3F2A01A014761F6E)
  %11 = tail call double @llvm.fma.f64(double %6, double %10, double 0x3F56C16C1852B7B0)
  %12 = tail call double @llvm.fma.f64(double %6, double %11, double 0x3F81111111122322)
  %13 = tail call double @llvm.fma.f64(double %6, double %12, double 0x3FA55555555502A1)
  %14 = tail call double @llvm.fma.f64(double %6, double %13, double 0x3FC5555555555511)
  %15 = tail call double @llvm.fma.f64(double %6, double %14, double 0x3FE000000000000B)
  %16 = tail call double @llvm.fma.f64(double %6, double %15, double 1.000000e+00)
  %17 = tail call double @llvm.fma.f64(double %6, double %16, double 1.000000e+00)
  %18 = fptosi double %3 to i32
  %19 = tail call double @llvm.amdgcn.ldexp.f64(double %17, i32 %18)
  %20 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %21 = icmp eq i8 %20, 0
  %22 = fcmp ogt double %0, 1.024000e+03
  %23 = select i1 %21, i1 %22, i1 false
  %24 = select i1 %23, double 0x7FF0000000000000, double %19
  %25 = fcmp olt double %0, -1.075000e+03
  %26 = select i1 %25, double 0.000000e+00, double %24
  ret double %26
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.rint.f64(double) #7

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.ldexp.f64(double, i32) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_exp_f32(float %0) #9 {
  %2 = load i8, i8 addrspace(4)* @__oclc_daz_opt, align 1, !tbaa !75, !range !79
  %3 = icmp eq i8 %2, 0
  %4 = load i8, i8 addrspace(4)* @__oclc_unsafe_math_opt, align 1, !tbaa !75, !range !79
  %5 = icmp eq i8 %4, 0
  br i1 %3, label %42, label %6

6:                                                ; preds = %1
  br i1 %5, label %10, label %7

7:                                                ; preds = %6
  %8 = fmul float %0, 0x3FF7154760000000
  %9 = tail call float @llvm.exp2.f32(float %8)
  br label %83

10:                                               ; preds = %6
  %11 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %12 = freeze i32 %11
  %13 = icmp sgt i32 %12, 8999
  br i1 %13, label %15, label %14

14:                                               ; preds = %10
  switch i32 %12, label %20 [
    i32 8001, label %15
    i32 7001, label %15
  ]

15:                                               ; preds = %14, %14, %10
  %16 = fmul float %0, 0x3FF7154760000000
  %17 = fneg float %16
  %18 = tail call float @llvm.fma.f32(float %0, float 0x3FF7154760000000, float %17)
  %19 = tail call float @llvm.fma.f32(float %0, float 0x3E54AE0BE0000000, float %18)
  br label %29

20:                                               ; preds = %14
  %21 = bitcast float %0 to i32
  %22 = and i32 %21, -4096
  %23 = bitcast i32 %22 to float
  %24 = fsub float %0, %23
  %25 = fmul float %23, 0x3FF7140000000000
  %26 = fmul float %24, 0x3F347652A0000000
  %27 = tail call float @__ocml_fmuladd_f32(float %24, float 0x3FF7140000000000, float %26) #17
  %28 = tail call float @__ocml_fmuladd_f32(float %23, float 0x3F347652A0000000, float %27) #17
  br label %29

29:                                               ; preds = %20, %15
  %30 = phi float [ %19, %15 ], [ %28, %20 ]
  %31 = phi float [ %16, %15 ], [ %25, %20 ]
  %32 = tail call float @llvm.rint.f32(float %31)
  %33 = fsub float %31, %32
  %34 = fadd float %30, %33
  %35 = tail call float @llvm.exp2.f32(float %34)
  %36 = fptosi float %32 to i32
  %37 = tail call float @llvm.amdgcn.ldexp.f32(float %35, i32 %36)
  %38 = fcmp olt float %0, 0xC055D58A00000000
  %39 = select i1 %38, float 0.000000e+00, float %37
  %40 = fcmp ogt float %0, 0x40562E4300000000
  %41 = select i1 %40, float 0x7FF0000000000000, float %39
  br label %83

42:                                               ; preds = %1
  br i1 %5, label %51, label %43

43:                                               ; preds = %42
  %44 = fcmp olt float %0, 0xC055D58A00000000
  %45 = select i1 %44, float 6.400000e+01, float 0.000000e+00
  %46 = fadd float %45, %0
  %47 = fmul float %46, 0x3FF7154760000000
  %48 = tail call float @llvm.exp2.f32(float %47)
  %49 = select i1 %44, float 0x3A2969D480000000, float 1.000000e+00
  %50 = fmul float %49, %48
  br label %83

51:                                               ; preds = %42
  %52 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %53 = freeze i32 %52
  %54 = icmp sgt i32 %53, 8999
  br i1 %54, label %56, label %55

55:                                               ; preds = %51
  switch i32 %53, label %61 [
    i32 8001, label %56
    i32 7001, label %56
  ]

56:                                               ; preds = %55, %55, %51
  %57 = fmul float %0, 0x3FF7154760000000
  %58 = fneg float %57
  %59 = tail call float @llvm.fma.f32(float %0, float 0x3FF7154760000000, float %58)
  %60 = tail call float @llvm.fma.f32(float %0, float 0x3E54AE0BE0000000, float %59)
  br label %70

61:                                               ; preds = %55
  %62 = bitcast float %0 to i32
  %63 = and i32 %62, -4096
  %64 = bitcast i32 %63 to float
  %65 = fsub float %0, %64
  %66 = fmul float %64, 0x3FF7140000000000
  %67 = fmul float %65, 0x3F347652A0000000
  %68 = tail call float @__ocml_fmuladd_f32(float %65, float 0x3FF7140000000000, float %67) #17
  %69 = tail call float @__ocml_fmuladd_f32(float %64, float 0x3F347652A0000000, float %68) #17
  br label %70

70:                                               ; preds = %61, %56
  %71 = phi float [ %60, %56 ], [ %69, %61 ]
  %72 = phi float [ %57, %56 ], [ %66, %61 ]
  %73 = tail call float @llvm.rint.f32(float %72)
  %74 = fsub float %72, %73
  %75 = fadd float %71, %74
  %76 = tail call float @llvm.exp2.f32(float %75)
  %77 = fptosi float %73 to i32
  %78 = tail call float @llvm.amdgcn.ldexp.f32(float %76, i32 %77)
  %79 = fcmp olt float %0, 0xC059D1DA00000000
  %80 = select i1 %79, float 0.000000e+00, float %78
  %81 = fcmp ogt float %0, 0x40562E4300000000
  %82 = select i1 %81, float 0x7FF0000000000000, float %80
  br label %83

83:                                               ; preds = %70, %43, %29, %7
  %84 = phi float [ %9, %7 ], [ %41, %29 ], [ %50, %43 ], [ %82, %70 ]
  ret float %84
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.exp2.f32(float) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.fma.f32(float, float, float) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.rint.f32(float) #7

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.amdgcn.ldexp.f32(float, i32) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_cos_f64(double %0) #9 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = tail call %0 @__ocmlpriv_trigred_f64(double %2) #17
  %4 = extractvalue %0 %3, 0
  %5 = extractvalue %0 %3, 1
  %6 = extractvalue %0 %3, 2
  %7 = tail call %1 @__ocmlpriv_sincosred2_f64(double %5, double %4) #17
  %8 = extractvalue %1 %7, 0
  %9 = extractvalue %1 %7, 1
  %10 = fneg double %8
  %11 = and i32 %6, 1
  %12 = icmp eq i32 %11, 0
  %13 = select i1 %12, double %9, double %10
  %14 = bitcast double %13 to <2 x i32>
  %15 = icmp sgt i32 %6, 1
  %16 = select i1 %15, i32 -2147483648, i32 0
  %17 = extractelement <2 x i32> %14, i64 1
  %18 = xor i32 %17, %16
  %19 = insertelement <2 x i32> %14, i32 %18, i64 1
  %20 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %21 = icmp eq i8 %20, 0
  %22 = tail call i1 @llvm.amdgcn.class.f64(double %2, i32 504)
  %23 = select i1 %22, <2 x i32> %19, <2 x i32> <i32 0, i32 2146959360>
  %24 = select i1 %21, <2 x i32> %23, <2 x i32> %19
  %25 = bitcast <2 x i32> %24 to double
  ret double %25
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal %0 @__ocmlpriv_trigred_f64(double %0) local_unnamed_addr #9 {
  %2 = fcmp olt double %0, 0x41D0000000000000
  br i1 %2, label %3, label %5

3:                                                ; preds = %1
  %4 = tail call %0 @__ocmlpriv_trigredsmall_f64(double %0) #17
  br label %7

5:                                                ; preds = %1
  %6 = tail call %0 @__ocmlpriv_trigredlarge_f64(double %0) #17
  br label %7

7:                                                ; preds = %5, %3
  %8 = phi %0 [ %4, %3 ], [ %6, %5 ]
  ret %0 %8
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal %1 @__ocmlpriv_sincosred2_f64(double %0, double %1) local_unnamed_addr #10 {
  %3 = fmul double %0, %0
  %4 = fmul double %3, 5.000000e-01
  %5 = fsub double 1.000000e+00, %4
  %6 = fsub double 1.000000e+00, %5
  %7 = fsub double %6, %4
  %8 = fmul double %3, %3
  %9 = tail call double @llvm.fma.f64(double %3, double 0xBDA907DB46CC5E42, double 0x3E21EEB69037AB78)
  %10 = tail call double @llvm.fma.f64(double %3, double %9, double 0xBE927E4FA17F65F6)
  %11 = tail call double @llvm.fma.f64(double %3, double %10, double 0x3EFA01A019F4EC90)
  %12 = tail call double @llvm.fma.f64(double %3, double %11, double 0xBF56C16C16C16967)
  %13 = tail call double @llvm.fma.f64(double %3, double %12, double 0x3FA5555555555555)
  %14 = fneg double %1
  %15 = tail call double @llvm.fma.f64(double %0, double %14, double %7)
  %16 = tail call double @llvm.fma.f64(double %8, double %13, double %15)
  %17 = fadd double %5, %16
  %18 = tail call double @llvm.fma.f64(double %3, double 0x3DE5E0B2F9A43BB8, double 0xBE5AE600B42FDFA7)
  %19 = tail call double @llvm.fma.f64(double %3, double %18, double 0x3EC71DE3796CDE01)
  %20 = tail call double @llvm.fma.f64(double %3, double %19, double 0xBF2A01A019E83E5C)
  %21 = tail call double @llvm.fma.f64(double %3, double %20, double 0x3F81111111110BB3)
  %22 = fneg double %3
  %23 = fmul double %22, %0
  %24 = fmul double %1, 5.000000e-01
  %25 = tail call double @llvm.fma.f64(double %23, double %21, double %24)
  %26 = tail call double @llvm.fma.f64(double %3, double %25, double %14)
  %27 = tail call double @llvm.fma.f64(double %23, double 0xBFC5555555555555, double %26)
  %28 = fsub double %0, %27
  %29 = insertvalue %1 undef, double %28, 0
  %30 = insertvalue %1 %29, double %17, 1
  ret %1 %30
}

; Function Attrs: nounwind readnone speculatable willreturn
declare i1 @llvm.amdgcn.class.f64(double, i32) #8

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal %0 @__ocmlpriv_trigredsmall_f64(double %0) local_unnamed_addr #10 {
  %2 = fmul double %0, 0x3FE45F306DC9C883
  %3 = tail call double @llvm.rint.f64(double %2)
  %4 = tail call double @llvm.fma.f64(double %3, double 0xBFF921FB54442D18, double %0)
  %5 = tail call double @llvm.fma.f64(double %3, double 0xBC91A62633145C00, double %4)
  %6 = fmul double %3, 0x3C91A62633145C00
  %7 = fneg double %6
  %8 = tail call double @llvm.fma.f64(double %3, double 0x3C91A62633145C00, double %7)
  %9 = fsub double %4, %6
  %10 = fsub double %4, %9
  %11 = fsub double %10, %6
  %12 = fsub double %9, %5
  %13 = fadd double %12, %11
  %14 = fsub double %13, %8
  %15 = tail call double @llvm.fma.f64(double %3, double 0xB97B839A252049C0, double %14)
  %16 = fadd double %5, %15
  %17 = fsub double %16, %5
  %18 = fsub double %15, %17
  %19 = fptosi double %3 to i32
  %20 = and i32 %19, 3
  %21 = insertvalue %0 undef, double %18, 0
  %22 = insertvalue %0 %21, double %16, 1
  %23 = insertvalue %0 %22, i32 %20, 2
  ret %0 %23
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal %0 @__ocmlpriv_trigredlarge_f64(double %0) local_unnamed_addr #10 {
  %2 = tail call double @llvm.amdgcn.trig.preop.f64(double %0, i32 0)
  %3 = tail call double @llvm.amdgcn.trig.preop.f64(double %0, i32 1)
  %4 = tail call double @llvm.amdgcn.trig.preop.f64(double %0, i32 2)
  %5 = fcmp oge double %0, 0x7B00000000000000
  %6 = tail call double @llvm.amdgcn.ldexp.f64(double %0, i32 -128)
  %7 = select i1 %5, double %6, double %0
  %8 = fmul double %4, %7
  %9 = fneg double %8
  %10 = tail call double @llvm.fma.f64(double %4, double %7, double %9)
  %11 = fmul double %3, %7
  %12 = fneg double %11
  %13 = tail call double @llvm.fma.f64(double %3, double %7, double %12)
  %14 = fmul double %2, %7
  %15 = fneg double %14
  %16 = tail call double @llvm.fma.f64(double %2, double %7, double %15)
  %17 = fadd double %11, %16
  %18 = fsub double %17, %11
  %19 = fsub double %17, %18
  %20 = fsub double %16, %18
  %21 = fsub double %11, %19
  %22 = fadd double %20, %21
  %23 = fadd double %8, %13
  %24 = fsub double %23, %8
  %25 = fsub double %23, %24
  %26 = fsub double %13, %24
  %27 = fsub double %8, %25
  %28 = fadd double %26, %27
  %29 = fadd double %23, %22
  %30 = fsub double %29, %23
  %31 = fsub double %29, %30
  %32 = fsub double %22, %30
  %33 = fsub double %23, %31
  %34 = fadd double %32, %33
  %35 = fadd double %28, %34
  %36 = fadd double %10, %35
  %37 = fadd double %14, %17
  %38 = fsub double %37, %14
  %39 = fsub double %17, %38
  %40 = fadd double %39, %29
  %41 = fsub double %40, %39
  %42 = fsub double %29, %41
  %43 = fadd double %42, %36
  %44 = tail call double @llvm.amdgcn.ldexp.f64(double %37, i32 -2)
  %45 = tail call double @llvm.amdgcn.fract.f64(double %44)
  %46 = tail call i1 @llvm.amdgcn.class.f64(double %44, i32 516)
  %47 = select i1 %46, double 0.000000e+00, double %45
  %48 = tail call double @llvm.amdgcn.ldexp.f64(double %47, i32 2)
  %49 = fadd double %48, %40
  %50 = fcmp olt double %49, 0.000000e+00
  %51 = select i1 %50, double 4.000000e+00, double 0.000000e+00
  %52 = fadd double %48, %51
  %53 = fadd double %40, %52
  %54 = fptosi double %53 to i32
  %55 = sitofp i32 %54 to double
  %56 = fsub double %52, %55
  %57 = fadd double %40, %56
  %58 = fsub double %57, %56
  %59 = fsub double %40, %58
  %60 = fadd double %43, %59
  %61 = fcmp oge double %57, 5.000000e-01
  %62 = zext i1 %61 to i32
  %63 = add nsw i32 %62, %54
  %64 = select i1 %61, double 1.000000e+00, double 0.000000e+00
  %65 = fsub double %57, %64
  %66 = fadd double %65, %60
  %67 = fsub double %66, %65
  %68 = fsub double %60, %67
  %69 = fmul double %66, 0x3FF921FB54442D18
  %70 = fneg double %69
  %71 = tail call double @llvm.fma.f64(double %66, double 0x3FF921FB54442D18, double %70)
  %72 = tail call double @llvm.fma.f64(double %66, double 0x3C91A62633145C07, double %71)
  %73 = tail call double @llvm.fma.f64(double %68, double 0x3FF921FB54442D18, double %72)
  %74 = fadd double %69, %73
  %75 = fsub double %74, %69
  %76 = fsub double %73, %75
  %77 = and i32 %63, 3
  %78 = insertvalue %0 undef, double %76, 0
  %79 = insertvalue %0 %78, double %74, 1
  %80 = insertvalue %0 %79, i32 %77, 2
  ret %0 %80
}

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.trig.preop.f64(double, i32) #8

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.fract.f64(double) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_cos_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = tail call [2 x i32] @__ocmlpriv_trigred_f32(float %2) #17
  %4 = extractvalue [2 x i32] %3, 0
  %5 = bitcast i32 %4 to float
  %6 = extractvalue [2 x i32] %3, 1
  %7 = tail call [2 x i32] @__ocmlpriv_sincosred_f32(float %5) #17
  %8 = extractvalue [2 x i32] %7, 0
  %9 = bitcast i32 %8 to float
  %10 = extractvalue [2 x i32] %7, 1
  %11 = fneg float %9
  %12 = and i32 %6, 1
  %13 = icmp eq i32 %12, 0
  %14 = bitcast float %11 to i32
  %15 = select i1 %13, i32 %10, i32 %14
  %16 = icmp sgt i32 %6, 1
  %17 = select i1 %16, i32 -2147483648, i32 0
  %18 = xor i32 %15, %17
  %19 = bitcast i32 %18 to float
  %20 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %21 = icmp eq i8 %20, 0
  %22 = tail call i1 @llvm.amdgcn.class.f32(float %2, i32 504)
  %23 = select i1 %22, float %19, float 0x7FF8000000000000
  %24 = select i1 %21, float %23, float %19
  ret float %24
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal [2 x i32] @__ocmlpriv_trigred_f32(float %0) local_unnamed_addr #9 {
  %2 = fcmp olt float %0, 1.310720e+05
  br i1 %2, label %3, label %5

3:                                                ; preds = %1
  %4 = tail call [2 x i32] @__ocmlpriv_trigredsmall_f32(float %0) #17
  br label %7

5:                                                ; preds = %1
  %6 = tail call [2 x i32] @__ocmlpriv_trigredlarge_f32(float %0) #17
  br label %7

7:                                                ; preds = %5, %3
  %8 = phi [2 x i32] [ %4, %3 ], [ %6, %5 ]
  ret [2 x i32] %8
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal [2 x i32] @__ocmlpriv_sincosred_f32(float %0) local_unnamed_addr #9 {
  %2 = fmul float %0, %0
  %3 = tail call float @__ocml_fmuladd_f32(float %2, float 0xBF29833040000000, float 0x3F81103880000000) #17
  %4 = tail call float @__ocml_fmuladd_f32(float %2, float %3, float 0xBFC55553A0000000) #17
  %5 = fmul float %2, %4
  %6 = tail call float @__ocml_fmuladd_f32(float %0, float %5, float %0) #17
  %7 = tail call float @__ocml_fmuladd_f32(float %2, float 0x3EFAEA6680000000, float 0xBF56C9E760000000) #17
  %8 = tail call float @__ocml_fmuladd_f32(float %2, float %7, float 0x3FA5557EE0000000) #17
  %9 = tail call float @__ocml_fmuladd_f32(float %2, float %8, float 0xBFE0000080000000) #17
  %10 = tail call float @__ocml_fmuladd_f32(float %2, float %9, float 1.000000e+00) #17
  %11 = bitcast float %6 to i32
  %12 = insertvalue [2 x i32] undef, i32 %11, 0
  %13 = bitcast float %10 to i32
  %14 = insertvalue [2 x i32] %12, i32 %13, 1
  ret [2 x i32] %14
}

; Function Attrs: nounwind readnone speculatable willreturn
declare i1 @llvm.amdgcn.class.f32(float, i32) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal [2 x i32] @__ocmlpriv_trigredsmall_f32(float %0) local_unnamed_addr #9 {
  %2 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %3 = freeze i32 %2
  %4 = icmp sgt i32 %3, 8999
  br i1 %4, label %6, label %5

5:                                                ; preds = %1
  switch i32 %3, label %12 [
    i32 8001, label %6
    i32 7001, label %6
  ]

6:                                                ; preds = %5, %5, %1
  %7 = fmul float %0, 0x3FE45F3060000000
  %8 = tail call float @llvm.rint.f32(float %7) #21
  %9 = tail call float @llvm.fma.f32(float %8, float 0xBFF921FB40000000, float %0) #21
  %10 = tail call float @llvm.fma.f32(float %8, float 0xBE74442D00000000, float %9) #21
  %11 = tail call float @llvm.fma.f32(float %8, float 0xBCF8469880000000, float %10) #21
  br label %42

12:                                               ; preds = %5
  %13 = fmul float %0, 0x3FE45F3060000000
  %14 = tail call float @llvm.rint.f32(float %13) #21
  %15 = bitcast float %14 to i32
  %16 = and i32 %15, -4096
  %17 = bitcast i32 %16 to float
  %18 = fsub float %14, %17
  %19 = fmul float %14, 0x3FF921FB40000000
  %20 = fneg float %19
  %21 = tail call float @__ocml_fmuladd_f32(float %17, float 0x3FF9200000000000, float %20) #17
  %22 = tail call float @__ocml_fmuladd_f32(float %17, float 0x3F3FB40000000000, float %21) #17
  %23 = tail call float @__ocml_fmuladd_f32(float %18, float 0x3FF9200000000000, float %22) #17
  %24 = tail call float @__ocml_fmuladd_f32(float %18, float 0x3F3FB40000000000, float %23) #17
  %25 = fsub float %0, %19
  %26 = fsub float %0, %25
  %27 = fsub float %26, %19
  %28 = fsub float %27, %24
  %29 = fadd float %25, %28
  %30 = fmul float %14, 0x3E74442D00000000
  %31 = fneg float %30
  %32 = tail call float @__ocml_fmuladd_f32(float %17, float 0x3E74440000000000, float %31) #17
  %33 = tail call float @__ocml_fmuladd_f32(float %17, float 0x3D86800000000000, float %32) #17
  %34 = tail call float @__ocml_fmuladd_f32(float %18, float 0x3E74440000000000, float %33) #17
  %35 = tail call float @__ocml_fmuladd_f32(float %18, float 0x3D86800000000000, float %34) #17
  %36 = fsub float %29, %30
  %37 = fsub float %29, %36
  %38 = fsub float %37, %30
  %39 = fsub float %38, %35
  %40 = fadd float %36, %39
  %41 = tail call float @__ocml_fmuladd_f32(float 0xBCF8469880000000, float %14, float %40) #17
  br label %42

42:                                               ; preds = %12, %6
  %43 = phi float [ %11, %6 ], [ %41, %12 ]
  %44 = phi float [ %8, %6 ], [ %14, %12 ]
  %45 = fptosi float %44 to i32
  %46 = and i32 %45, 3
  %47 = bitcast float %43 to i32
  %48 = insertvalue [2 x i32] undef, i32 %47, 0
  %49 = insertvalue [2 x i32] %48, i32 %46, 1
  ret [2 x i32] %49
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal [2 x i32] @__ocmlpriv_trigredlarge_f32(float %0) local_unnamed_addr #9 {
  %2 = bitcast float %0 to i32
  %3 = lshr i32 %2, 23
  %4 = and i32 %2, 8388607
  %5 = or i32 %4, 8388608
  %6 = zext i32 %5 to i64
  %7 = mul nuw nsw i64 %6, 4266746795
  %8 = trunc i64 %7 to i32
  %9 = lshr i64 %7, 32
  %10 = mul nuw nsw i64 %6, 1011060801
  %11 = add nuw nsw i64 %9, %10
  %12 = trunc i64 %11 to i32
  %13 = lshr i64 %11, 32
  %14 = mul nuw nsw i64 %6, 3680671129
  %15 = add nuw nsw i64 %13, %14
  %16 = trunc i64 %15 to i32
  %17 = lshr i64 %15, 32
  %18 = mul nuw nsw i64 %6, 4113882560
  %19 = add nuw nsw i64 %17, %18
  %20 = trunc i64 %19 to i32
  %21 = lshr i64 %19, 32
  %22 = mul nuw nsw i64 %6, 4230436817
  %23 = add nuw nsw i64 %21, %22
  %24 = trunc i64 %23 to i32
  %25 = lshr i64 %23, 32
  %26 = mul nuw nsw i64 %6, 1313084713
  %27 = add nuw nsw i64 %25, %26
  %28 = trunc i64 %27 to i32
  %29 = lshr i64 %27, 32
  %30 = mul nuw nsw i64 %6, 2734261102
  %31 = add nuw nsw i64 %29, %30
  %32 = trunc i64 %31 to i32
  %33 = lshr i64 %31, 32
  %34 = trunc i64 %33 to i32
  %35 = add nsw i32 %3, -120
  %36 = icmp ugt i32 %35, 63
  %37 = select i1 %36, i32 %28, i32 %34
  %38 = select i1 %36, i32 %24, i32 %32
  %39 = select i1 %36, i32 %20, i32 %28
  %40 = select i1 %36, i32 %16, i32 %24
  %41 = select i1 %36, i32 %12, i32 %20
  %42 = select i1 %36, i32 %8, i32 %16
  %43 = select i1 %36, i32 -64, i32 0
  %44 = add nsw i32 %43, %35
  %45 = icmp ugt i32 %44, 31
  %46 = select i1 %45, i32 %38, i32 %37
  %47 = select i1 %45, i32 %39, i32 %38
  %48 = select i1 %45, i32 %40, i32 %39
  %49 = select i1 %45, i32 %41, i32 %40
  %50 = select i1 %45, i32 %42, i32 %41
  %51 = select i1 %45, i32 -32, i32 0
  %52 = add nsw i32 %51, %44
  %53 = icmp ugt i32 %52, 31
  %54 = select i1 %53, i32 %47, i32 %46
  %55 = select i1 %53, i32 %48, i32 %47
  %56 = select i1 %53, i32 %49, i32 %48
  %57 = select i1 %53, i32 %50, i32 %49
  %58 = select i1 %53, i32 -32, i32 0
  %59 = add nsw i32 %58, %52
  %60 = icmp eq i32 %59, 0
  %61 = sub nsw i32 32, %59
  %62 = tail call i32 @llvm.fshr.i32(i32 %54, i32 %55, i32 %61)
  %63 = tail call i32 @llvm.fshr.i32(i32 %55, i32 %56, i32 %61)
  %64 = tail call i32 @llvm.fshr.i32(i32 %56, i32 %57, i32 %61)
  %65 = select i1 %60, i32 %54, i32 %62
  %66 = select i1 %60, i32 %55, i32 %63
  %67 = select i1 %60, i32 %56, i32 %64
  %68 = lshr i32 %65, 29
  %69 = tail call i32 @llvm.fshl.i32(i32 %65, i32 %66, i32 2)
  %70 = tail call i32 @llvm.fshl.i32(i32 %66, i32 %67, i32 2)
  %71 = tail call i32 @llvm.fshl.i32(i32 %67, i32 %57, i32 2)
  %72 = and i32 %68, 1
  %73 = sub nsw i32 0, %72
  %74 = shl i32 %68, 31
  %75 = xor i32 %69, %73
  %76 = xor i32 %70, %73
  %77 = xor i32 %71, %73
  %78 = tail call i32 @llvm.ctlz.i32(i32 %75, i1 false), !range !83
  %79 = sub nsw i32 31, %78
  %80 = tail call i32 @llvm.fshr.i32(i32 %75, i32 %76, i32 %79)
  %81 = tail call i32 @llvm.fshr.i32(i32 %76, i32 %77, i32 %79)
  %82 = mul nsw i32 %78, -8388608
  %83 = add nsw i32 %82, 1056964608
  %84 = or i32 %83, %74
  %85 = lshr i32 %80, 9
  %86 = or i32 %84, %85
  %87 = bitcast i32 %86 to float
  %88 = tail call i32 @llvm.fshl.i32(i32 %80, i32 %81, i32 23)
  %89 = tail call i32 @llvm.ctlz.i32(i32 %88, i1 false), !range !83
  %90 = sub nsw i32 31, %89
  %91 = tail call i32 @llvm.fshr.i32(i32 %88, i32 %81, i32 %90)
  %92 = add nuw nsw i32 %89, %78
  %93 = mul nsw i32 %92, -8388608
  %94 = add nsw i32 %93, 855638016
  %95 = or i32 %94, %74
  %96 = lshr i32 %91, 9
  %97 = or i32 %95, %96
  %98 = bitcast i32 %97 to float
  %99 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %100 = freeze i32 %99
  %101 = icmp sgt i32 %100, 8999
  br i1 %101, label %103, label %102

102:                                              ; preds = %1
  switch i32 %100, label %109 [
    i32 8001, label %103
    i32 7001, label %103
  ]

103:                                              ; preds = %102, %102, %1
  %104 = fmul float %87, 0x3FF921FB40000000
  %105 = fneg float %104
  %106 = tail call float @llvm.fma.f32(float %87, float 0x3FF921FB40000000, float %105)
  %107 = tail call float @llvm.fma.f32(float %87, float 0x3E74442D00000000, float %106)
  %108 = tail call float @llvm.fma.f32(float %98, float 0x3FF921FB40000000, float %107)
  br label %122

109:                                              ; preds = %102
  %110 = and i32 %86, -4096
  %111 = bitcast i32 %110 to float
  %112 = fsub float %87, %111
  %113 = fmul float %87, 0x3FF921FB40000000
  %114 = fneg float %113
  %115 = tail call float @__ocml_fmuladd_f32(float %111, float 0x3FF9200000000000, float %114) #17
  %116 = tail call float @__ocml_fmuladd_f32(float %111, float 0x3F3FB40000000000, float %115) #17
  %117 = tail call float @__ocml_fmuladd_f32(float %112, float 0x3FF9200000000000, float %116) #17
  %118 = tail call float @__ocml_fmuladd_f32(float %112, float 0x3F3FB40000000000, float %117) #17
  %119 = fmul float %87, 0x3E74442D00000000
  %120 = tail call float @__ocml_fmuladd_f32(float %98, float 0x3FF921FB40000000, float %119) #17
  %121 = fadd float %118, %120
  br label %122

122:                                              ; preds = %109, %103
  %123 = phi float [ %108, %103 ], [ %121, %109 ]
  %124 = phi float [ %104, %103 ], [ %113, %109 ]
  %125 = fadd float %123, %124
  %126 = lshr i32 %65, 30
  %127 = add nuw nsw i32 %72, %126
  %128 = and i32 %127, 3
  %129 = bitcast float %125 to i32
  %130 = insertvalue [2 x i32] undef, i32 %129, 0
  %131 = insertvalue [2 x i32] %130, i32 %128, 1
  ret [2 x i32] %131
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.fshr.i32(i32, i32, i32) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.fshl.i32(i32, i32, i32) #7

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.ctlz.i32(i32, i1 immarg) #7

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocml_log_f64(double %0) #6 {
  %2 = tail call double @llvm.amdgcn.frexp.mant.f64(double %0)
  %3 = fcmp olt double %2, 0x3FE5555555555555
  %4 = zext i1 %3 to i32
  %5 = tail call double @llvm.amdgcn.ldexp.f64(double %2, i32 %4)
  %6 = tail call i32 @llvm.amdgcn.frexp.exp.i32.f64(double %0)
  %7 = sub nsw i32 %6, %4
  %8 = fadd double %5, -1.000000e+00
  %9 = fadd double %5, 1.000000e+00
  %10 = fadd double %9, -1.000000e+00
  %11 = fsub double %5, %10
  %12 = tail call double @llvm.amdgcn.rcp.f64(double %9) #21
  %13 = fneg double %9
  %14 = tail call double @llvm.fma.f64(double %13, double %12, double 1.000000e+00) #21
  %15 = tail call double @llvm.fma.f64(double %14, double %12, double %12) #21
  %16 = tail call double @llvm.fma.f64(double %13, double %15, double 1.000000e+00) #21
  %17 = tail call double @llvm.fma.f64(double %16, double %15, double %15) #21
  %18 = fmul double %8, %17
  %19 = fmul double %9, %18
  %20 = fneg double %19
  %21 = tail call double @llvm.fma.f64(double %18, double %9, double %20) #21
  %22 = tail call double @llvm.fma.f64(double %18, double %11, double %21) #21
  %23 = fadd double %19, %22
  %24 = fsub double %23, %19
  %25 = fsub double %22, %24
  %26 = fsub double %8, %23
  %27 = fsub double %8, %26
  %28 = fsub double %27, %23
  %29 = fsub double %28, %25
  %30 = fadd double %26, %29
  %31 = fmul double %17, %30
  %32 = fadd double %18, %31
  %33 = fsub double %32, %18
  %34 = fsub double %31, %33
  %35 = fmul double %32, %32
  %36 = tail call double @llvm.fma.f64(double %35, double 0x3FC3AB76BF559E2B, double 0x3FC385386B47B09A)
  %37 = tail call double @llvm.fma.f64(double %35, double %36, double 0x3FC7474DD7F4DF2E)
  %38 = tail call double @llvm.fma.f64(double %35, double %37, double 0x3FCC71C016291751)
  %39 = tail call double @llvm.fma.f64(double %35, double %38, double 0x3FD249249B27ACF1)
  %40 = tail call double @llvm.fma.f64(double %35, double %39, double 0x3FD99999998EF7B6)
  %41 = tail call double @llvm.fma.f64(double %35, double %40, double 0x3FE5555555555780)
  %42 = tail call double @llvm.amdgcn.ldexp.f64(double %32, i32 1) #21
  %43 = tail call double @llvm.amdgcn.ldexp.f64(double %34, i32 1) #21
  %44 = fmul double %32, %35
  %45 = fmul double %44, %41
  %46 = fadd double %42, %45
  %47 = fsub double %46, %42
  %48 = fsub double %45, %47
  %49 = fadd double %43, %48
  %50 = fadd double %46, %49
  %51 = fsub double %50, %46
  %52 = fsub double %49, %51
  %53 = sitofp i32 %7 to double
  %54 = fmul double %53, 0x3FE62E42FEFA39EF
  %55 = fneg double %54
  %56 = tail call double @llvm.fma.f64(double %53, double 0x3FE62E42FEFA39EF, double %55) #21
  %57 = tail call double @llvm.fma.f64(double %53, double 0x3C7ABC9E3B39803F, double %56) #21
  %58 = fadd double %54, %57
  %59 = fsub double %58, %54
  %60 = fsub double %57, %59
  %61 = fadd double %58, %50
  %62 = fsub double %61, %58
  %63 = fsub double %61, %62
  %64 = fsub double %58, %63
  %65 = fsub double %50, %62
  %66 = fadd double %65, %64
  %67 = fadd double %60, %52
  %68 = fsub double %67, %60
  %69 = fsub double %67, %68
  %70 = fsub double %60, %69
  %71 = fsub double %52, %68
  %72 = fadd double %71, %70
  %73 = fadd double %67, %66
  %74 = fadd double %61, %73
  %75 = fsub double %74, %61
  %76 = fsub double %73, %75
  %77 = fadd double %72, %76
  %78 = fadd double %74, %77
  %79 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %80 = icmp eq i8 %79, 0
  %81 = tail call double @llvm.fabs.f64(double %0) #26
  %82 = fcmp oeq double %81, 0x7FF0000000000000
  %83 = select i1 %82, double %0, double %78
  %84 = fcmp olt double %0, 0.000000e+00
  %85 = select i1 %84, double 0x7FF8000000000000, double %83
  %86 = fcmp oeq double %0, 0.000000e+00
  %87 = select i1 %86, double 0xFFF0000000000000, double %85
  %88 = select i1 %80, double %87, double %78
  ret double %88
}

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.amdgcn.frexp.mant.f64(double) #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.frexp.exp.i32.f64(double) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_log_f32(float %0) #9 {
  %2 = load i8, i8 addrspace(4)* @__oclc_daz_opt, align 1, !tbaa !75, !range !79
  %3 = icmp eq i8 %2, 0
  %4 = load i8, i8 addrspace(4)* @__oclc_unsafe_math_opt, align 1, !tbaa !75, !range !79
  %5 = icmp eq i8 %4, 0
  br i1 %3, label %34, label %6

6:                                                ; preds = %1
  %7 = tail call float @llvm.log2.f32(float %0)
  br i1 %5, label %10, label %8

8:                                                ; preds = %6
  %9 = fmul float %7, 0x3FE62E4300000000
  br label %68

10:                                               ; preds = %6
  %11 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %12 = freeze i32 %11
  %13 = icmp sgt i32 %12, 8999
  br i1 %13, label %15, label %14

14:                                               ; preds = %10
  switch i32 %12, label %21 [
    i32 8001, label %15
    i32 7001, label %15
  ]

15:                                               ; preds = %14, %14, %10
  %16 = fmul float %7, 0x3FE62E42E0000000
  %17 = fneg float %16
  %18 = tail call float @llvm.fma.f32(float %7, float 0x3FE62E42E0000000, float %17)
  %19 = tail call float @llvm.fma.f32(float %7, float 0x3E6EFA39E0000000, float %18)
  %20 = fadd float %16, %19
  br label %30

21:                                               ; preds = %14
  %22 = bitcast float %7 to i32
  %23 = and i32 %22, -4096
  %24 = bitcast i32 %23 to float
  %25 = fsub float %7, %24
  %26 = fmul float %25, 0x3F00BFBE80000000
  %27 = tail call float @__ocml_fmuladd_f32(float %24, float 0x3F00BFBE80000000, float %26) #17
  %28 = tail call float @__ocml_fmuladd_f32(float %25, float 0x3FE62E0000000000, float %27) #17
  %29 = tail call float @__ocml_fmuladd_f32(float %24, float 0x3FE62E0000000000, float %28) #17
  br label %30

30:                                               ; preds = %21, %15
  %31 = phi float [ %20, %15 ], [ %29, %21 ]
  %32 = tail call i1 @llvm.amdgcn.class.f32(float %7, i32 519)
  %33 = select i1 %32, float %7, float %31
  br label %68

34:                                               ; preds = %1
  %35 = tail call i1 @llvm.amdgcn.class.f32(float %0, i32 144)
  %36 = select i1 %35, float 0x41F0000000000000, float 1.000000e+00
  %37 = fmul float %36, %0
  %38 = tail call float @llvm.log2.f32(float %37)
  br i1 %5, label %42, label %39

39:                                               ; preds = %34
  %40 = select i1 %35, float 0xC0362E4300000000, float 0.000000e+00
  %41 = tail call float @__ocml_fmuladd_f32(float %38, float 0x3FE62E4300000000, float %40) #17
  br label %68

42:                                               ; preds = %34
  %43 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %44 = freeze i32 %43
  %45 = icmp sgt i32 %44, 8999
  br i1 %45, label %47, label %46

46:                                               ; preds = %42
  switch i32 %44, label %53 [
    i32 8001, label %47
    i32 7001, label %47
  ]

47:                                               ; preds = %46, %46, %42
  %48 = fmul float %38, 0x3FE62E42E0000000
  %49 = fneg float %48
  %50 = tail call float @llvm.fma.f32(float %38, float 0x3FE62E42E0000000, float %49)
  %51 = tail call float @llvm.fma.f32(float %38, float 0x3E6EFA39E0000000, float %50)
  %52 = fadd float %48, %51
  br label %62

53:                                               ; preds = %46
  %54 = bitcast float %38 to i32
  %55 = and i32 %54, -4096
  %56 = bitcast i32 %55 to float
  %57 = fsub float %38, %56
  %58 = fmul float %57, 0x3F00BFBE80000000
  %59 = tail call float @__ocml_fmuladd_f32(float %56, float 0x3F00BFBE80000000, float %58) #17
  %60 = tail call float @__ocml_fmuladd_f32(float %57, float 0x3FE62E0000000000, float %59) #17
  %61 = tail call float @__ocml_fmuladd_f32(float %56, float 0x3FE62E0000000000, float %60) #17
  br label %62

62:                                               ; preds = %53, %47
  %63 = phi float [ %52, %47 ], [ %61, %53 ]
  %64 = tail call i1 @llvm.amdgcn.class.f32(float %38, i32 519)
  %65 = select i1 %64, float %38, float %63
  %66 = select i1 %35, float 0x40362E4300000000, float 0.000000e+00
  %67 = fsub float %65, %66
  br label %68

68:                                               ; preds = %62, %39, %30, %8
  %69 = phi float [ %9, %8 ], [ %33, %30 ], [ %41, %39 ], [ %67, %62 ]
  ret float %69
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.log2.f32(float) #7

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocml_fabs_f64(double %0) #10 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  ret double %2
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal float @__ocml_fabs_f32(float %0) #10 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  ret float %2
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_pow_f64(double %0, double %1) #11 {
  %3 = tail call double @llvm.fabs.f64(double %0)
  %4 = tail call <2 x double> @__ocmlpriv_epln_f64(double %3) #17
  %5 = extractelement <2 x double> %4, i64 1
  %6 = fmul double %5, %1
  %7 = fneg double %6
  %8 = tail call double @llvm.fma.f64(double %1, double %5, double %7) #21
  %9 = extractelement <2 x double> %4, i64 0
  %10 = tail call double @llvm.fma.f64(double %1, double %9, double %8) #21
  %11 = fadd double %6, %10
  %12 = fsub double %11, %6
  %13 = fsub double %10, %12
  %14 = tail call double @llvm.fabs.f64(double %6) #27
  %15 = fcmp oeq double %14, 0x7FF0000000000000
  %16 = select i1 %15, double %6, double %11
  %17 = tail call double @llvm.fabs.f64(double %16) #27
  %18 = fcmp oeq double %17, 0x7FF0000000000000
  %19 = select i1 %18, double 0.000000e+00, double %13
  %20 = insertelement <2 x double> undef, double %19, i64 0
  %21 = insertelement <2 x double> %20, double %16, i64 1
  %22 = tail call double @__ocmlpriv_expep_f64(<2 x double> %21) #17
  %23 = tail call double @llvm.fabs.f64(double %1)
  %24 = tail call double @llvm.trunc.f64(double %23)
  %25 = fcmp oeq double %23, %24
  %26 = zext i1 %25 to i32
  %27 = fmul double %24, 5.000000e-01
  %28 = tail call double @llvm.amdgcn.fract.f64(double %27)
  %29 = tail call i1 @llvm.amdgcn.class.f64(double %27, i32 516)
  %30 = select i1 %29, double 0.000000e+00, double %28
  %31 = fcmp oeq double %30, 0.000000e+00
  %32 = and i1 %25, %31
  %33 = zext i1 %32 to i32
  %34 = add nuw nsw i32 %33, %26
  %35 = icmp eq i32 %34, 1
  %36 = fcmp olt double %0, 0.000000e+00
  %37 = and i1 %36, %35
  %38 = select i1 %37, double -0.000000e+00, double 0.000000e+00
  %39 = tail call double @llvm.copysign.f64(double %22, double %38)
  %40 = fcmp uge double %0, 0.000000e+00
  %41 = icmp ne i32 %34, 0
  %42 = select i1 %40, i1 true, i1 %41
  %43 = select i1 %42, double %39, double 0x7FF8000000000000
  %44 = fcmp oeq double %23, 0x7FF0000000000000
  br i1 %44, label %45, label %56

45:                                               ; preds = %2
  %46 = fcmp oeq double %3, 1.000000e+00
  br i1 %46, label %56, label %47

47:                                               ; preds = %45
  %48 = fadd double %3, -1.000000e+00
  %49 = bitcast double %48 to <2 x i32>
  %50 = extractelement <2 x i32> %49, i64 1
  %51 = bitcast double %1 to <2 x i32>
  %52 = extractelement <2 x i32> %51, i64 1
  %53 = xor i32 %50, %52
  %54 = icmp sgt i32 %53, -1
  %55 = select i1 %54, double 0x7FF0000000000000, double 0.000000e+00
  br label %56

56:                                               ; preds = %47, %45, %2
  %57 = phi double [ %43, %2 ], [ %55, %47 ], [ 1.000000e+00, %45 ]
  %58 = fcmp oeq double %3, 0x7FF0000000000000
  %59 = fcmp oeq double %0, 0.000000e+00
  %60 = select i1 %58, i1 true, i1 %59
  %61 = fcmp olt double %1, 0.000000e+00
  %62 = xor i1 %59, %61
  %63 = select i1 %62, double 0.000000e+00, double 0x7FF0000000000000
  %64 = select i1 %35, double %0, double 0.000000e+00
  %65 = tail call double @llvm.copysign.f64(double %63, double %64)
  %66 = select i1 %60, double %65, double %57
  %67 = fcmp uno double %0, 0.000000e+00
  %68 = fcmp uno double %1, 0.000000e+00
  %69 = select i1 %67, i1 true, i1 %68
  %70 = select i1 %69, double 0x7FF8000000000000, double %66
  %71 = fcmp oeq double %0, 1.000000e+00
  %72 = fcmp oeq double %1, 0.000000e+00
  %73 = select i1 %71, i1 true, i1 %72
  %74 = select i1 %73, double 1.000000e+00, double %70
  ret double %74
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal <2 x double> @__ocmlpriv_epln_f64(double %0) local_unnamed_addr #6 {
  %2 = tail call double @llvm.amdgcn.frexp.mant.f64(double %0)
  %3 = fcmp olt double %2, 0x3FE5555555555555
  %4 = zext i1 %3 to i32
  %5 = tail call double @llvm.amdgcn.ldexp.f64(double %2, i32 %4)
  %6 = tail call i32 @llvm.amdgcn.frexp.exp.i32.f64(double %0)
  %7 = sub nsw i32 %6, %4
  %8 = fadd double %5, -1.000000e+00
  %9 = fadd double %5, 1.000000e+00
  %10 = fadd double %9, -1.000000e+00
  %11 = fsub double %5, %10
  %12 = tail call double @llvm.amdgcn.rcp.f64(double %9) #21
  %13 = fneg double %9
  %14 = tail call double @llvm.fma.f64(double %13, double %12, double 1.000000e+00) #21
  %15 = tail call double @llvm.fma.f64(double %14, double %12, double %12) #21
  %16 = tail call double @llvm.fma.f64(double %13, double %15, double 1.000000e+00) #21
  %17 = tail call double @llvm.fma.f64(double %16, double %15, double %15) #21
  %18 = fmul double %8, %17
  %19 = fmul double %9, %18
  %20 = fneg double %19
  %21 = tail call double @llvm.fma.f64(double %18, double %9, double %20) #21
  %22 = tail call double @llvm.fma.f64(double %18, double %11, double %21) #21
  %23 = fadd double %19, %22
  %24 = fsub double %23, %19
  %25 = fsub double %22, %24
  %26 = fsub double %8, %23
  %27 = fsub double %8, %26
  %28 = fsub double %27, %23
  %29 = fsub double %28, %25
  %30 = fadd double %26, %29
  %31 = fmul double %17, %30
  %32 = fadd double %18, %31
  %33 = fsub double %32, %18
  %34 = fsub double %31, %33
  %35 = fmul double %32, %32
  %36 = fneg double %35
  %37 = tail call double @llvm.fma.f64(double %32, double %32, double %36) #21
  %38 = fmul double %34, 2.000000e+00
  %39 = tail call double @llvm.fma.f64(double %32, double %38, double %37) #21
  %40 = fadd double %35, %39
  %41 = fsub double %40, %35
  %42 = fsub double %39, %41
  %43 = tail call double @llvm.fma.f64(double %40, double 0x3FBDEE674222DE17, double 0x3FBA6564968915A9)
  %44 = tail call double @llvm.fma.f64(double %40, double %43, double 0x3FBE25E43ABE935A)
  %45 = tail call double @llvm.fma.f64(double %40, double %44, double 0x3FC110EF47E6C9C2)
  %46 = tail call double @llvm.fma.f64(double %40, double %45, double 0x3FC3B13BCFA74449)
  %47 = tail call double @llvm.fma.f64(double %40, double %46, double 0x3FC745D171BF3C30)
  %48 = tail call double @llvm.fma.f64(double %40, double %47, double 0x3FCC71C71C7792CE)
  %49 = tail call double @llvm.fma.f64(double %40, double %48, double 0x3FD24924924920DA)
  %50 = tail call double @llvm.fma.f64(double %40, double %49, double 0x3FD999999999999C)
  %51 = sitofp i32 %7 to double
  %52 = fmul double %51, 0x3FE62E42FEFA39EF
  %53 = fneg double %52
  %54 = tail call double @llvm.fma.f64(double %51, double 0x3FE62E42FEFA39EF, double %53) #21
  %55 = tail call double @llvm.fma.f64(double %51, double 0x3C7ABC9E3B39803F, double %54) #21
  %56 = fadd double %52, %55
  %57 = fsub double %56, %52
  %58 = fsub double %55, %57
  %59 = tail call double @llvm.amdgcn.ldexp.f64(double %32, i32 1) #21
  %60 = tail call double @llvm.amdgcn.ldexp.f64(double %34, i32 1) #21
  %61 = fmul double %32, %40
  %62 = fneg double %61
  %63 = tail call double @llvm.fma.f64(double %40, double %32, double %62) #21
  %64 = tail call double @llvm.fma.f64(double %40, double %34, double %63) #21
  %65 = tail call double @llvm.fma.f64(double %42, double %32, double %64) #21
  %66 = fadd double %61, %65
  %67 = fsub double %66, %61
  %68 = fsub double %65, %67
  %69 = fmul double %40, %50
  %70 = fneg double %69
  %71 = tail call double @llvm.fma.f64(double %40, double %50, double %70) #21
  %72 = tail call double @llvm.fma.f64(double %42, double %50, double %71) #21
  %73 = fadd double %69, %72
  %74 = fsub double %73, %69
  %75 = fsub double %72, %74
  %76 = fadd double %73, 0x3FE5555555555555
  %77 = fadd double %76, 0xBFE5555555555555
  %78 = fsub double %73, %77
  %79 = fadd double %75, 0x3C8543B0D5DF274D
  %80 = fadd double %79, %78
  %81 = fadd double %76, %80
  %82 = fsub double %81, %76
  %83 = fsub double %80, %82
  %84 = fmul double %66, %81
  %85 = fneg double %84
  %86 = tail call double @llvm.fma.f64(double %66, double %81, double %85) #21
  %87 = tail call double @llvm.fma.f64(double %66, double %83, double %86) #21
  %88 = tail call double @llvm.fma.f64(double %68, double %81, double %87) #21
  %89 = fadd double %84, %88
  %90 = fsub double %89, %84
  %91 = fsub double %88, %90
  %92 = fadd double %59, %89
  %93 = fsub double %92, %59
  %94 = fsub double %89, %93
  %95 = fadd double %60, %91
  %96 = fadd double %95, %94
  %97 = fadd double %92, %96
  %98 = fsub double %97, %92
  %99 = fsub double %96, %98
  %100 = fadd double %56, %97
  %101 = fsub double %100, %56
  %102 = fsub double %100, %101
  %103 = fsub double %56, %102
  %104 = fsub double %97, %101
  %105 = fadd double %104, %103
  %106 = fadd double %58, %99
  %107 = fsub double %106, %58
  %108 = fsub double %106, %107
  %109 = fsub double %58, %108
  %110 = fsub double %99, %107
  %111 = fadd double %110, %109
  %112 = fadd double %106, %105
  %113 = fadd double %100, %112
  %114 = fsub double %113, %100
  %115 = fsub double %112, %114
  %116 = fadd double %111, %115
  %117 = fadd double %113, %116
  %118 = fsub double %117, %113
  %119 = fsub double %116, %118
  %120 = insertelement <2 x double> undef, double %119, i64 0
  %121 = insertelement <2 x double> %120, double %117, i64 1
  ret <2 x double> %121
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocmlpriv_expep_f64(<2 x double> %0) local_unnamed_addr #11 {
  %2 = extractelement <2 x double> %0, i64 1
  %3 = tail call double @__ocml_exp_f64(double %2) #17
  %4 = extractelement <2 x double> %0, i64 0
  %5 = tail call double @llvm.fma.f64(double %3, double %4, double %3)
  %6 = tail call double @llvm.fabs.f64(double %3) #26
  %7 = fcmp oeq double %6, 0x7FF0000000000000
  %8 = select i1 %7, double %3, double %5
  ret double %8
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare double @llvm.trunc.f64(double) #7

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_pow_f32(float %0, float %1) #12 {
  %3 = tail call float @llvm.fabs.f32(float %0)
  %4 = load i8, i8 addrspace(4)* @__oclc_unsafe_math_opt, align 1, !tbaa !75, !range !79
  %5 = icmp eq i8 %4, 0
  br i1 %5, label %27, label %6

6:                                                ; preds = %2
  %7 = load i8, i8 addrspace(4)* @__oclc_daz_opt, align 1, !tbaa !75, !range !79
  %8 = icmp eq i8 %7, 0
  br i1 %8, label %13, label %9

9:                                                ; preds = %6
  %10 = tail call float @llvm.log2.f32(float %3)
  %11 = fmul float %10, %1
  %12 = tail call float @llvm.exp2.f32(float %11)
  br label %75

13:                                               ; preds = %6
  %14 = fcmp olt float %3, 0x3810000000000000
  %15 = select i1 %14, float 0x4170000000000000, float 1.000000e+00
  %16 = fmul float %3, %15
  %17 = tail call float @llvm.log2.f32(float %16)
  %18 = select i1 %14, float 2.400000e+01, float 0.000000e+00
  %19 = fsub float %17, %18
  %20 = fmul float %19, %1
  %21 = fcmp olt float %20, -1.260000e+02
  %22 = select i1 %21, float 2.400000e+01, float 0.000000e+00
  %23 = fadd float %20, %22
  %24 = tail call float @llvm.exp2.f32(float %23)
  %25 = select i1 %21, float 0x3E70000000000000, float 1.000000e+00
  %26 = fmul float %25, %24
  br label %75

27:                                               ; preds = %2
  %28 = tail call <2 x float> @__ocmlpriv_epln_f32(float %3) #17
  %29 = extractelement <2 x float> %28, i64 1
  %30 = fmul float %29, %1
  %31 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %32 = freeze i32 %31
  %33 = icmp sgt i32 %32, 8999
  br i1 %33, label %35, label %34

34:                                               ; preds = %27
  switch i32 %32, label %38 [
    i32 8001, label %35
    i32 7001, label %35
  ]

35:                                               ; preds = %34, %34, %27
  %36 = fneg float %30
  %37 = tail call float @llvm.fma.f32(float %1, float %29, float %36) #21
  br label %52

38:                                               ; preds = %34
  %39 = bitcast float %1 to i32
  %40 = and i32 %39, -4096
  %41 = bitcast i32 %40 to float
  %42 = fsub float %1, %41
  %43 = bitcast float %29 to i32
  %44 = and i32 %43, -4096
  %45 = bitcast i32 %44 to float
  %46 = fsub float %29, %45
  %47 = fneg float %30
  %48 = tail call float @llvm.fmuladd.f32(float %41, float %45, float %47) #21
  %49 = tail call float @llvm.fmuladd.f32(float %41, float %46, float %48) #21
  %50 = tail call float @llvm.fmuladd.f32(float %42, float %45, float %49) #21
  %51 = tail call float @llvm.fmuladd.f32(float %42, float %46, float %50) #21
  br label %52

52:                                               ; preds = %38, %35
  %53 = phi float [ %51, %38 ], [ %37, %35 ]
  br i1 %33, label %55, label %54

54:                                               ; preds = %52
  switch i32 %32, label %58 [
    i32 8001, label %55
    i32 7001, label %55
  ]

55:                                               ; preds = %54, %54, %52
  %56 = extractelement <2 x float> %28, i64 0
  %57 = tail call float @llvm.fma.f32(float %1, float %56, float %53) #21
  br label %61

58:                                               ; preds = %54
  %59 = extractelement <2 x float> %28, i64 0
  %60 = tail call float @llvm.fmuladd.f32(float %1, float %59, float %53) #21
  br label %61

61:                                               ; preds = %58, %55
  %62 = phi float [ %60, %58 ], [ %57, %55 ]
  %63 = fadd float %30, %62
  %64 = fsub float %63, %30
  %65 = fsub float %62, %64
  %66 = tail call float @llvm.fabs.f32(float %30) #27
  %67 = fcmp oeq float %66, 0x7FF0000000000000
  %68 = select i1 %67, float %30, float %63
  %69 = tail call float @llvm.fabs.f32(float %68) #27
  %70 = fcmp oeq float %69, 0x7FF0000000000000
  %71 = select i1 %70, float 0.000000e+00, float %65
  %72 = insertelement <2 x float> undef, float %71, i64 0
  %73 = insertelement <2 x float> %72, float %68, i64 1
  %74 = tail call float @__ocmlpriv_expep_f32(<2 x float> %73) #17
  br label %75

75:                                               ; preds = %61, %13, %9
  %76 = phi float [ %12, %9 ], [ %26, %13 ], [ %74, %61 ]
  %77 = tail call float @llvm.fabs.f32(float %1)
  %78 = tail call float @llvm.trunc.f32(float %77)
  %79 = fcmp oeq float %77, %78
  %80 = zext i1 %79 to i32
  %81 = fmul float %78, 5.000000e-01
  %82 = tail call float @llvm.amdgcn.fract.f32(float %81)
  %83 = tail call i1 @llvm.amdgcn.class.f32(float %81, i32 516)
  %84 = select i1 %83, float 0.000000e+00, float %82
  %85 = fcmp oeq float %84, 0.000000e+00
  %86 = and i1 %79, %85
  %87 = zext i1 %86 to i32
  %88 = add nuw nsw i32 %87, %80
  %89 = icmp eq i32 %88, 1
  %90 = fcmp olt float %0, 0.000000e+00
  %91 = and i1 %90, %89
  %92 = select i1 %91, float -0.000000e+00, float 0.000000e+00
  %93 = tail call float @llvm.copysign.f32(float %76, float %92)
  %94 = fcmp uge float %0, 0.000000e+00
  %95 = icmp ne i32 %88, 0
  %96 = select i1 %94, i1 true, i1 %95
  %97 = select i1 %96, float %93, float 0x7FF8000000000000
  %98 = fcmp oeq float %77, 0x7FF0000000000000
  br i1 %98, label %99, label %108

99:                                               ; preds = %75
  %100 = fcmp oeq float %3, 1.000000e+00
  br i1 %100, label %108, label %101

101:                                              ; preds = %99
  %102 = fadd float %3, -1.000000e+00
  %103 = bitcast float %102 to i32
  %104 = bitcast float %1 to i32
  %105 = xor i32 %103, %104
  %106 = icmp sgt i32 %105, -1
  %107 = select i1 %106, float 0x7FF0000000000000, float 0.000000e+00
  br label %108

108:                                              ; preds = %101, %99, %75
  %109 = phi float [ %97, %75 ], [ %107, %101 ], [ 1.000000e+00, %99 ]
  %110 = fcmp oeq float %3, 0x7FF0000000000000
  %111 = fcmp oeq float %0, 0.000000e+00
  %112 = select i1 %110, i1 true, i1 %111
  %113 = fcmp olt float %1, 0.000000e+00
  %114 = xor i1 %111, %113
  %115 = select i1 %114, float 0.000000e+00, float 0x7FF0000000000000
  %116 = select i1 %89, float %0, float 0.000000e+00
  %117 = tail call float @llvm.copysign.f32(float %115, float %116)
  %118 = select i1 %112, float %117, float %109
  %119 = fcmp uno float %0, 0.000000e+00
  %120 = fcmp uno float %1, 0.000000e+00
  %121 = select i1 %119, i1 true, i1 %120
  %122 = select i1 %121, float 0x7FF8000000000000, float %118
  %123 = fcmp oeq float %0, 1.000000e+00
  %124 = fcmp oeq float %1, 0.000000e+00
  %125 = select i1 %123, i1 true, i1 %124
  %126 = select i1 %125, float 1.000000e+00, float %122
  ret float %126
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal <2 x float> @__ocmlpriv_epln_f32(float %0) local_unnamed_addr #12 {
  %2 = tail call float @llvm.amdgcn.frexp.mant.f32(float %0)
  %3 = fcmp olt float %2, 0x3FE5555560000000
  %4 = zext i1 %3 to i32
  %5 = tail call float @llvm.amdgcn.ldexp.f32(float %2, i32 %4)
  %6 = tail call i32 @llvm.amdgcn.frexp.exp.i32.f32(float %0)
  %7 = sub nsw i32 %6, %4
  %8 = fadd float %5, -1.000000e+00
  %9 = fadd float %5, 1.000000e+00
  %10 = fadd float %9, -1.000000e+00
  %11 = fsub float %5, %10
  %12 = tail call float @llvm.amdgcn.rcp.f32(float %9) #21
  %13 = fmul float %8, %12
  %14 = fmul float %9, %13
  %15 = load i32, i32 addrspace(4)* @__oclc_ISA_version, align 4, !tbaa !81
  %16 = freeze i32 %15
  %17 = icmp sgt i32 %16, 8999
  br i1 %17, label %19, label %18

18:                                               ; preds = %1
  switch i32 %16, label %22 [
    i32 8001, label %19
    i32 7001, label %19
  ]

19:                                               ; preds = %18, %18, %1
  %20 = fneg float %14
  %21 = tail call float @llvm.fma.f32(float %13, float %9, float %20) #21
  br label %36

22:                                               ; preds = %18
  %23 = bitcast float %13 to i32
  %24 = and i32 %23, -4096
  %25 = bitcast i32 %24 to float
  %26 = fsub float %13, %25
  %27 = bitcast float %9 to i32
  %28 = and i32 %27, -4096
  %29 = bitcast i32 %28 to float
  %30 = fsub float %9, %29
  %31 = fneg float %14
  %32 = tail call float @llvm.fmuladd.f32(float %25, float %29, float %31) #21
  %33 = tail call float @llvm.fmuladd.f32(float %25, float %30, float %32) #21
  %34 = tail call float @llvm.fmuladd.f32(float %26, float %29, float %33) #21
  %35 = tail call float @llvm.fmuladd.f32(float %26, float %30, float %34) #21
  br label %36

36:                                               ; preds = %22, %19
  %37 = phi float [ %35, %22 ], [ %21, %19 ]
  br i1 %17, label %39, label %38

38:                                               ; preds = %36
  switch i32 %16, label %41 [
    i32 8001, label %39
    i32 7001, label %39
  ]

39:                                               ; preds = %38, %38, %36
  %40 = tail call float @llvm.fma.f32(float %13, float %11, float %37) #21
  br label %43

41:                                               ; preds = %38
  %42 = tail call float @llvm.fmuladd.f32(float %13, float %11, float %37) #21
  br label %43

43:                                               ; preds = %41, %39
  %44 = phi float [ %42, %41 ], [ %40, %39 ]
  %45 = fadd float %14, %44
  %46 = fsub float %45, %14
  %47 = fsub float %44, %46
  %48 = fsub float %8, %45
  %49 = fsub float %8, %48
  %50 = fsub float %49, %45
  %51 = fsub float %50, %47
  %52 = fadd float %48, %51
  %53 = fmul float %12, %52
  %54 = fadd float %13, %53
  %55 = fsub float %54, %13
  %56 = fsub float %53, %55
  %57 = fmul float %54, %54
  br i1 %17, label %59, label %58

58:                                               ; preds = %43
  switch i32 %16, label %62 [
    i32 8001, label %59
    i32 7001, label %59
  ]

59:                                               ; preds = %58, %58, %43
  %60 = fneg float %57
  %61 = tail call float @llvm.fma.f32(float %54, float %54, float %60) #21
  br label %72

62:                                               ; preds = %58
  %63 = bitcast float %54 to i32
  %64 = and i32 %63, -4096
  %65 = bitcast i32 %64 to float
  %66 = fsub float %54, %65
  %67 = fneg float %57
  %68 = tail call float @llvm.fmuladd.f32(float %65, float %65, float %67) #21
  %69 = fmul float %65, 2.000000e+00
  %70 = tail call float @llvm.fmuladd.f32(float %69, float %66, float %68) #21
  %71 = tail call float @llvm.fmuladd.f32(float %66, float %66, float %70) #21
  br label %72

72:                                               ; preds = %62, %59
  %73 = phi float [ %71, %62 ], [ %61, %59 ]
  br i1 %17, label %75, label %74

74:                                               ; preds = %72
  switch i32 %16, label %78 [
    i32 8001, label %75
    i32 7001, label %75
  ]

75:                                               ; preds = %74, %74, %72
  %76 = fmul float %56, 2.000000e+00
  %77 = tail call float @llvm.fma.f32(float %54, float %76, float %73) #21
  br label %81

78:                                               ; preds = %74
  %79 = fmul float %56, 2.000000e+00
  %80 = tail call float @llvm.fmuladd.f32(float %79, float %54, float %73) #21
  br label %81

81:                                               ; preds = %78, %75
  %82 = phi float [ %80, %78 ], [ %77, %75 ]
  %83 = fadd float %57, %82
  %84 = fsub float %83, %57
  %85 = fsub float %82, %84
  %86 = tail call float @__ocml_fmuladd_f32(float %83, float 0x3FCED89C20000000, float 0x3FD23E9880000000) #17
  %87 = tail call float @__ocml_fmuladd_f32(float %83, float %86, float 0x3FD999BDE0000000) #17
  %88 = sitofp i32 %7 to float
  %89 = fmul float %88, 0x3FE62E4300000000
  br i1 %17, label %91, label %90

90:                                               ; preds = %81
  switch i32 %16, label %94 [
    i32 8001, label %91
    i32 7001, label %91
  ]

91:                                               ; preds = %90, %90, %81
  %92 = fneg float %89
  %93 = tail call float @llvm.fma.f32(float %88, float 0x3FE62E4300000000, float %92) #21
  br label %104

94:                                               ; preds = %90
  %95 = bitcast float %88 to i32
  %96 = and i32 %95, -4096
  %97 = bitcast i32 %96 to float
  %98 = fsub float %88, %97
  %99 = fneg float %89
  %100 = tail call float @llvm.fmuladd.f32(float %97, float 0x3FE62E0000000000, float %99) #21
  %101 = tail call float @llvm.fmuladd.f32(float %98, float 0x3FE62E0000000000, float %100) #21
  %102 = tail call float @llvm.fmuladd.f32(float %97, float 0x3F00C00000000000, float %101) #21
  %103 = tail call float @llvm.fmuladd.f32(float %98, float 0x3F00C00000000000, float %102) #21
  br label %104

104:                                              ; preds = %94, %91
  %105 = phi float [ %103, %94 ], [ %93, %91 ]
  br i1 %17, label %107, label %106

106:                                              ; preds = %104
  switch i32 %16, label %109 [
    i32 8001, label %107
    i32 7001, label %107
  ]

107:                                              ; preds = %106, %106, %104
  %108 = tail call float @llvm.fma.f32(float %88, float 0xBE205C6100000000, float %105) #21
  br label %111

109:                                              ; preds = %106
  %110 = tail call float @llvm.fmuladd.f32(float %88, float 0xBE205C6100000000, float %105) #21
  br label %111

111:                                              ; preds = %109, %107
  %112 = phi float [ %110, %109 ], [ %108, %107 ]
  %113 = fadd float %89, %112
  %114 = fsub float %113, %89
  %115 = fsub float %112, %114
  %116 = tail call float @llvm.amdgcn.ldexp.f32(float %54, i32 1) #21
  %117 = fmul float %54, %83
  br i1 %17, label %119, label %118

118:                                              ; preds = %111
  switch i32 %16, label %122 [
    i32 8001, label %119
    i32 7001, label %119
  ]

119:                                              ; preds = %118, %118, %111
  %120 = fneg float %117
  %121 = tail call float @llvm.fma.f32(float %83, float %54, float %120) #21
  br label %136

122:                                              ; preds = %118
  %123 = bitcast float %83 to i32
  %124 = and i32 %123, -4096
  %125 = bitcast i32 %124 to float
  %126 = fsub float %83, %125
  %127 = bitcast float %54 to i32
  %128 = and i32 %127, -4096
  %129 = bitcast i32 %128 to float
  %130 = fsub float %54, %129
  %131 = fneg float %117
  %132 = tail call float @llvm.fmuladd.f32(float %125, float %129, float %131) #21
  %133 = tail call float @llvm.fmuladd.f32(float %125, float %130, float %132) #21
  %134 = tail call float @llvm.fmuladd.f32(float %126, float %129, float %133) #21
  %135 = tail call float @llvm.fmuladd.f32(float %126, float %130, float %134) #21
  br label %136

136:                                              ; preds = %122, %119
  %137 = phi float [ %135, %122 ], [ %121, %119 ]
  br i1 %17, label %139, label %138

138:                                              ; preds = %136
  switch i32 %16, label %142 [
    i32 8001, label %139
    i32 7001, label %139
  ]

139:                                              ; preds = %138, %138, %136
  %140 = tail call float @llvm.fma.f32(float %83, float %56, float %137) #21
  %141 = tail call float @llvm.fma.f32(float %85, float %54, float %140) #21
  br label %146

142:                                              ; preds = %138
  %143 = fmul float %54, %85
  %144 = tail call float @llvm.fmuladd.f32(float %83, float %56, float %143) #21
  %145 = fadd float %144, %137
  br label %146

146:                                              ; preds = %142, %139
  %147 = phi float [ %145, %142 ], [ %141, %139 ]
  %148 = fadd float %117, %147
  %149 = fsub float %148, %117
  %150 = fsub float %147, %149
  %151 = fmul float %83, %87
  br i1 %17, label %153, label %152

152:                                              ; preds = %146
  switch i32 %16, label %156 [
    i32 8001, label %153
    i32 7001, label %153
  ]

153:                                              ; preds = %152, %152, %146
  %154 = fneg float %151
  %155 = tail call float @llvm.fma.f32(float %83, float %87, float %154) #21
  br label %170

156:                                              ; preds = %152
  %157 = bitcast float %83 to i32
  %158 = and i32 %157, -4096
  %159 = bitcast i32 %158 to float
  %160 = fsub float %83, %159
  %161 = bitcast float %87 to i32
  %162 = and i32 %161, -4096
  %163 = bitcast i32 %162 to float
  %164 = fsub float %87, %163
  %165 = fneg float %151
  %166 = tail call float @llvm.fmuladd.f32(float %159, float %163, float %165) #21
  %167 = tail call float @llvm.fmuladd.f32(float %159, float %164, float %166) #21
  %168 = tail call float @llvm.fmuladd.f32(float %160, float %163, float %167) #21
  %169 = tail call float @llvm.fmuladd.f32(float %160, float %164, float %168) #21
  br label %170

170:                                              ; preds = %156, %153
  %171 = phi float [ %169, %156 ], [ %155, %153 ]
  br i1 %17, label %173, label %172

172:                                              ; preds = %170
  switch i32 %16, label %175 [
    i32 8001, label %173
    i32 7001, label %173
  ]

173:                                              ; preds = %172, %172, %170
  %174 = tail call float @llvm.fma.f32(float %85, float %87, float %171) #21
  br label %177

175:                                              ; preds = %172
  %176 = tail call float @llvm.fmuladd.f32(float %85, float %87, float %171) #21
  br label %177

177:                                              ; preds = %175, %173
  %178 = phi float [ %176, %175 ], [ %174, %173 ]
  %179 = fadd float %151, %178
  %180 = fsub float %179, %151
  %181 = fsub float %178, %180
  %182 = fadd float %179, 0x3FE5555540000000
  %183 = fadd float %182, 0xBFE5555540000000
  %184 = fsub float %179, %183
  %185 = fadd float %181, 0x3E2E720200000000
  %186 = fadd float %185, %184
  %187 = fadd float %182, %186
  %188 = fsub float %187, %182
  %189 = fsub float %186, %188
  %190 = fmul float %148, %187
  br i1 %17, label %192, label %191

191:                                              ; preds = %177
  switch i32 %16, label %195 [
    i32 8001, label %192
    i32 7001, label %192
  ]

192:                                              ; preds = %191, %191, %177
  %193 = fneg float %190
  %194 = tail call float @llvm.fma.f32(float %148, float %187, float %193) #21
  br label %209

195:                                              ; preds = %191
  %196 = bitcast float %148 to i32
  %197 = and i32 %196, -4096
  %198 = bitcast i32 %197 to float
  %199 = fsub float %148, %198
  %200 = bitcast float %187 to i32
  %201 = and i32 %200, -4096
  %202 = bitcast i32 %201 to float
  %203 = fsub float %187, %202
  %204 = fneg float %190
  %205 = tail call float @llvm.fmuladd.f32(float %198, float %202, float %204) #21
  %206 = tail call float @llvm.fmuladd.f32(float %198, float %203, float %205) #21
  %207 = tail call float @llvm.fmuladd.f32(float %199, float %202, float %206) #21
  %208 = tail call float @llvm.fmuladd.f32(float %199, float %203, float %207) #21
  br label %209

209:                                              ; preds = %195, %192
  %210 = phi float [ %208, %195 ], [ %194, %192 ]
  br i1 %17, label %212, label %211

211:                                              ; preds = %209
  switch i32 %16, label %215 [
    i32 8001, label %212
    i32 7001, label %212
  ]

212:                                              ; preds = %211, %211, %209
  %213 = tail call float @llvm.fma.f32(float %148, float %189, float %210) #21
  %214 = tail call float @llvm.fma.f32(float %150, float %187, float %213) #21
  br label %219

215:                                              ; preds = %211
  %216 = fmul float %150, %187
  %217 = tail call float @llvm.fmuladd.f32(float %148, float %189, float %216) #21
  %218 = fadd float %217, %210
  br label %219

219:                                              ; preds = %215, %212
  %220 = phi float [ %218, %215 ], [ %214, %212 ]
  %221 = tail call float @llvm.amdgcn.ldexp.f32(float %56, i32 1) #21
  %222 = fadd float %190, %220
  %223 = fsub float %222, %190
  %224 = fsub float %220, %223
  %225 = fadd float %116, %222
  %226 = fsub float %225, %116
  %227 = fsub float %222, %226
  %228 = fadd float %221, %224
  %229 = fadd float %228, %227
  %230 = fadd float %225, %229
  %231 = fsub float %230, %225
  %232 = fsub float %229, %231
  %233 = fadd float %113, %230
  %234 = fsub float %233, %113
  %235 = fsub float %233, %234
  %236 = fsub float %113, %235
  %237 = fsub float %230, %234
  %238 = fadd float %237, %236
  %239 = fadd float %115, %232
  %240 = fsub float %239, %115
  %241 = fsub float %239, %240
  %242 = fsub float %115, %241
  %243 = fsub float %232, %240
  %244 = fadd float %243, %242
  %245 = fadd float %239, %238
  %246 = fadd float %233, %245
  %247 = fsub float %246, %233
  %248 = fsub float %245, %247
  %249 = fadd float %244, %248
  %250 = fadd float %246, %249
  %251 = fsub float %250, %246
  %252 = fsub float %249, %251
  %253 = insertelement <2 x float> undef, float %252, i64 0
  %254 = insertelement <2 x float> %253, float %250, i64 1
  ret <2 x float> %254
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocmlpriv_expep_f32(<2 x float> %0) local_unnamed_addr #12 {
  %2 = extractelement <2 x float> %0, i64 1
  %3 = fcmp oeq float %2, 0x40562E4300000000
  %4 = select i1 %3, float 0x3EE0000000000000, float 0.000000e+00
  %5 = fsub float %2, %4
  %6 = extractelement <2 x float> %0, i64 0
  %7 = fadd float %6, %4
  %8 = tail call float @__ocml_exp_f32(float %5) #17
  %9 = tail call float @llvm.fma.f32(float %8, float %7, float %8)
  %10 = tail call float @llvm.fabs.f32(float %8) #26
  %11 = fcmp oeq float %10, 0x7FF0000000000000
  %12 = select i1 %11, float %8, float %9
  ret float %12
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.trunc.f32(float) #7

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.amdgcn.fract.f32(float) #8

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.amdgcn.frexp.mant.f32(float) #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.frexp.exp.i32.f32(float) #8

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_sin_f64(double %0) #9 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = tail call %0 @__ocmlpriv_trigred_f64(double %2) #17
  %4 = extractvalue %0 %3, 0
  %5 = extractvalue %0 %3, 1
  %6 = extractvalue %0 %3, 2
  %7 = tail call %1 @__ocmlpriv_sincosred2_f64(double %5, double %4) #17
  %8 = extractvalue %1 %7, 0
  %9 = extractvalue %1 %7, 1
  %10 = and i32 %6, 1
  %11 = icmp eq i32 %10, 0
  %12 = select i1 %11, double %8, double %9
  %13 = bitcast double %12 to <2 x i32>
  %14 = icmp sgt i32 %6, 1
  %15 = select i1 %14, i32 -2147483648, i32 0
  %16 = bitcast double %0 to <2 x i32>
  %17 = extractelement <2 x i32> %16, i64 1
  %18 = and i32 %17, -2147483648
  %19 = xor i32 %15, %18
  %20 = extractelement <2 x i32> %13, i64 1
  %21 = xor i32 %19, %20
  %22 = insertelement <2 x i32> %13, i32 %21, i64 1
  %23 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %24 = icmp eq i8 %23, 0
  %25 = tail call i1 @llvm.amdgcn.class.f64(double %2, i32 504)
  %26 = select i1 %25, <2 x i32> %22, <2 x i32> <i32 0, i32 2146959360>
  %27 = select i1 %24, <2 x i32> %26, <2 x i32> %22
  %28 = bitcast <2 x i32> %27 to double
  ret double %28
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_sin_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = tail call [2 x i32] @__ocmlpriv_trigred_f32(float %2) #17
  %4 = extractvalue [2 x i32] %3, 0
  %5 = bitcast i32 %4 to float
  %6 = extractvalue [2 x i32] %3, 1
  %7 = tail call [2 x i32] @__ocmlpriv_sincosred_f32(float %5) #17
  %8 = extractvalue [2 x i32] %7, 0
  %9 = extractvalue [2 x i32] %7, 1
  %10 = and i32 %6, 1
  %11 = icmp eq i32 %10, 0
  %12 = select i1 %11, i32 %8, i32 %9
  %13 = icmp sgt i32 %6, 1
  %14 = select i1 %13, i32 -2147483648, i32 0
  %15 = bitcast float %0 to i32
  %16 = bitcast float %2 to i32
  %17 = xor i32 %16, %15
  %18 = xor i32 %17, %14
  %19 = xor i32 %18, %12
  %20 = bitcast i32 %19 to float
  %21 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %22 = icmp eq i8 %21, 0
  %23 = tail call i1 @llvm.amdgcn.class.f32(float %2, i32 504)
  %24 = select i1 %23, float %20, float 0x7FF8000000000000
  %25 = select i1 %22, float %24, float %20
  ret float %25
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_tan_f64(double %0) #9 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = tail call %0 @__ocmlpriv_trigred_f64(double %2) #17
  %4 = extractvalue %0 %3, 0
  %5 = extractvalue %0 %3, 1
  %6 = extractvalue %0 %3, 2
  %7 = and i32 %6, 1
  %8 = tail call double @__ocmlpriv_tanred2_f64(double %5, double %4, i32 %7) #17
  %9 = bitcast double %8 to <2 x i32>
  %10 = bitcast double %0 to <2 x i32>
  %11 = extractelement <2 x i32> %10, i64 1
  %12 = and i32 %11, -2147483648
  %13 = extractelement <2 x i32> %9, i64 1
  %14 = xor i32 %13, %12
  %15 = insertelement <2 x i32> %9, i32 %14, i64 1
  %16 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %17 = icmp eq i8 %16, 0
  %18 = tail call i1 @llvm.amdgcn.class.f64(double %2, i32 504)
  %19 = select i1 %18, <2 x i32> %15, <2 x i32> <i32 0, i32 2146959360>
  %20 = select i1 %17, <2 x i32> %19, <2 x i32> %15
  %21 = bitcast <2 x i32> %20 to double
  ret double %21
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal double @__ocmlpriv_tanred2_f64(double %0, double %1, i32 %2) local_unnamed_addr #6 {
  %4 = fmul double %0, %0
  %5 = fneg double %4
  %6 = tail call double @llvm.fma.f64(double %0, double %0, double %5) #21
  %7 = fmul double %1, 2.000000e+00
  %8 = tail call double @llvm.fma.f64(double %0, double %7, double %6) #21
  %9 = fadd double %4, %8
  %10 = tail call double @llvm.fma.f64(double %9, double 0x3EF5E089C751C08C, double 0xBF078809A9A29F71)
  %11 = tail call double @llvm.fma.f64(double %9, double %10, double 0x3F17746F90A8AAE0)
  %12 = tail call double @llvm.fma.f64(double %9, double %11, double 0xBEFBB44DA6FBF144)
  %13 = tail call double @llvm.fma.f64(double %9, double %12, double 0x3F21E634A7943ACF)
  %14 = tail call double @llvm.fma.f64(double %9, double %13, double 0x3F2D250FDEB68FEB)
  %15 = tail call double @llvm.fma.f64(double %9, double %14, double 0x3F437FD9B58C4D95)
  %16 = tail call double @llvm.fma.f64(double %9, double %15, double 0x3F57D5AF15120E2C)
  %17 = tail call double @llvm.fma.f64(double %9, double %16, double 0x3F6D6D93E09491DF)
  %18 = tail call double @llvm.fma.f64(double %9, double %17, double 0x3F8226E12033784D)
  %19 = tail call double @llvm.fma.f64(double %9, double %18, double 0x3F9664F49AC36AE2)
  %20 = tail call double @llvm.fma.f64(double %9, double %19, double 0x3FABA1BA1B451C21)
  %21 = tail call double @llvm.fma.f64(double %9, double %20, double 0x3FC11111111185B7)
  %22 = tail call double @llvm.fma.f64(double %9, double %21, double 0x3FD55555555554EE)
  %23 = fmul double %9, %22
  %24 = fmul double %23, %0
  %25 = fneg double %24
  %26 = tail call double @llvm.fma.f64(double %0, double %23, double %25) #21
  %27 = fadd double %24, %0
  %28 = fsub double %27, %0
  %29 = fsub double %24, %28
  %30 = fadd double %26, %1
  %31 = fadd double %30, %29
  %32 = fadd double %27, %31
  %33 = fsub double %32, %27
  %34 = fsub double %31, %33
  %35 = tail call double @llvm.amdgcn.rcp.f64(double %32) #21
  %36 = fneg double %32
  %37 = tail call double @llvm.fma.f64(double %36, double %35, double 1.000000e+00) #21
  %38 = tail call double @llvm.fma.f64(double %37, double %35, double %35) #21
  %39 = tail call double @llvm.fma.f64(double %36, double %38, double 1.000000e+00) #21
  %40 = tail call double @llvm.fma.f64(double %39, double %38, double %38) #21
  %41 = fmul double %32, %40
  %42 = fneg double %41
  %43 = tail call double @llvm.fma.f64(double %40, double %32, double %42) #21
  %44 = tail call double @llvm.fma.f64(double %40, double %34, double %43) #21
  %45 = fadd double %41, %44
  %46 = fsub double %45, %41
  %47 = fsub double %44, %46
  %48 = fsub double 1.000000e+00, %45
  %49 = fsub double 1.000000e+00, %48
  %50 = fsub double %49, %45
  %51 = fsub double %50, %47
  %52 = fadd double %48, %51
  %53 = fmul double %40, %52
  %54 = fadd double %40, %53
  %55 = icmp eq i32 %2, 0
  %56 = fneg double %54
  %57 = select i1 %55, double %32, double %56
  ret double %57
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_tan_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = tail call [2 x i32] @__ocmlpriv_trigred_f32(float %2) #17
  %4 = extractvalue [2 x i32] %3, 0
  %5 = bitcast i32 %4 to float
  %6 = extractvalue [2 x i32] %3, 1
  %7 = and i32 %6, 1
  %8 = tail call float @__ocmlpriv_tanred_f32(float %5, i32 %7) #17
  %9 = bitcast float %8 to i32
  %10 = bitcast float %0 to i32
  %11 = bitcast float %2 to i32
  %12 = xor i32 %11, %10
  %13 = xor i32 %12, %9
  %14 = bitcast i32 %13 to float
  %15 = load i8, i8 addrspace(4)* @__oclc_finite_only_opt, align 1, !tbaa !75, !range !79
  %16 = icmp eq i8 %15, 0
  %17 = tail call i1 @llvm.amdgcn.class.f32(float %2, i32 504)
  %18 = select i1 %17, float %14, float 0x7FF8000000000000
  %19 = select i1 %16, float %18, float %14
  ret float %19
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocmlpriv_tanred_f32(float %0, i32 %1) local_unnamed_addr #9 {
  %3 = fmul float %0, %0
  %4 = tail call float @__ocml_fmuladd_f32(float %3, float 0xBF919DBA60000000, float 0x3FD8A8B0E0000000) #17
  %5 = tail call float @__ocml_fmuladd_f32(float %3, float 0x3F92E29000000000, float 0xBFE07266E0000000) #17
  %6 = tail call float @__ocml_fmuladd_f32(float %3, float %5, float 0x3FF27E84A0000000) #17
  %7 = tail call float @llvm.amdgcn.rcp.f32(float %6)
  %8 = fmul float %4, %7
  %9 = fmul float %3, %8
  %10 = tail call float @llvm.fma.f32(float %9, float %0, float %0)
  %11 = fsub float %10, %0
  %12 = fneg float %11
  %13 = tail call float @llvm.fma.f32(float %9, float %0, float %12)
  %14 = tail call float @llvm.amdgcn.rcp.f32(float %10)
  %15 = fneg float %14
  %16 = tail call float @llvm.fma.f32(float %10, float %15, float 1.000000e+00)
  %17 = tail call float @llvm.fma.f32(float %13, float %15, float %16)
  %18 = tail call float @llvm.fma.f32(float %17, float %15, float %15)
  %19 = icmp eq i32 %1, 0
  %20 = select i1 %19, float %10, float %18
  ret float %20
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal double @__ocml_tanh_f64(double %0) #11 {
  %2 = tail call double @llvm.fabs.f64(double %0)
  %3 = insertelement <2 x double> <double 0.000000e+00, double poison>, double %2, i64 1
  %4 = tail call <2 x double> @__ocmlpriv_epexpep_f64(<2 x double> %3) #17
  %5 = extractelement <2 x double> %4, i64 1
  %6 = tail call double @llvm.amdgcn.rcp.f64(double %5) #21
  %7 = fneg double %5
  %8 = tail call double @llvm.fma.f64(double %7, double %6, double 1.000000e+00) #21
  %9 = tail call double @llvm.fma.f64(double %8, double %6, double %6) #21
  %10 = tail call double @llvm.fma.f64(double %7, double %9, double 1.000000e+00) #21
  %11 = tail call double @llvm.fma.f64(double %10, double %9, double %9) #21
  %12 = fmul double %5, %11
  %13 = fneg double %12
  %14 = tail call double @llvm.fma.f64(double %11, double %5, double %13) #21
  %15 = extractelement <2 x double> %4, i64 0
  %16 = tail call double @llvm.fma.f64(double %11, double %15, double %14) #21
  %17 = fadd double %12, %16
  %18 = fsub double %17, %12
  %19 = fsub double %16, %18
  %20 = fsub double 1.000000e+00, %17
  %21 = fsub double 1.000000e+00, %20
  %22 = fsub double %21, %17
  %23 = fsub double %22, %19
  %24 = fadd double %20, %23
  %25 = fsub double %24, %20
  %26 = fsub double %23, %25
  %27 = fmul double %11, %24
  %28 = fmul double %5, %27
  %29 = fneg double %28
  %30 = tail call double @llvm.fma.f64(double %27, double %5, double %29) #21
  %31 = tail call double @llvm.fma.f64(double %27, double %15, double %30) #21
  %32 = fadd double %28, %31
  %33 = fsub double %32, %28
  %34 = fsub double %31, %33
  %35 = fsub double %24, %32
  %36 = fsub double %24, %35
  %37 = fsub double %36, %32
  %38 = fadd double %26, %37
  %39 = fsub double %38, %34
  %40 = fadd double %35, %39
  %41 = fmul double %11, %40
  %42 = fadd double %11, %27
  %43 = fsub double %42, %11
  %44 = fsub double %27, %43
  %45 = fadd double %44, %41
  %46 = fadd double %42, %45
  %47 = fsub double %46, %42
  %48 = fsub double %45, %47
  %49 = fsub double %5, %46
  %50 = fsub double %5, %49
  %51 = fsub double %50, %46
  %52 = fadd double %15, %51
  %53 = fsub double %52, %48
  %54 = fadd double %49, %53
  %55 = fsub double %54, %49
  %56 = fsub double %53, %55
  %57 = fadd double %5, %46
  %58 = fsub double %57, %5
  %59 = fsub double %46, %58
  %60 = fadd double %15, %48
  %61 = fadd double %60, %59
  %62 = fadd double %57, %61
  %63 = fsub double %62, %57
  %64 = fsub double %61, %63
  %65 = tail call double @llvm.amdgcn.rcp.f64(double %62) #21
  %66 = fneg double %62
  %67 = tail call double @llvm.fma.f64(double %66, double %65, double 1.000000e+00) #21
  %68 = tail call double @llvm.fma.f64(double %67, double %65, double %65) #21
  %69 = tail call double @llvm.fma.f64(double %66, double %68, double 1.000000e+00) #21
  %70 = tail call double @llvm.fma.f64(double %69, double %68, double %68) #21
  %71 = fmul double %54, %70
  %72 = fmul double %62, %71
  %73 = fneg double %72
  %74 = tail call double @llvm.fma.f64(double %71, double %62, double %73) #21
  %75 = tail call double @llvm.fma.f64(double %71, double %64, double %74) #21
  %76 = fadd double %72, %75
  %77 = fsub double %76, %72
  %78 = fsub double %75, %77
  %79 = fsub double %54, %76
  %80 = fsub double %54, %79
  %81 = fsub double %80, %76
  %82 = fsub double %81, %78
  %83 = fadd double %56, %82
  %84 = fadd double %79, %83
  %85 = fmul double %70, %84
  %86 = fadd double %71, %85
  %87 = fcmp ogt double %2, 1.906250e+01
  %88 = select i1 %87, double 1.000000e+00, double %86
  %89 = fcmp olt double %2, 0x3E40000000000000
  %90 = select i1 %89, double %2, double %88
  %91 = tail call double @llvm.copysign.f64(double %90, double %0)
  ret double %91
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal <2 x double> @__ocmlpriv_epexpep_f64(<2 x double> %0) local_unnamed_addr #6 {
  %2 = extractelement <2 x double> %0, i64 1
  %3 = fmul double %2, 0x3FF71547652B82FE
  %4 = tail call double @llvm.rint.f64(double %3)
  %5 = tail call double @llvm.fma.f64(double %4, double 0xBFE62E42FEFA3000, double %2)
  %6 = extractelement <2 x double> %0, i64 0
  %7 = fadd double %6, %5
  %8 = fsub double %7, %5
  %9 = fsub double %6, %8
  %10 = fmul double %4, 0x3D53DE6AF278E000
  %11 = fsub double %7, %10
  %12 = fsub double %7, %11
  %13 = fsub double %12, %10
  %14 = fadd double %9, %13
  %15 = fadd double %11, %14
  %16 = fsub double %15, %11
  %17 = fsub double %14, %16
  %18 = fmul double %4, 0x3AC9CC01F97B57A0
  %19 = fsub double %15, %18
  %20 = fsub double %15, %19
  %21 = fsub double %20, %18
  %22 = fadd double %17, %21
  %23 = fadd double %19, %22
  %24 = fsub double %23, %19
  %25 = fsub double %22, %24
  %26 = tail call double @llvm.fma.f64(double %23, double 0x3E5ADE156A5DCB37, double 0x3E928AF3FCA7AB0C)
  %27 = tail call double @llvm.fma.f64(double %23, double %26, double 0x3EC71DEE623FDE64)
  %28 = tail call double @llvm.fma.f64(double %23, double %27, double 0x3EFA01997C89E6B0)
  %29 = tail call double @llvm.fma.f64(double %23, double %28, double 0x3F2A01A014761F6E)
  %30 = tail call double @llvm.fma.f64(double %23, double %29, double 0x3F56C16C1852B7B0)
  %31 = tail call double @llvm.fma.f64(double %23, double %30, double 0x3F81111111122322)
  %32 = tail call double @llvm.fma.f64(double %23, double %31, double 0x3FA55555555502A1)
  %33 = tail call double @llvm.fma.f64(double %23, double %32, double 0x3FC5555555555511)
  %34 = tail call double @llvm.fma.f64(double %23, double %33, double 0x3FE000000000000B)
  %35 = fmul double %23, %23
  %36 = fneg double %35
  %37 = tail call double @llvm.fma.f64(double %23, double %23, double %36) #21
  %38 = fmul double %25, 2.000000e+00
  %39 = tail call double @llvm.fma.f64(double %23, double %38, double %37) #21
  %40 = fadd double %35, %39
  %41 = fsub double %40, %35
  %42 = fsub double %39, %41
  %43 = fmul double %40, %34
  %44 = fneg double %43
  %45 = tail call double @llvm.fma.f64(double %40, double %34, double %44) #21
  %46 = tail call double @llvm.fma.f64(double %42, double %34, double %45) #21
  %47 = fadd double %43, %46
  %48 = fsub double %47, %43
  %49 = fsub double %46, %48
  %50 = fadd double %23, %47
  %51 = fsub double %50, %23
  %52 = fsub double %47, %51
  %53 = fadd double %25, %49
  %54 = fadd double %53, %52
  %55 = fadd double %50, %54
  %56 = fsub double %55, %50
  %57 = fsub double %54, %56
  %58 = fadd double %55, 1.000000e+00
  %59 = fadd double %58, -1.000000e+00
  %60 = fsub double %55, %59
  %61 = fadd double %57, %60
  %62 = fadd double %58, %61
  %63 = fsub double %62, %58
  %64 = fsub double %61, %63
  %65 = fptosi double %4 to i32
  %66 = tail call double @llvm.amdgcn.ldexp.f64(double %62, i32 %65) #21
  %67 = tail call double @llvm.amdgcn.ldexp.f64(double %64, i32 %65) #21
  %68 = insertelement <2 x double> undef, double %67, i64 0
  %69 = insertelement <2 x double> %68, double %66, i64 1
  ret <2 x double> %69
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind readnone willreturn
define internal float @__ocml_tanh_f32(float %0) #9 {
  %2 = tail call float @llvm.fabs.f32(float %0)
  %3 = fcmp olt float %2, 6.250000e-01
  br i1 %3, label %4, label %12

4:                                                ; preds = %1
  %5 = fmul float %0, %0
  %6 = tail call float @__ocml_fmuladd_f32(float %5, float 0xBF7758E7A0000000, float 0x3F95211920000000) #17
  %7 = tail call float @__ocml_fmuladd_f32(float %5, float %6, float 0xBFAB8389C0000000) #17
  %8 = tail call float @__ocml_fmuladd_f32(float %5, float %7, float 0x3FC1107040000000) #17
  %9 = tail call float @__ocml_fmuladd_f32(float %5, float %8, float 0xBFD5555320000000) #17
  %10 = fmul float %2, %9
  %11 = tail call float @__ocml_fmuladd_f32(float %5, float %10, float %2) #17
  br label %19

12:                                               ; preds = %1
  %13 = fmul float %2, 2.000000e+00
  %14 = tail call float @__ocml_exp_f32(float %13) #17
  %15 = fadd float %14, 1.000000e+00
  %16 = tail call float @llvm.amdgcn.rcp.f32(float %15)
  %17 = fmul float %16, 2.000000e+00
  %18 = fsub float 1.000000e+00, %17
  br label %19

19:                                               ; preds = %12, %4
  %20 = phi float [ %11, %4 ], [ %18, %12 ]
  %21 = tail call float @llvm.copysign.f32(float %20, float %0)
  ret float %21
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal i64 @__ockl_get_num_groups(i32 %0) #10 {
  %2 = tail call align 4 dereferenceable(64) i8 addrspace(4)* @llvm.amdgcn.dispatch.ptr()
  switch i32 %0, label %24 [
    i32 0, label %3
    i32 1, label %10
    i32 2, label %17
  ]

3:                                                ; preds = %1
  %4 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 12
  %5 = bitcast i8 addrspace(4)* %4 to i32 addrspace(4)*
  %6 = load i32, i32 addrspace(4)* %5, align 4, !tbaa !84
  %7 = getelementptr i8, i8 addrspace(4)* %2, i64 4
  %8 = bitcast i8 addrspace(4)* %7 to i16 addrspace(4)*
  %9 = load i16, i16 addrspace(4)* %8, align 4, !range !90, !invariant.load !91
  br label %24

10:                                               ; preds = %1
  %11 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 16
  %12 = bitcast i8 addrspace(4)* %11 to i32 addrspace(4)*
  %13 = load i32, i32 addrspace(4)* %12, align 8, !tbaa !92
  %14 = getelementptr i8, i8 addrspace(4)* %2, i64 6
  %15 = bitcast i8 addrspace(4)* %14 to i16 addrspace(4)*
  %16 = load i16, i16 addrspace(4)* %15, align 2, !range !90, !invariant.load !91
  br label %24

17:                                               ; preds = %1
  %18 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 20
  %19 = bitcast i8 addrspace(4)* %18 to i32 addrspace(4)*
  %20 = load i32, i32 addrspace(4)* %19, align 4, !tbaa !93
  %21 = getelementptr i8, i8 addrspace(4)* %2, i64 8
  %22 = bitcast i8 addrspace(4)* %21 to i16 addrspace(4)*
  %23 = load i16, i16 addrspace(4)* %22, align 4, !range !90, !invariant.load !91
  br label %24

24:                                               ; preds = %17, %10, %3, %1
  %25 = phi i16 [ %23, %17 ], [ %16, %10 ], [ %9, %3 ], [ 1, %1 ]
  %26 = phi i32 [ %20, %17 ], [ %13, %10 ], [ %6, %3 ], [ 1, %1 ]
  %27 = zext i16 %25 to i32
  %28 = udiv i32 %26, %27
  %29 = mul i32 %28, %27
  %30 = icmp ugt i32 %26, %29
  %31 = zext i1 %30 to i32
  %32 = add i32 %28, %31
  %33 = zext i32 %32 to i64
  ret i64 %33
}

; Function Attrs: nounwind readnone speculatable willreturn
declare align 4 i8 addrspace(4)* @llvm.amdgcn.dispatch.ptr() #8

; Function Attrs: convergent norecurse nounwind
define internal i64 @__ockl_fprintf_stderr_begin() #13 {
  %1 = tail call <2 x i64> @__ockl_hostcall_preview(i32 2, i64 33, i64 1, i64 0, i64 0, i64 0, i64 0, i64 0, i64 0) #24
  %2 = extractelement <2 x i64> %1, i64 0
  ret i64 %2
}

; Function Attrs: convergent norecurse nounwind
define internal <2 x i64> @__ockl_hostcall_preview(i32 %0, i64 %1, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8) local_unnamed_addr #13 {
  %10 = tail call i8 addrspace(4)* @llvm.amdgcn.implicitarg.ptr()
  %11 = getelementptr inbounds i8, i8 addrspace(4)* %10, i64 24
  %12 = bitcast i8 addrspace(4)* %11 to i64 addrspace(4)*
  %13 = load i64, i64 addrspace(4)* %12, align 8, !tbaa !94
  %14 = inttoptr i64 %13 to i8*
  %15 = tail call <2 x i64> @__ockl_hostcall_internal(i8* %14, i32 %0, i64 %1, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8) #24
  ret <2 x i64> %15
}

; Function Attrs: nounwind readnone speculatable willreturn
declare align 4 i8 addrspace(4)* @llvm.amdgcn.implicitarg.ptr() #8

; Function Attrs: convergent noinline norecurse nounwind optnone
define internal <2 x i64> @__ockl_hostcall_internal(i8* %0, i32 %1, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8, i64 %9) local_unnamed_addr #14 {
  %11 = alloca i8*, align 8, addrspace(5)
  %12 = alloca i32, align 4, addrspace(5)
  %13 = alloca i64, align 8, addrspace(5)
  %14 = alloca i64, align 8, addrspace(5)
  %15 = alloca i64, align 8, addrspace(5)
  %16 = alloca i64, align 8, addrspace(5)
  %17 = alloca i64, align 8, addrspace(5)
  %18 = alloca i64, align 8, addrspace(5)
  %19 = alloca i64, align 8, addrspace(5)
  %20 = alloca i64, align 8, addrspace(5)
  %21 = alloca i32, align 4, addrspace(5)
  %22 = alloca i32, align 4, addrspace(5)
  %23 = alloca %2 addrspace(1)*, align 8, addrspace(5)
  %24 = alloca i64, align 8, addrspace(5)
  %25 = alloca %3 addrspace(1)*, align 8, addrspace(5)
  %26 = alloca %4 addrspace(1)*, align 8, addrspace(5)
  %27 = alloca <2 x i64>, align 16, addrspace(5)
  store i8* %0, i8* addrspace(5)* %11, align 8, !tbaa !95
  store i32 %1, i32 addrspace(5)* %12, align 4, !tbaa !81
  store i64 %2, i64 addrspace(5)* %13, align 8, !tbaa !94
  store i64 %3, i64 addrspace(5)* %14, align 8, !tbaa !94
  store i64 %4, i64 addrspace(5)* %15, align 8, !tbaa !94
  store i64 %5, i64 addrspace(5)* %16, align 8, !tbaa !94
  store i64 %6, i64 addrspace(5)* %17, align 8, !tbaa !94
  store i64 %7, i64 addrspace(5)* %18, align 8, !tbaa !94
  store i64 %8, i64 addrspace(5)* %19, align 8, !tbaa !94
  store i64 %9, i64 addrspace(5)* %20, align 8, !tbaa !94
  %28 = bitcast i32 addrspace(5)* %21 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 4, i8 addrspace(5)* %28) #21
  %29 = call i32 @llvm.amdgcn.mbcnt.lo(i32 -1, i32 0) #21
  store i32 %29, i32 addrspace(5)* %21, align 4, !tbaa !81
  %30 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %31 = call fastcc i32 @0(i32 %30) #28
  store i32 %31, i32 addrspace(5)* %21, align 4, !tbaa !81
  %32 = bitcast i32 addrspace(5)* %22 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 4, i8 addrspace(5)* %32) #21
  %33 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %34 = call i32 @llvm.amdgcn.readfirstlane(i32 %33)
  store i32 %34, i32 addrspace(5)* %22, align 4, !tbaa !81
  %35 = bitcast %2 addrspace(1)* addrspace(5)* %23 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 8, i8 addrspace(5)* %35) #21
  %36 = load i8*, i8* addrspace(5)* %11, align 8, !tbaa !95
  %37 = addrspacecast i8* %36 to %2 addrspace(1)*
  store %2 addrspace(1)* %37, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %38 = bitcast i64 addrspace(5)* %24 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 8, i8 addrspace(5)* %38) #21
  %39 = load %2 addrspace(1)*, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %40 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %41 = load i32, i32 addrspace(5)* %22, align 4, !tbaa !81
  %42 = call fastcc i64 @1(%2 addrspace(1)* %39, i32 %40, i32 %41) #28
  store i64 %42, i64 addrspace(5)* %24, align 8, !tbaa !94
  %43 = bitcast %3 addrspace(1)* addrspace(5)* %25 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 8, i8 addrspace(5)* %43) #21
  %44 = load %2 addrspace(1)*, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %45 = load i64, i64 addrspace(5)* %24, align 8, !tbaa !94
  %46 = getelementptr %2, %2 addrspace(1)* %44, i64 0, i32 0
  %47 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(1)* %46, align 8, !tbaa !96
  %48 = getelementptr %2, %2 addrspace(1)* %44, i64 0, i32 5
  %49 = load i64, i64 addrspace(1)* %48, align 8, !tbaa !98
  %50 = call fastcc %3 addrspace(1)* @2(%3 addrspace(1)* %47, i64 %49, i64 %45) #28
  store %3 addrspace(1)* %50, %3 addrspace(1)* addrspace(5)* %25, align 8, !tbaa !95
  %51 = bitcast %4 addrspace(1)* addrspace(5)* %26 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 8, i8 addrspace(5)* %51) #21
  %52 = load %2 addrspace(1)*, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %53 = load i64, i64 addrspace(5)* %24, align 8, !tbaa !94
  %54 = getelementptr %2, %2 addrspace(1)* %52, i64 0, i32 1
  %55 = load %4 addrspace(1)*, %4 addrspace(1)* addrspace(1)* %54, align 8, !tbaa !99
  %56 = getelementptr %2, %2 addrspace(1)* %52, i64 0, i32 5
  %57 = load i64, i64 addrspace(1)* %56, align 8, !tbaa !98
  %58 = call fastcc %4 addrspace(1)* @3(%4 addrspace(1)* %55, i64 %57, i64 %53) #28
  store %4 addrspace(1)* %58, %4 addrspace(1)* addrspace(5)* %26, align 8, !tbaa !95
  %59 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(5)* %25, align 8, !tbaa !95
  %60 = load %4 addrspace(1)*, %4 addrspace(1)* addrspace(5)* %26, align 8, !tbaa !95
  %61 = load i32, i32 addrspace(5)* %12, align 4, !tbaa !81
  %62 = load i64, i64 addrspace(5)* %13, align 8, !tbaa !94
  %63 = load i64, i64 addrspace(5)* %14, align 8, !tbaa !94
  %64 = load i64, i64 addrspace(5)* %15, align 8, !tbaa !94
  %65 = load i64, i64 addrspace(5)* %16, align 8, !tbaa !94
  %66 = load i64, i64 addrspace(5)* %17, align 8, !tbaa !94
  %67 = load i64, i64 addrspace(5)* %18, align 8, !tbaa !94
  %68 = load i64, i64 addrspace(5)* %19, align 8, !tbaa !94
  %69 = load i64, i64 addrspace(5)* %20, align 8, !tbaa !94
  %70 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %71 = load i32, i32 addrspace(5)* %22, align 4, !tbaa !81
  call fastcc void @4(%3 addrspace(1)* %59, %4 addrspace(1)* %60, i32 %61, i64 %62, i64 %63, i64 %64, i64 %65, i64 %66, i64 %67, i64 %68, i64 %69, i32 %70, i32 %71) #28
  %72 = load %2 addrspace(1)*, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %73 = load i64, i64 addrspace(5)* %24, align 8, !tbaa !94
  %74 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %75 = load i32, i32 addrspace(5)* %22, align 4, !tbaa !81
  call fastcc void @5(%2 addrspace(1)* %72, i64 %73, i32 %74, i32 %75) #28
  %76 = bitcast <2 x i64> addrspace(5)* %27 to i8 addrspace(5)*
  call void @llvm.lifetime.start.p5i8(i64 16, i8 addrspace(5)* %76) #21
  %77 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(5)* %25, align 8, !tbaa !95
  %78 = load %4 addrspace(1)*, %4 addrspace(1)* addrspace(5)* %26, align 8, !tbaa !95
  %79 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %80 = load i32, i32 addrspace(5)* %22, align 4, !tbaa !81
  %81 = call fastcc <2 x i64> @6(%3 addrspace(1)* %77, %4 addrspace(1)* %78, i32 %79, i32 %80) #28
  store <2 x i64> %81, <2 x i64> addrspace(5)* %27, align 16, !tbaa !100
  %82 = load %2 addrspace(1)*, %2 addrspace(1)* addrspace(5)* %23, align 8, !tbaa !95
  %83 = load i64, i64 addrspace(5)* %24, align 8, !tbaa !94
  %84 = load i32, i32 addrspace(5)* %21, align 4, !tbaa !81
  %85 = load i32, i32 addrspace(5)* %22, align 4, !tbaa !81
  call fastcc void @7(%2 addrspace(1)* %82, i64 %83, i32 %84, i32 %85) #28
  %86 = load <2 x i64>, <2 x i64> addrspace(5)* %27, align 16, !tbaa !100
  %87 = bitcast <2 x i64> addrspace(5)* %27 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 16, i8 addrspace(5)* %87) #21
  %88 = bitcast %4 addrspace(1)* addrspace(5)* %26 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 8, i8 addrspace(5)* %88) #21
  %89 = bitcast %3 addrspace(1)* addrspace(5)* %25 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 8, i8 addrspace(5)* %89) #21
  %90 = bitcast i64 addrspace(5)* %24 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 8, i8 addrspace(5)* %90) #21
  %91 = bitcast %2 addrspace(1)* addrspace(5)* %23 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 8, i8 addrspace(5)* %91) #21
  %92 = bitcast i32 addrspace(5)* %22 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 4, i8 addrspace(5)* %92) #21
  %93 = bitcast i32 addrspace(5)* %21 to i8 addrspace(5)*
  call void @llvm.lifetime.end.p5i8(i64 4, i8 addrspace(5)* %93) #21
  ret <2 x i64> %86
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.start.p5i8(i64 immarg, i8 addrspace(5)* nocapture) #15

; Function Attrs: convergent norecurse nounwind
define internal fastcc i32 @0(i32 %0) unnamed_addr #16 {
  %2 = tail call i32 asm sideeffect "; ockl readfirstlane hoisting hack $0", "=v,0"(i32 %0) #24, !srcloc !101
  ret i32 %2
}

; Function Attrs: convergent nounwind readnone willreturn
declare i32 @llvm.amdgcn.readfirstlane(i32) #17

; Function Attrs: convergent norecurse nounwind
define internal fastcc i64 @1(%2 addrspace(1)* nocapture %0, i32 %1, i32 %2) unnamed_addr #16 {
  %4 = icmp eq i32 %1, %2
  br i1 %4, label %5, label %29

5:                                                ; preds = %3
  %6 = getelementptr inbounds %2, %2 addrspace(1)* %0, i64 0, i32 3
  %7 = load atomic i64, i64 addrspace(1)* %6 syncscope("one-as") acquire, align 8
  %8 = getelementptr %2, %2 addrspace(1)* %0, i64 0, i32 0
  %9 = getelementptr %2, %2 addrspace(1)* %0, i64 0, i32 5
  %10 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(1)* %8, align 8, !tbaa !96
  %11 = load i64, i64 addrspace(1)* %9, align 8, !tbaa !98
  %12 = and i64 %11, %7
  %13 = getelementptr inbounds %3, %3 addrspace(1)* %10, i64 %12, i32 0
  %14 = load atomic i64, i64 addrspace(1)* %13 syncscope("one-as") monotonic, align 8
  %15 = cmpxchg i64 addrspace(1)* %6, i64 %7, i64 %14 syncscope("one-as") acquire monotonic, align 8
  %16 = extractvalue { i64, i1 } %15, 1
  br i1 %16, label %29, label %17

17:                                               ; preds = %17, %5
  %18 = phi { i64, i1 } [ %25, %17 ], [ %15, %5 ]
  %19 = extractvalue { i64, i1 } %18, 0
  tail call void @llvm.amdgcn.s.sleep(i32 1) #21
  %20 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(1)* %8, align 8, !tbaa !96
  %21 = load i64, i64 addrspace(1)* %9, align 8, !tbaa !98
  %22 = and i64 %21, %19
  %23 = getelementptr inbounds %3, %3 addrspace(1)* %20, i64 %22, i32 0
  %24 = load atomic i64, i64 addrspace(1)* %23 syncscope("one-as") monotonic, align 8
  %25 = cmpxchg i64 addrspace(1)* %6, i64 %19, i64 %24 syncscope("one-as") acquire monotonic, align 8
  %26 = extractvalue { i64, i1 } %25, 1
  br i1 %26, label %27, label %17

27:                                               ; preds = %17
  %28 = extractvalue { i64, i1 } %18, 0
  br label %29

29:                                               ; preds = %27, %5, %3
  %30 = phi i64 [ 0, %3 ], [ %7, %5 ], [ %28, %27 ]
  %31 = trunc i64 %30 to i32
  %32 = lshr i64 %30, 32
  %33 = trunc i64 %32 to i32
  %34 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %31)
  %35 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %33)
  %36 = zext i32 %35 to i64
  %37 = shl nuw i64 %36, 32
  %38 = zext i32 %34 to i64
  %39 = or i64 %37, %38
  ret i64 %39
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readonly willreturn
define internal fastcc %3 addrspace(1)* @2(%3 addrspace(1)* %0, i64 %1, i64 %2) unnamed_addr #18 {
  %4 = and i64 %2, %1
  %5 = getelementptr inbounds %3, %3 addrspace(1)* %0, i64 %4
  ret %3 addrspace(1)* %5
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readonly willreturn
define internal fastcc %4 addrspace(1)* @3(%4 addrspace(1)* %0, i64 %1, i64 %2) unnamed_addr #18 {
  %4 = and i64 %2, %1
  %5 = getelementptr inbounds %4, %4 addrspace(1)* %0, i64 %4
  ret %4 addrspace(1)* %5
}

; Function Attrs: convergent mustprogress nofree norecurse nounwind willreturn
define internal fastcc void @4(%3 addrspace(1)* nocapture writeonly %0, %4 addrspace(1)* nocapture writeonly %1, i32 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8, i64 %9, i64 %10, i32 %11, i32 %12) unnamed_addr #19 {
  %14 = tail call i64 @llvm.read_register.i64(metadata !102) #28
  %15 = icmp eq i32 %11, %12
  br i1 %15, label %16, label %20

16:                                               ; preds = %13
  %17 = getelementptr inbounds %3, %3 addrspace(1)* %0, i64 0, i32 3
  %18 = getelementptr inbounds %3, %3 addrspace(1)* %0, i64 0, i32 1
  %19 = getelementptr inbounds %3, %3 addrspace(1)* %0, i64 0, i32 2
  store i32 %2, i32 addrspace(1)* %19, align 8, !tbaa !103
  store i64 %14, i64 addrspace(1)* %18, align 8, !tbaa !105
  store i32 1, i32 addrspace(1)* %17, align 4, !tbaa !106
  br label %20

20:                                               ; preds = %16, %13
  %21 = zext i32 %11 to i64
  %22 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 0
  store i64 %3, i64 addrspace(1)* %22, align 8, !tbaa !94
  %23 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 1
  store i64 %4, i64 addrspace(1)* %23, align 8, !tbaa !94
  %24 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 2
  store i64 %5, i64 addrspace(1)* %24, align 8, !tbaa !94
  %25 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 3
  store i64 %6, i64 addrspace(1)* %25, align 8, !tbaa !94
  %26 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 4
  store i64 %7, i64 addrspace(1)* %26, align 8, !tbaa !94
  %27 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 5
  store i64 %8, i64 addrspace(1)* %27, align 8, !tbaa !94
  %28 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 6
  store i64 %9, i64 addrspace(1)* %28, align 8, !tbaa !94
  %29 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %21, i64 7
  store i64 %10, i64 addrspace(1)* %29, align 8, !tbaa !94
  ret void
}

; Function Attrs: convergent norecurse nounwind
define internal fastcc void @5(%2 addrspace(1)* nocapture %0, i64 %1, i32 %2, i32 %3) unnamed_addr #16 {
  %5 = icmp eq i32 %2, %3
  br i1 %5, label %6, label %25

6:                                                ; preds = %4
  %7 = getelementptr inbounds %2, %2 addrspace(1)* %0, i64 0, i32 4
  %8 = load atomic i64, i64 addrspace(1)* %7 syncscope("one-as") monotonic, align 8
  %9 = getelementptr %2, %2 addrspace(1)* %0, i64 0, i32 0
  %10 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(1)* %9, align 8, !tbaa !96
  %11 = getelementptr %2, %2 addrspace(1)* %0, i64 0, i32 5
  %12 = load i64, i64 addrspace(1)* %11, align 8, !tbaa !98
  %13 = and i64 %12, %1
  %14 = getelementptr inbounds %3, %3 addrspace(1)* %10, i64 %13, i32 0
  store i64 %8, i64 addrspace(1)* %14, align 8, !tbaa !107
  %15 = cmpxchg i64 addrspace(1)* %7, i64 %8, i64 %1 syncscope("one-as") release monotonic, align 8
  %16 = extractvalue { i64, i1 } %15, 1
  br i1 %16, label %22, label %17

17:                                               ; preds = %17, %6
  %18 = phi { i64, i1 } [ %20, %17 ], [ %15, %6 ]
  %19 = extractvalue { i64, i1 } %18, 0
  tail call void @llvm.amdgcn.s.sleep(i32 1) #21
  store i64 %19, i64 addrspace(1)* %14, align 8, !tbaa !107
  %20 = cmpxchg i64 addrspace(1)* %7, i64 %19, i64 %1 syncscope("one-as") release monotonic, align 8
  %21 = extractvalue { i64, i1 } %20, 1
  br i1 %21, label %22, label %17

22:                                               ; preds = %17, %6
  %23 = getelementptr inbounds %2, %2 addrspace(1)* %0, i64 0, i32 2, i32 0
  %24 = load i64, i64 addrspace(1)* %23, align 8
  tail call void @__ockl_hsa_signal_add(i64 %24, i64 1, i32 3) #24
  br label %25

25:                                               ; preds = %22, %4
  ret void
}

; Function Attrs: convergent norecurse nounwind
define internal fastcc <2 x i64> @6(%3 addrspace(1)* nocapture readonly %0, %4 addrspace(1)* nocapture readonly %1, i32 %2, i32 %3) unnamed_addr #13 {
  %5 = icmp eq i32 %2, %3
  %6 = getelementptr inbounds %3, %3 addrspace(1)* %0, i64 0, i32 3
  br label %7

7:                                                ; preds = %15, %4
  br i1 %5, label %8, label %11

8:                                                ; preds = %7
  %9 = load atomic i32, i32 addrspace(1)* %6 syncscope("one-as") acquire, align 4
  %10 = and i32 %9, 1
  br label %11

11:                                               ; preds = %8, %7
  %12 = phi i32 [ %10, %8 ], [ 1, %7 ]
  %13 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %12)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %16, label %15

15:                                               ; preds = %11
  tail call void @llvm.amdgcn.s.sleep(i32 1)
  br label %7

16:                                               ; preds = %11
  %17 = zext i32 %2 to i64
  %18 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %17, i64 0
  %19 = getelementptr inbounds %4, %4 addrspace(1)* %1, i64 0, i32 0, i64 %17, i64 1
  %20 = load i64, i64 addrspace(1)* %18, align 8, !tbaa !94
  %21 = load i64, i64 addrspace(1)* %19, align 8, !tbaa !94
  %22 = insertelement <2 x i64> undef, i64 %20, i64 0
  %23 = insertelement <2 x i64> %22, i64 %21, i64 1
  ret <2 x i64> %23
}

; Function Attrs: convergent norecurse nounwind
define internal fastcc void @7(%2 addrspace(1)* nocapture %0, i64 %1, i32 %2, i32 %3) unnamed_addr #16 {
  %5 = icmp eq i32 %2, %3
  br i1 %5, label %6, label %26

6:                                                ; preds = %4
  %7 = getelementptr inbounds %2, %2 addrspace(1)* %0, i64 0, i32 5
  %8 = load i64, i64 addrspace(1)* %7, align 8, !tbaa !98
  %9 = add i64 %8, 1
  %10 = add i64 %9, %1
  %11 = icmp eq i64 %10, 0
  %12 = select i1 %11, i64 %9, i64 %10
  %13 = getelementptr inbounds %2, %2 addrspace(1)* %0, i64 0, i32 3
  %14 = load atomic i64, i64 addrspace(1)* %13 syncscope("one-as") monotonic, align 8
  %15 = getelementptr %2, %2 addrspace(1)* %0, i64 0, i32 0
  %16 = load %3 addrspace(1)*, %3 addrspace(1)* addrspace(1)* %15, align 8, !tbaa !96
  %17 = and i64 %12, %8
  %18 = getelementptr inbounds %3, %3 addrspace(1)* %16, i64 %17, i32 0
  store i64 %14, i64 addrspace(1)* %18, align 8, !tbaa !107
  %19 = cmpxchg i64 addrspace(1)* %13, i64 %14, i64 %12 syncscope("one-as") release monotonic, align 8
  %20 = extractvalue { i64, i1 } %19, 1
  br i1 %20, label %26, label %21

21:                                               ; preds = %21, %6
  %22 = phi { i64, i1 } [ %24, %21 ], [ %19, %6 ]
  %23 = extractvalue { i64, i1 } %22, 0
  tail call void @llvm.amdgcn.s.sleep(i32 1) #21
  store i64 %23, i64 addrspace(1)* %18, align 8, !tbaa !107
  %24 = cmpxchg i64 addrspace(1)* %13, i64 %23, i64 %12 syncscope("one-as") release monotonic, align 8
  %25 = extractvalue { i64, i1 } %24, 1
  br i1 %25, label %26, label %21

26:                                               ; preds = %21, %6, %4
  ret void
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.end.p5i8(i64 immarg, i8 addrspace(5)* nocapture) #15

; Function Attrs: nounwind willreturn
declare void @llvm.amdgcn.s.sleep(i32 immarg) #20

; Function Attrs: convergent norecurse nounwind
define internal void @__ockl_hsa_signal_add(i64 %0, i64 %1, i32 %2) local_unnamed_addr #16 {
  %4 = inttoptr i64 %0 to %5 addrspace(1)*
  %5 = getelementptr inbounds %5, %5 addrspace(1)* %4, i64 0, i32 1, i32 0
  switch i32 %2, label %6 [
    i32 1, label %8
    i32 2, label %8
    i32 3, label %10
    i32 4, label %12
    i32 5, label %14
  ]

6:                                                ; preds = %3
  %7 = atomicrmw add i64 addrspace(1)* %5, i64 %1 syncscope("one-as") monotonic, align 8
  br label %16

8:                                                ; preds = %3, %3
  %9 = atomicrmw add i64 addrspace(1)* %5, i64 %1 syncscope("one-as") acquire, align 8
  br label %16

10:                                               ; preds = %3
  %11 = atomicrmw add i64 addrspace(1)* %5, i64 %1 syncscope("one-as") release, align 8
  br label %16

12:                                               ; preds = %3
  %13 = atomicrmw add i64 addrspace(1)* %5, i64 %1 syncscope("one-as") acq_rel, align 8
  br label %16

14:                                               ; preds = %3
  %15 = atomicrmw add i64 addrspace(1)* %5, i64 %1 seq_cst, align 8
  br label %16

16:                                               ; preds = %14, %12, %10, %8, %6
  %17 = getelementptr inbounds %5, %5 addrspace(1)* %4, i64 0, i32 2
  %18 = load i64, i64 addrspace(1)* %17, align 16, !tbaa !108
  %19 = icmp eq i64 %18, 0
  br i1 %19, label %27, label %20

20:                                               ; preds = %16
  %21 = inttoptr i64 %18 to i64 addrspace(1)*
  %22 = getelementptr inbounds %5, %5 addrspace(1)* %4, i64 0, i32 3
  %23 = load i32, i32 addrspace(1)* %22, align 8, !tbaa !110
  %24 = zext i32 %23 to i64
  store atomic i64 %24, i64 addrspace(1)* %21 syncscope("one-as") release, align 8
  %25 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %23) #21
  %26 = and i32 %25, 255
  tail call void @llvm.amdgcn.s.sendmsg(i32 1, i32 %26) #21
  br label %27

27:                                               ; preds = %20, %16
  ret void
}

; Function Attrs: nounwind
declare void @llvm.amdgcn.s.sendmsg(i32 immarg, i32) #21

; Function Attrs: nounwind readonly
declare i64 @llvm.read_register.i64(metadata) #22

; Function Attrs: nounwind readnone willreturn
declare i32 @llvm.amdgcn.mbcnt.lo(i32, i32) #23

; Function Attrs: nounwind readnone willreturn
declare i32 @llvm.amdgcn.mbcnt.hi(i32, i32) #23

; Function Attrs: convergent norecurse nounwind
define internal i64 @__ockl_fprintf_append_args(i64 %0, i32 %1, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8, i32 %9) #13 {
  %11 = icmp eq i32 %9, 0
  %12 = or i64 %0, 2
  %13 = select i1 %11, i64 %0, i64 %12
  %14 = and i64 %13, -225
  %15 = zext i32 %1 to i64
  %16 = shl nuw nsw i64 %15, 5
  %17 = or i64 %14, %16
  %18 = tail call <2 x i64> @__ockl_hostcall_preview(i32 2, i64 %17, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8) #24
  %19 = extractelement <2 x i64> %18, i64 0
  ret i64 %19
}

; Function Attrs: convergent norecurse nounwind
define internal i64 @__ockl_fprintf_append_string_n(i64 %0, i8* readonly %1, i64 %2, i32 %3) #13 {
  %5 = icmp eq i32 %3, 0
  %6 = or i64 %0, 2
  %7 = select i1 %5, i64 %0, i64 %6
  %8 = icmp eq i8* %1, null
  br i1 %8, label %9, label %13

9:                                                ; preds = %4
  %10 = and i64 %7, -225
  %11 = or i64 %10, 32
  %12 = tail call <2 x i64> @__ockl_hostcall_preview(i32 2, i64 %11, i64 0, i64 0, i64 0, i64 0, i64 0, i64 0, i64 0) #24
  br label %455

13:                                               ; preds = %4
  %14 = and i64 %7, 2
  %15 = and i64 %7, -3
  %16 = insertelement <2 x i64> <i64 poison, i64 0>, i64 %15, i64 0
  br label %17

17:                                               ; preds = %444, %13
  %18 = phi i64 [ %2, %13 ], [ %452, %444 ]
  %19 = phi i8* [ %1, %13 ], [ %453, %444 ]
  %20 = phi <2 x i64> [ %16, %13 ], [ %451, %444 ]
  %21 = icmp ugt i64 %18, 56
  %22 = extractelement <2 x i64> %20, i64 0
  %23 = or i64 %22, %14
  %24 = insertelement <2 x i64> poison, i64 %23, i64 0
  %25 = select i1 %21, <2 x i64> %20, <2 x i64> %24
  %26 = icmp ult i64 %18, 56
  %27 = select i1 %26, i64 %18, i64 56
  %28 = trunc i64 %27 to i32
  %29 = extractelement <2 x i64> %25, i64 0
  %30 = icmp ugt i32 %28, 7
  br i1 %30, label %33, label %31

31:                                               ; preds = %17
  %32 = icmp eq i32 %28, 0
  br i1 %32, label %86, label %73

33:                                               ; preds = %17
  %34 = load i8, i8* %19, align 1, !tbaa !100
  %35 = zext i8 %34 to i64
  %36 = getelementptr inbounds i8, i8* %19, i64 1
  %37 = load i8, i8* %36, align 1, !tbaa !100
  %38 = zext i8 %37 to i64
  %39 = shl nuw nsw i64 %38, 8
  %40 = or i64 %39, %35
  %41 = getelementptr inbounds i8, i8* %19, i64 2
  %42 = load i8, i8* %41, align 1, !tbaa !100
  %43 = zext i8 %42 to i64
  %44 = shl nuw nsw i64 %43, 16
  %45 = or i64 %40, %44
  %46 = getelementptr inbounds i8, i8* %19, i64 3
  %47 = load i8, i8* %46, align 1, !tbaa !100
  %48 = zext i8 %47 to i64
  %49 = shl nuw nsw i64 %48, 24
  %50 = or i64 %45, %49
  %51 = getelementptr inbounds i8, i8* %19, i64 4
  %52 = load i8, i8* %51, align 1, !tbaa !100
  %53 = zext i8 %52 to i64
  %54 = shl nuw nsw i64 %53, 32
  %55 = or i64 %50, %54
  %56 = getelementptr inbounds i8, i8* %19, i64 5
  %57 = load i8, i8* %56, align 1, !tbaa !100
  %58 = zext i8 %57 to i64
  %59 = shl nuw nsw i64 %58, 40
  %60 = or i64 %55, %59
  %61 = getelementptr inbounds i8, i8* %19, i64 6
  %62 = load i8, i8* %61, align 1, !tbaa !100
  %63 = zext i8 %62 to i64
  %64 = shl nuw nsw i64 %63, 48
  %65 = or i64 %60, %64
  %66 = getelementptr inbounds i8, i8* %19, i64 7
  %67 = load i8, i8* %66, align 1, !tbaa !100
  %68 = zext i8 %67 to i64
  %69 = shl nuw i64 %68, 56
  %70 = or i64 %65, %69
  %71 = add nsw i32 %28, -8
  %72 = getelementptr inbounds i8, i8* %19, i64 8
  br label %86

73:                                               ; preds = %73, %31
  %74 = phi i32 [ %84, %73 ], [ 0, %31 ]
  %75 = phi i64 [ %83, %73 ], [ 0, %31 ]
  %76 = zext i32 %74 to i64
  %77 = getelementptr inbounds i8, i8* %19, i64 %76
  %78 = load i8, i8* %77, align 1, !tbaa !100
  %79 = zext i8 %78 to i64
  %80 = shl i32 %74, 3
  %81 = zext i32 %80 to i64
  %82 = shl nuw i64 %79, %81
  %83 = or i64 %82, %75
  %84 = add nuw nsw i32 %74, 1
  %85 = icmp eq i32 %84, %28
  br i1 %85, label %86, label %73

86:                                               ; preds = %73, %33, %31
  %87 = phi i8* [ %72, %33 ], [ %19, %31 ], [ %19, %73 ]
  %88 = phi i32 [ %71, %33 ], [ 0, %31 ], [ 0, %73 ]
  %89 = phi i64 [ %70, %33 ], [ 0, %31 ], [ %83, %73 ]
  %90 = icmp ugt i32 %88, 7
  br i1 %90, label %93, label %91

91:                                               ; preds = %86
  %92 = icmp eq i32 %88, 0
  br i1 %92, label %146, label %133

93:                                               ; preds = %86
  %94 = load i8, i8* %87, align 1, !tbaa !100
  %95 = zext i8 %94 to i64
  %96 = getelementptr inbounds i8, i8* %87, i64 1
  %97 = load i8, i8* %96, align 1, !tbaa !100
  %98 = zext i8 %97 to i64
  %99 = shl nuw nsw i64 %98, 8
  %100 = or i64 %99, %95
  %101 = getelementptr inbounds i8, i8* %87, i64 2
  %102 = load i8, i8* %101, align 1, !tbaa !100
  %103 = zext i8 %102 to i64
  %104 = shl nuw nsw i64 %103, 16
  %105 = or i64 %100, %104
  %106 = getelementptr inbounds i8, i8* %87, i64 3
  %107 = load i8, i8* %106, align 1, !tbaa !100
  %108 = zext i8 %107 to i64
  %109 = shl nuw nsw i64 %108, 24
  %110 = or i64 %105, %109
  %111 = getelementptr inbounds i8, i8* %87, i64 4
  %112 = load i8, i8* %111, align 1, !tbaa !100
  %113 = zext i8 %112 to i64
  %114 = shl nuw nsw i64 %113, 32
  %115 = or i64 %110, %114
  %116 = getelementptr inbounds i8, i8* %87, i64 5
  %117 = load i8, i8* %116, align 1, !tbaa !100
  %118 = zext i8 %117 to i64
  %119 = shl nuw nsw i64 %118, 40
  %120 = or i64 %115, %119
  %121 = getelementptr inbounds i8, i8* %87, i64 6
  %122 = load i8, i8* %121, align 1, !tbaa !100
  %123 = zext i8 %122 to i64
  %124 = shl nuw nsw i64 %123, 48
  %125 = or i64 %120, %124
  %126 = getelementptr inbounds i8, i8* %87, i64 7
  %127 = load i8, i8* %126, align 1, !tbaa !100
  %128 = zext i8 %127 to i64
  %129 = shl nuw i64 %128, 56
  %130 = or i64 %125, %129
  %131 = add nsw i32 %88, -8
  %132 = getelementptr inbounds i8, i8* %87, i64 8
  br label %146

133:                                              ; preds = %133, %91
  %134 = phi i32 [ %144, %133 ], [ 0, %91 ]
  %135 = phi i64 [ %143, %133 ], [ 0, %91 ]
  %136 = zext i32 %134 to i64
  %137 = getelementptr inbounds i8, i8* %87, i64 %136
  %138 = load i8, i8* %137, align 1, !tbaa !100
  %139 = zext i8 %138 to i64
  %140 = shl i32 %134, 3
  %141 = zext i32 %140 to i64
  %142 = shl nuw i64 %139, %141
  %143 = or i64 %142, %135
  %144 = add nuw nsw i32 %134, 1
  %145 = icmp eq i32 %144, %88
  br i1 %145, label %146, label %133

146:                                              ; preds = %133, %93, %91
  %147 = phi i8* [ %132, %93 ], [ %87, %91 ], [ %87, %133 ]
  %148 = phi i32 [ %131, %93 ], [ 0, %91 ], [ 0, %133 ]
  %149 = phi i64 [ %130, %93 ], [ 0, %91 ], [ %143, %133 ]
  %150 = icmp ugt i32 %148, 7
  br i1 %150, label %153, label %151

151:                                              ; preds = %146
  %152 = icmp eq i32 %148, 0
  br i1 %152, label %206, label %193

153:                                              ; preds = %146
  %154 = load i8, i8* %147, align 1, !tbaa !100
  %155 = zext i8 %154 to i64
  %156 = getelementptr inbounds i8, i8* %147, i64 1
  %157 = load i8, i8* %156, align 1, !tbaa !100
  %158 = zext i8 %157 to i64
  %159 = shl nuw nsw i64 %158, 8
  %160 = or i64 %159, %155
  %161 = getelementptr inbounds i8, i8* %147, i64 2
  %162 = load i8, i8* %161, align 1, !tbaa !100
  %163 = zext i8 %162 to i64
  %164 = shl nuw nsw i64 %163, 16
  %165 = or i64 %160, %164
  %166 = getelementptr inbounds i8, i8* %147, i64 3
  %167 = load i8, i8* %166, align 1, !tbaa !100
  %168 = zext i8 %167 to i64
  %169 = shl nuw nsw i64 %168, 24
  %170 = or i64 %165, %169
  %171 = getelementptr inbounds i8, i8* %147, i64 4
  %172 = load i8, i8* %171, align 1, !tbaa !100
  %173 = zext i8 %172 to i64
  %174 = shl nuw nsw i64 %173, 32
  %175 = or i64 %170, %174
  %176 = getelementptr inbounds i8, i8* %147, i64 5
  %177 = load i8, i8* %176, align 1, !tbaa !100
  %178 = zext i8 %177 to i64
  %179 = shl nuw nsw i64 %178, 40
  %180 = or i64 %175, %179
  %181 = getelementptr inbounds i8, i8* %147, i64 6
  %182 = load i8, i8* %181, align 1, !tbaa !100
  %183 = zext i8 %182 to i64
  %184 = shl nuw nsw i64 %183, 48
  %185 = or i64 %180, %184
  %186 = getelementptr inbounds i8, i8* %147, i64 7
  %187 = load i8, i8* %186, align 1, !tbaa !100
  %188 = zext i8 %187 to i64
  %189 = shl nuw i64 %188, 56
  %190 = or i64 %185, %189
  %191 = add nsw i32 %148, -8
  %192 = getelementptr inbounds i8, i8* %147, i64 8
  br label %206

193:                                              ; preds = %193, %151
  %194 = phi i32 [ %204, %193 ], [ 0, %151 ]
  %195 = phi i64 [ %203, %193 ], [ 0, %151 ]
  %196 = zext i32 %194 to i64
  %197 = getelementptr inbounds i8, i8* %147, i64 %196
  %198 = load i8, i8* %197, align 1, !tbaa !100
  %199 = zext i8 %198 to i64
  %200 = shl i32 %194, 3
  %201 = zext i32 %200 to i64
  %202 = shl nuw i64 %199, %201
  %203 = or i64 %202, %195
  %204 = add nuw nsw i32 %194, 1
  %205 = icmp eq i32 %204, %148
  br i1 %205, label %206, label %193

206:                                              ; preds = %193, %153, %151
  %207 = phi i8* [ %192, %153 ], [ %147, %151 ], [ %147, %193 ]
  %208 = phi i32 [ %191, %153 ], [ 0, %151 ], [ 0, %193 ]
  %209 = phi i64 [ %190, %153 ], [ 0, %151 ], [ %203, %193 ]
  %210 = icmp ugt i32 %208, 7
  br i1 %210, label %213, label %211

211:                                              ; preds = %206
  %212 = icmp eq i32 %208, 0
  br i1 %212, label %266, label %253

213:                                              ; preds = %206
  %214 = load i8, i8* %207, align 1, !tbaa !100
  %215 = zext i8 %214 to i64
  %216 = getelementptr inbounds i8, i8* %207, i64 1
  %217 = load i8, i8* %216, align 1, !tbaa !100
  %218 = zext i8 %217 to i64
  %219 = shl nuw nsw i64 %218, 8
  %220 = or i64 %219, %215
  %221 = getelementptr inbounds i8, i8* %207, i64 2
  %222 = load i8, i8* %221, align 1, !tbaa !100
  %223 = zext i8 %222 to i64
  %224 = shl nuw nsw i64 %223, 16
  %225 = or i64 %220, %224
  %226 = getelementptr inbounds i8, i8* %207, i64 3
  %227 = load i8, i8* %226, align 1, !tbaa !100
  %228 = zext i8 %227 to i64
  %229 = shl nuw nsw i64 %228, 24
  %230 = or i64 %225, %229
  %231 = getelementptr inbounds i8, i8* %207, i64 4
  %232 = load i8, i8* %231, align 1, !tbaa !100
  %233 = zext i8 %232 to i64
  %234 = shl nuw nsw i64 %233, 32
  %235 = or i64 %230, %234
  %236 = getelementptr inbounds i8, i8* %207, i64 5
  %237 = load i8, i8* %236, align 1, !tbaa !100
  %238 = zext i8 %237 to i64
  %239 = shl nuw nsw i64 %238, 40
  %240 = or i64 %235, %239
  %241 = getelementptr inbounds i8, i8* %207, i64 6
  %242 = load i8, i8* %241, align 1, !tbaa !100
  %243 = zext i8 %242 to i64
  %244 = shl nuw nsw i64 %243, 48
  %245 = or i64 %240, %244
  %246 = getelementptr inbounds i8, i8* %207, i64 7
  %247 = load i8, i8* %246, align 1, !tbaa !100
  %248 = zext i8 %247 to i64
  %249 = shl nuw i64 %248, 56
  %250 = or i64 %245, %249
  %251 = add nsw i32 %208, -8
  %252 = getelementptr inbounds i8, i8* %207, i64 8
  br label %266

253:                                              ; preds = %253, %211
  %254 = phi i32 [ %264, %253 ], [ 0, %211 ]
  %255 = phi i64 [ %263, %253 ], [ 0, %211 ]
  %256 = zext i32 %254 to i64
  %257 = getelementptr inbounds i8, i8* %207, i64 %256
  %258 = load i8, i8* %257, align 1, !tbaa !100
  %259 = zext i8 %258 to i64
  %260 = shl i32 %254, 3
  %261 = zext i32 %260 to i64
  %262 = shl nuw i64 %259, %261
  %263 = or i64 %262, %255
  %264 = add nuw nsw i32 %254, 1
  %265 = icmp eq i32 %264, %208
  br i1 %265, label %266, label %253

266:                                              ; preds = %253, %213, %211
  %267 = phi i8* [ %252, %213 ], [ %207, %211 ], [ %207, %253 ]
  %268 = phi i32 [ %251, %213 ], [ 0, %211 ], [ 0, %253 ]
  %269 = phi i64 [ %250, %213 ], [ 0, %211 ], [ %263, %253 ]
  %270 = icmp ugt i32 %268, 7
  br i1 %270, label %273, label %271

271:                                              ; preds = %266
  %272 = icmp eq i32 %268, 0
  br i1 %272, label %326, label %313

273:                                              ; preds = %266
  %274 = load i8, i8* %267, align 1, !tbaa !100
  %275 = zext i8 %274 to i64
  %276 = getelementptr inbounds i8, i8* %267, i64 1
  %277 = load i8, i8* %276, align 1, !tbaa !100
  %278 = zext i8 %277 to i64
  %279 = shl nuw nsw i64 %278, 8
  %280 = or i64 %279, %275
  %281 = getelementptr inbounds i8, i8* %267, i64 2
  %282 = load i8, i8* %281, align 1, !tbaa !100
  %283 = zext i8 %282 to i64
  %284 = shl nuw nsw i64 %283, 16
  %285 = or i64 %280, %284
  %286 = getelementptr inbounds i8, i8* %267, i64 3
  %287 = load i8, i8* %286, align 1, !tbaa !100
  %288 = zext i8 %287 to i64
  %289 = shl nuw nsw i64 %288, 24
  %290 = or i64 %285, %289
  %291 = getelementptr inbounds i8, i8* %267, i64 4
  %292 = load i8, i8* %291, align 1, !tbaa !100
  %293 = zext i8 %292 to i64
  %294 = shl nuw nsw i64 %293, 32
  %295 = or i64 %290, %294
  %296 = getelementptr inbounds i8, i8* %267, i64 5
  %297 = load i8, i8* %296, align 1, !tbaa !100
  %298 = zext i8 %297 to i64
  %299 = shl nuw nsw i64 %298, 40
  %300 = or i64 %295, %299
  %301 = getelementptr inbounds i8, i8* %267, i64 6
  %302 = load i8, i8* %301, align 1, !tbaa !100
  %303 = zext i8 %302 to i64
  %304 = shl nuw nsw i64 %303, 48
  %305 = or i64 %300, %304
  %306 = getelementptr inbounds i8, i8* %267, i64 7
  %307 = load i8, i8* %306, align 1, !tbaa !100
  %308 = zext i8 %307 to i64
  %309 = shl nuw i64 %308, 56
  %310 = or i64 %305, %309
  %311 = add nsw i32 %268, -8
  %312 = getelementptr inbounds i8, i8* %267, i64 8
  br label %326

313:                                              ; preds = %313, %271
  %314 = phi i32 [ %324, %313 ], [ 0, %271 ]
  %315 = phi i64 [ %323, %313 ], [ 0, %271 ]
  %316 = zext i32 %314 to i64
  %317 = getelementptr inbounds i8, i8* %267, i64 %316
  %318 = load i8, i8* %317, align 1, !tbaa !100
  %319 = zext i8 %318 to i64
  %320 = shl i32 %314, 3
  %321 = zext i32 %320 to i64
  %322 = shl nuw i64 %319, %321
  %323 = or i64 %322, %315
  %324 = add nuw nsw i32 %314, 1
  %325 = icmp eq i32 %324, %268
  br i1 %325, label %326, label %313

326:                                              ; preds = %313, %273, %271
  %327 = phi i8* [ %312, %273 ], [ %267, %271 ], [ %267, %313 ]
  %328 = phi i32 [ %311, %273 ], [ 0, %271 ], [ 0, %313 ]
  %329 = phi i64 [ %310, %273 ], [ 0, %271 ], [ %323, %313 ]
  %330 = icmp ugt i32 %328, 7
  br i1 %330, label %333, label %331

331:                                              ; preds = %326
  %332 = icmp eq i32 %328, 0
  br i1 %332, label %386, label %373

333:                                              ; preds = %326
  %334 = load i8, i8* %327, align 1, !tbaa !100
  %335 = zext i8 %334 to i64
  %336 = getelementptr inbounds i8, i8* %327, i64 1
  %337 = load i8, i8* %336, align 1, !tbaa !100
  %338 = zext i8 %337 to i64
  %339 = shl nuw nsw i64 %338, 8
  %340 = or i64 %339, %335
  %341 = getelementptr inbounds i8, i8* %327, i64 2
  %342 = load i8, i8* %341, align 1, !tbaa !100
  %343 = zext i8 %342 to i64
  %344 = shl nuw nsw i64 %343, 16
  %345 = or i64 %340, %344
  %346 = getelementptr inbounds i8, i8* %327, i64 3
  %347 = load i8, i8* %346, align 1, !tbaa !100
  %348 = zext i8 %347 to i64
  %349 = shl nuw nsw i64 %348, 24
  %350 = or i64 %345, %349
  %351 = getelementptr inbounds i8, i8* %327, i64 4
  %352 = load i8, i8* %351, align 1, !tbaa !100
  %353 = zext i8 %352 to i64
  %354 = shl nuw nsw i64 %353, 32
  %355 = or i64 %350, %354
  %356 = getelementptr inbounds i8, i8* %327, i64 5
  %357 = load i8, i8* %356, align 1, !tbaa !100
  %358 = zext i8 %357 to i64
  %359 = shl nuw nsw i64 %358, 40
  %360 = or i64 %355, %359
  %361 = getelementptr inbounds i8, i8* %327, i64 6
  %362 = load i8, i8* %361, align 1, !tbaa !100
  %363 = zext i8 %362 to i64
  %364 = shl nuw nsw i64 %363, 48
  %365 = or i64 %360, %364
  %366 = getelementptr inbounds i8, i8* %327, i64 7
  %367 = load i8, i8* %366, align 1, !tbaa !100
  %368 = zext i8 %367 to i64
  %369 = shl nuw i64 %368, 56
  %370 = or i64 %365, %369
  %371 = add nsw i32 %328, -8
  %372 = getelementptr inbounds i8, i8* %327, i64 8
  br label %386

373:                                              ; preds = %373, %331
  %374 = phi i32 [ %384, %373 ], [ 0, %331 ]
  %375 = phi i64 [ %383, %373 ], [ 0, %331 ]
  %376 = zext i32 %374 to i64
  %377 = getelementptr inbounds i8, i8* %327, i64 %376
  %378 = load i8, i8* %377, align 1, !tbaa !100
  %379 = zext i8 %378 to i64
  %380 = shl i32 %374, 3
  %381 = zext i32 %380 to i64
  %382 = shl nuw i64 %379, %381
  %383 = or i64 %382, %375
  %384 = add nuw nsw i32 %374, 1
  %385 = icmp eq i32 %384, %328
  br i1 %385, label %386, label %373

386:                                              ; preds = %373, %333, %331
  %387 = phi i8* [ %372, %333 ], [ %327, %331 ], [ %327, %373 ]
  %388 = phi i32 [ %371, %333 ], [ 0, %331 ], [ 0, %373 ]
  %389 = phi i64 [ %370, %333 ], [ 0, %331 ], [ %383, %373 ]
  %390 = icmp ugt i32 %388, 7
  br i1 %390, label %393, label %391

391:                                              ; preds = %386
  %392 = icmp eq i32 %388, 0
  br i1 %392, label %444, label %431

393:                                              ; preds = %386
  %394 = load i8, i8* %387, align 1, !tbaa !100
  %395 = zext i8 %394 to i64
  %396 = getelementptr inbounds i8, i8* %387, i64 1
  %397 = load i8, i8* %396, align 1, !tbaa !100
  %398 = zext i8 %397 to i64
  %399 = shl nuw nsw i64 %398, 8
  %400 = or i64 %399, %395
  %401 = getelementptr inbounds i8, i8* %387, i64 2
  %402 = load i8, i8* %401, align 1, !tbaa !100
  %403 = zext i8 %402 to i64
  %404 = shl nuw nsw i64 %403, 16
  %405 = or i64 %400, %404
  %406 = getelementptr inbounds i8, i8* %387, i64 3
  %407 = load i8, i8* %406, align 1, !tbaa !100
  %408 = zext i8 %407 to i64
  %409 = shl nuw nsw i64 %408, 24
  %410 = or i64 %405, %409
  %411 = getelementptr inbounds i8, i8* %387, i64 4
  %412 = load i8, i8* %411, align 1, !tbaa !100
  %413 = zext i8 %412 to i64
  %414 = shl nuw nsw i64 %413, 32
  %415 = or i64 %410, %414
  %416 = getelementptr inbounds i8, i8* %387, i64 5
  %417 = load i8, i8* %416, align 1, !tbaa !100
  %418 = zext i8 %417 to i64
  %419 = shl nuw nsw i64 %418, 40
  %420 = or i64 %415, %419
  %421 = getelementptr inbounds i8, i8* %387, i64 6
  %422 = load i8, i8* %421, align 1, !tbaa !100
  %423 = zext i8 %422 to i64
  %424 = shl nuw nsw i64 %423, 48
  %425 = or i64 %420, %424
  %426 = getelementptr inbounds i8, i8* %387, i64 7
  %427 = load i8, i8* %426, align 1, !tbaa !100
  %428 = zext i8 %427 to i64
  %429 = shl nuw i64 %428, 56
  %430 = or i64 %425, %429
  br label %444

431:                                              ; preds = %431, %391
  %432 = phi i32 [ %442, %431 ], [ 0, %391 ]
  %433 = phi i64 [ %441, %431 ], [ 0, %391 ]
  %434 = zext i32 %432 to i64
  %435 = getelementptr inbounds i8, i8* %387, i64 %434
  %436 = load i8, i8* %435, align 1, !tbaa !100
  %437 = zext i8 %436 to i64
  %438 = shl i32 %432, 3
  %439 = zext i32 %438 to i64
  %440 = shl nuw i64 %437, %439
  %441 = or i64 %440, %433
  %442 = add nuw nsw i32 %432, 1
  %443 = icmp eq i32 %442, %388
  br i1 %443, label %444, label %431

444:                                              ; preds = %431, %393, %391
  %445 = phi i64 [ %430, %393 ], [ 0, %391 ], [ %441, %431 ]
  %446 = shl nuw nsw i64 %27, 2
  %447 = add nuw nsw i64 %446, 28
  %448 = and i64 %447, 480
  %449 = and i64 %29, -225
  %450 = or i64 %449, %448
  %451 = tail call <2 x i64> @__ockl_hostcall_preview(i32 2, i64 %450, i64 %89, i64 %149, i64 %209, i64 %269, i64 %329, i64 %389, i64 %445) #24
  %452 = sub i64 %18, %27
  %453 = getelementptr inbounds i8, i8* %19, i64 %27
  %454 = icmp eq i64 %452, 0
  br i1 %454, label %455, label %17

455:                                              ; preds = %444, %9
  %456 = phi <2 x i64> [ %12, %9 ], [ %451, %444 ]
  %457 = extractelement <2 x i64> %456, i64 0
  ret i64 %457
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal i64 @__ockl_get_local_size(i32 %0) #10 {
  %2 = tail call align 4 dereferenceable(64) i8 addrspace(4)* @llvm.amdgcn.dispatch.ptr()
  switch i32 %0, label %27 [
    i32 0, label %3
    i32 1, label %11
    i32 2, label %19
  ]

3:                                                ; preds = %1
  %4 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %5 = getelementptr i8, i8 addrspace(4)* %2, i64 4
  %6 = bitcast i8 addrspace(4)* %5 to i16 addrspace(4)*
  %7 = load i16, i16 addrspace(4)* %6, align 4, !range !90, !invariant.load !91
  %8 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 12
  %9 = bitcast i8 addrspace(4)* %8 to i32 addrspace(4)*
  %10 = load i32, i32 addrspace(4)* %9, align 4, !tbaa !84
  br label %27

11:                                               ; preds = %1
  %12 = tail call i32 @llvm.amdgcn.workgroup.id.y()
  %13 = getelementptr i8, i8 addrspace(4)* %2, i64 6
  %14 = bitcast i8 addrspace(4)* %13 to i16 addrspace(4)*
  %15 = load i16, i16 addrspace(4)* %14, align 2, !range !90, !invariant.load !91
  %16 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 16
  %17 = bitcast i8 addrspace(4)* %16 to i32 addrspace(4)*
  %18 = load i32, i32 addrspace(4)* %17, align 8, !tbaa !92
  br label %27

19:                                               ; preds = %1
  %20 = tail call i32 @llvm.amdgcn.workgroup.id.z()
  %21 = getelementptr i8, i8 addrspace(4)* %2, i64 8
  %22 = bitcast i8 addrspace(4)* %21 to i16 addrspace(4)*
  %23 = load i16, i16 addrspace(4)* %22, align 4, !range !90, !invariant.load !91
  %24 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 20
  %25 = bitcast i8 addrspace(4)* %24 to i32 addrspace(4)*
  %26 = load i32, i32 addrspace(4)* %25, align 4, !tbaa !93
  br label %27

27:                                               ; preds = %19, %11, %3, %1
  %28 = phi i32 [ %26, %19 ], [ %18, %11 ], [ %10, %3 ], [ 0, %1 ]
  %29 = phi i16 [ %23, %19 ], [ %15, %11 ], [ %7, %3 ], [ 1, %1 ]
  %30 = phi i32 [ %20, %19 ], [ %12, %11 ], [ %4, %3 ], [ 0, %1 ]
  %31 = zext i16 %29 to i32
  %32 = mul i32 %30, %31
  %33 = sub i32 %28, %32
  %34 = icmp ult i32 %33, %31
  %35 = select i1 %34, i32 %33, i32 %31
  %36 = zext i32 %35 to i64
  ret i64 %36
}

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workgroup.id.x() #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workgroup.id.y() #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workgroup.id.z() #8

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal i64 @__ockl_get_local_id(i32 %0) #10 {
  switch i32 %0, label %8 [
    i32 0, label %2
    i32 1, label %4
    i32 2, label %6
  ]

2:                                                ; preds = %1
  %3 = tail call i32 @llvm.amdgcn.workitem.id.x(), !range !111
  br label %8

4:                                                ; preds = %1
  %5 = tail call i32 @llvm.amdgcn.workitem.id.y(), !range !111
  br label %8

6:                                                ; preds = %1
  %7 = tail call i32 @llvm.amdgcn.workitem.id.z(), !range !111
  br label %8

8:                                                ; preds = %6, %4, %2, %1
  %9 = phi i32 [ %7, %6 ], [ %5, %4 ], [ %3, %2 ], [ 0, %1 ]
  %10 = zext i32 %9 to i64
  ret i64 %10
}

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workitem.id.x() #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workitem.id.y() #8

; Function Attrs: nounwind readnone speculatable willreturn
declare i32 @llvm.amdgcn.workitem.id.z() #8

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal i64 @__ockl_get_group_id(i32 %0) #10 {
  switch i32 %0, label %8 [
    i32 0, label %2
    i32 1, label %4
    i32 2, label %6
  ]

2:                                                ; preds = %1
  %3 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  br label %8

4:                                                ; preds = %1
  %5 = tail call i32 @llvm.amdgcn.workgroup.id.y()
  br label %8

6:                                                ; preds = %1
  %7 = tail call i32 @llvm.amdgcn.workgroup.id.z()
  br label %8

8:                                                ; preds = %6, %4, %2, %1
  %9 = phi i32 [ %7, %6 ], [ %5, %4 ], [ %3, %2 ], [ 0, %1 ]
  %10 = zext i32 %9 to i64
  ret i64 %10
}

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind readnone willreturn
define internal i64 @__ockl_get_global_size(i32 %0) #10 {
  %2 = tail call align 4 dereferenceable(64) i8 addrspace(4)* @llvm.amdgcn.dispatch.ptr()
  switch i32 %0, label %15 [
    i32 0, label %3
    i32 1, label %7
    i32 2, label %11
  ]

3:                                                ; preds = %1
  %4 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 12
  %5 = bitcast i8 addrspace(4)* %4 to i32 addrspace(4)*
  %6 = load i32, i32 addrspace(4)* %5, align 4, !tbaa !84
  br label %15

7:                                                ; preds = %1
  %8 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 16
  %9 = bitcast i8 addrspace(4)* %8 to i32 addrspace(4)*
  %10 = load i32, i32 addrspace(4)* %9, align 8, !tbaa !92
  br label %15

11:                                               ; preds = %1
  %12 = getelementptr inbounds i8, i8 addrspace(4)* %2, i64 20
  %13 = bitcast i8 addrspace(4)* %12 to i32 addrspace(4)*
  %14 = load i32, i32 addrspace(4)* %13, align 4, !tbaa !93
  br label %15

15:                                               ; preds = %11, %7, %3, %1
  %16 = phi i32 [ %14, %11 ], [ %10, %7 ], [ %6, %3 ], [ 1, %1 ]
  %17 = zext i32 %16 to i64
  ret i64 %17
}

attributes #0 = { convergent mustprogress noinline noreturn nounwind optnone "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx1030" "target-features"="+16-bit-insts,+ci-insts,+dl-insts,+dot1-insts,+dot2-insts,+dot5-insts,+dot6-insts,+dot7-insts,+dpp,+flat-address-space,+gfx10-3-insts,+gfx10-insts,+gfx8-insts,+gfx9-insts,+s-memrealtime,+s-memtime-inst" }
attributes #1 = { cold noreturn nounwind }
attributes #2 = { convergent mustprogress noinline nounwind optnone "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx1030" "target-features"="+16-bit-insts,+ci-insts,+dl-insts,+dot1-insts,+dot2-insts,+dot5-insts,+dot6-insts,+dot7-insts,+dpp,+flat-address-space,+gfx10-3-insts,+gfx10-insts,+gfx8-insts,+gfx9-insts,+s-memrealtime,+s-memtime-inst" }
attributes #3 = { argmemonly nofree nounwind willreturn }
attributes #4 = { convergent mustprogress noinline norecurse nounwind optnone "amdgpu-flat-work-group-size"="1,1024" "amdgpu-implicitarg-num-bytes"="56" "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx1030" "target-features"="+16-bit-insts,+ci-insts,+dl-insts,+dot1-insts,+dot2-insts,+dot5-insts,+dot6-insts,+dot7-insts,+dpp,+flat-address-space,+gfx10-3-insts,+gfx10-insts,+gfx8-insts,+gfx9-insts,+s-memrealtime,+s-memtime-inst" "uniform-work-group-size"="true" }
attributes #5 = { convergent noinline nounwind optnone "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx1030" "target-features"="+16-bit-insts,+ci-insts,+dl-insts,+dot1-insts,+dot2-insts,+dot5-insts,+dot6-insts,+dot7-insts,+dpp,+flat-address-space,+gfx10-3-insts,+gfx10-insts,+gfx8-insts,+gfx9-insts,+s-memrealtime,+s-memtime-inst" }
attributes #6 = { convergent mustprogress nofree norecurse nosync nounwind readnone willreturn "frame-pointer"="all" "min-legal-vector-width"="128" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #7 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #8 = { nounwind readnone speculatable willreturn }
attributes #9 = { convergent mustprogress nofree norecurse nounwind readnone willreturn "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #10 = { convergent mustprogress nofree norecurse nosync nounwind readnone willreturn "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #11 = { convergent mustprogress nofree norecurse nounwind readnone willreturn "frame-pointer"="all" "min-legal-vector-width"="128" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #12 = { convergent mustprogress nofree norecurse nounwind readnone willreturn "frame-pointer"="all" "min-legal-vector-width"="64" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #13 = { convergent norecurse nounwind "frame-pointer"="all" "min-legal-vector-width"="128" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #14 = { convergent noinline norecurse nounwind optnone "frame-pointer"="all" "min-legal-vector-width"="128" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #15 = { argmemonly nofree nosync nounwind willreturn }
attributes #16 = { convergent norecurse nounwind "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #17 = { convergent nounwind readnone willreturn }
attributes #18 = { convergent mustprogress nofree norecurse nosync nounwind readonly willreturn "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #19 = { convergent mustprogress nofree norecurse nounwind willreturn "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
attributes #20 = { nounwind willreturn }
attributes #21 = { nounwind }
attributes #22 = { nounwind readonly }
attributes #23 = { nounwind readnone willreturn }
attributes #24 = { convergent nounwind }
attributes #25 = { convergent nounwind readonly willreturn }
attributes #26 = { readnone }
attributes #27 = { nounwind readnone }
attributes #28 = { convergent }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3}
!opencl.ocl.version = !{!4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4, !4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 7, !"frame-pointer", i32 2}
!3 = !{!"AMD clang version 14.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-5.1.2 22114 5cba46feb6af367b1cafaa183ec42dbfb8207b14)"}
!4 = !{i32 2, i32 0}
!5 = distinct !{!5, !6}
!6 = !{!"llvm.loop.mustprogress"}
!7 = distinct !{!7, !6}
!8 = distinct !{!8, !6}
!9 = distinct !{!9, !6}
!10 = distinct !{!10, !6}
!11 = distinct !{!11, !6}
!12 = distinct !{!12, !6}
!13 = distinct !{!13, !6}
!14 = distinct !{!14, !6}
!15 = distinct !{!15, !6}
!16 = distinct !{!16, !6}
!17 = distinct !{!17, !6}
!18 = distinct !{!18, !6}
!19 = distinct !{!19, !6}
!20 = distinct !{!20, !6}
!21 = distinct !{!21, !6}
!22 = distinct !{!22, !6}
!23 = distinct !{!23, !6}
!24 = distinct !{!24, !6}
!25 = distinct !{!25, !6}
!26 = distinct !{!26, !6}
!27 = distinct !{!27, !6}
!28 = distinct !{!28, !6}
!29 = distinct !{!29, !6}
!30 = distinct !{!30, !6}
!31 = distinct !{!31, !6}
!32 = distinct !{!32, !6}
!33 = distinct !{!33, !6}
!34 = distinct !{!34, !6}
!35 = distinct !{!35, !6}
!36 = distinct !{!36, !6}
!37 = distinct !{!37, !6}
!38 = distinct !{!38, !6}
!39 = distinct !{!39, !6}
!40 = distinct !{!40, !6}
!41 = distinct !{!41, !6}
!42 = distinct !{!42, !6}
!43 = distinct !{!43, !6}
!44 = distinct !{!44, !6}
!45 = distinct !{!45, !6}
!46 = distinct !{!46, !6}
!47 = distinct !{!47, !6}
!48 = distinct !{!48, !6}
!49 = distinct !{!49, !6}
!50 = distinct !{!50, !6}
!51 = distinct !{!51, !6}
!52 = distinct !{!52, !6}
!53 = distinct !{!53, !6}
!54 = distinct !{!54, !6}
!55 = distinct !{!55, !6}
!56 = distinct !{!56, !6}
!57 = distinct !{!57, !6}
!58 = distinct !{!58, !6}
!59 = distinct !{!59, !6}
!60 = distinct !{!60, !6}
!61 = distinct !{!61, !6}
!62 = distinct !{!62, !6}
!63 = distinct !{!63, !6}
!64 = distinct !{!64, !6}
!65 = distinct !{!65, !6}
!66 = distinct !{!66, !6}
!67 = distinct !{!67, !6}
!68 = distinct !{!68, !6}
!69 = distinct !{!69, !6}
!70 = distinct !{!70, !6}
!71 = distinct !{!71, !6}
!72 = distinct !{!72, !6}
!73 = distinct !{!73, !6}
!74 = distinct !{!74, !6}
!75 = !{!76, !76, i64 0}
!76 = !{!"bool", !77, i64 0}
!77 = !{!"omnipotent char", !78, i64 0}
!78 = !{!"Simple C/C++ TBAA"}
!79 = !{i8 0, i8 2}
!80 = !{float 2.500000e+00}
!81 = !{!82, !82, i64 0}
!82 = !{!"int", !77, i64 0}
!83 = !{i32 0, i32 33}
!84 = !{!85, !82, i64 12}
!85 = !{!"hsa_kernel_dispatch_packet_s", !86, i64 0, !86, i64 2, !86, i64 4, !86, i64 6, !86, i64 8, !86, i64 10, !82, i64 12, !82, i64 16, !82, i64 20, !82, i64 24, !82, i64 28, !87, i64 32, !88, i64 40, !87, i64 48, !89, i64 56}
!86 = !{!"short", !77, i64 0}
!87 = !{!"long", !77, i64 0}
!88 = !{!"any pointer", !77, i64 0}
!89 = !{!"hsa_signal_s", !87, i64 0}
!90 = !{i16 1, i16 1025}
!91 = !{}
!92 = !{!85, !82, i64 16}
!93 = !{!85, !82, i64 20}
!94 = !{!87, !87, i64 0}
!95 = !{!88, !88, i64 0}
!96 = !{!97, !88, i64 0}
!97 = !{!"", !88, i64 0, !88, i64 8, !89, i64 16, !87, i64 24, !87, i64 32, !87, i64 40}
!98 = !{!97, !87, i64 40}
!99 = !{!97, !88, i64 8}
!100 = !{!77, !77, i64 0}
!101 = !{i64 2509}
!102 = !{!"exec"}
!103 = !{!104, !82, i64 16}
!104 = !{!"", !87, i64 0, !87, i64 8, !82, i64 16, !82, i64 20}
!105 = !{!104, !87, i64 8}
!106 = !{!104, !82, i64 20}
!107 = !{!104, !87, i64 0}
!108 = !{!109, !87, i64 16}
!109 = !{!"amd_signal_s", !87, i64 0, !77, i64 8, !87, i64 16, !82, i64 24, !82, i64 28, !87, i64 32, !87, i64 40, !77, i64 48, !77, i64 56}
!110 = !{!109, !82, i64 24}
!111 = !{i32 0, i32 1024}
